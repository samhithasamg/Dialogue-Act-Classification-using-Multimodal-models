{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7efd7dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from sklearn.metrics import classification_report\n",
    "import platform \n",
    "import sys\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# For data conversions\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# To get NN models in pytorch\n",
    "import torch.nn as nn\n",
    "\n",
    "# To get optimizers such as GD\n",
    "import torch.optim as optim\n",
    "\n",
    "# To get relu and tanh functions\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Platform: Windows-10-10.0.22621-SP0\n",
      "PyTorch Version: 2.0.0+cu117\n",
      "Python 3.10.10 | packaged by Anaconda, Inc. | (main, Mar 21 2023, 18:39:17) [MSC v.1916 64 bit (AMD64)]\n",
      "Pandas 1.5.2\n",
      "GPU is available\n",
      "Target device is cuda\n"
     ]
    }
   ],
   "source": [
    "has_gpu = torch.cuda.is_available()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Python Platform: {platform.platform()}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(\"GPU is\", \"available\" if has_gpu else \"NOT AVAILABLE\")\n",
    "print(f\"Target device is {device}\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "# Reading text features that are in line with alignment data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "text_aligned_train_features = np.load('train_features_alignment.npy')\n",
    "text_aligned_test_features = np.load('test_features_alignment.npy')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 5.04321873e-01,  4.63240236e-01, -3.76627706e-02, ...,\n        -1.18496947e-01,  3.52687031e-01,  3.75959247e-01],\n       [ 1.97530389e-01,  2.23903388e-01,  8.67783278e-03, ...,\n        -1.96769819e-01,  2.87841946e-01,  2.99373597e-01],\n       [ 2.55765110e-01,  1.04705095e-01, -3.99574637e-05, ...,\n        -4.20659125e-01,  5.00405431e-01,  3.90712887e-01],\n       ...,\n       [-5.71486726e-02,  1.16940103e-01,  7.00608790e-02, ...,\n        -1.42893016e-01,  9.89775807e-02,  3.86829376e-01],\n       [ 4.03575838e-01,  3.52924109e-01,  1.08982280e-01, ...,\n         5.49964160e-02,  3.62594336e-01,  2.75151968e-01],\n       [ 4.99402825e-03,  2.02879816e-01,  2.22124338e-01, ...,\n        -3.82540077e-02,  1.75538465e-01,  1.02757707e-01]], dtype=float32)"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_aligned_train_features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.04274414, -0.0171528 , -0.10105078, ...,  0.00315719,\n         0.48995572,  0.5435381 ],\n       [-0.13518153,  0.2433322 ,  0.03738693, ..., -0.10703974,\n         0.15525305,  0.14268635],\n       [-0.05714867,  0.1169401 ,  0.07006088, ..., -0.14289302,\n         0.09897758,  0.38682938],\n       ...,\n       [-0.05788826,  0.03145726, -0.46042058, ...,  0.2709097 ,\n         0.09669909,  0.41208312],\n       [ 0.08848633,  0.06102172, -0.21425134, ...,  0.46084163,\n         0.5064107 ,  0.46143913],\n       [ 0.14460252,  0.18129203, -0.04733727, ..., -0.19956347,\n         0.87466955,  0.3786211 ]], dtype=float32)"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_aligned_test_features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "# Reading audio features that are in line with alignment data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "audio_aligned_train_features = pd.read_csv('train_features_audio_aligned.csv')\n",
    "audio_aligned_test_features = pd.read_csv('test_features_audio_aligned.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "data": {
      "text/plain": "   Unnamed: 0 Channel                                          Utterance  \\\n0       24551       A                     otherwise theyre pretty smelly   \n1       23101       B  and i think a lot of their friends feel the sa...   \n2       27063       B  well now so if you were going to have a dinner...   \n3       20702       A                                            they uh   \n4        5976       B  i i dont feel like theyre a benefit to society...   \n\n       file       start         end    pitch0    pitch1    pitch2      fb00  \\\n0  4019.txt  220.071750  221.542875  0.058766  0.731051  0.638110  0.899390   \n1  2709.txt  432.140875  434.443250  0.098493  0.476925  0.364297  0.851667   \n2  3506.txt    3.879375    7.575625  0.119491  0.390849  0.420487  0.963859   \n3  3228.txt   55.163000   55.805875 -0.028226  0.336798  0.912177  0.927696   \n4  3247.txt   40.875875   44.615500  0.110553  0.543148  1.124164  0.841859   \n\n       fb01      fb02 Label  \n0  0.884808  0.852887    sv  \n1  0.897019  0.761266    sd  \n2  0.968053  0.937679    qw  \n3  0.921769  0.901077     %  \n4  0.877972  0.821876    sv  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Channel</th>\n      <th>Utterance</th>\n      <th>file</th>\n      <th>start</th>\n      <th>end</th>\n      <th>pitch0</th>\n      <th>pitch1</th>\n      <th>pitch2</th>\n      <th>fb00</th>\n      <th>fb01</th>\n      <th>fb02</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>24551</td>\n      <td>A</td>\n      <td>otherwise theyre pretty smelly</td>\n      <td>4019.txt</td>\n      <td>220.071750</td>\n      <td>221.542875</td>\n      <td>0.058766</td>\n      <td>0.731051</td>\n      <td>0.638110</td>\n      <td>0.899390</td>\n      <td>0.884808</td>\n      <td>0.852887</td>\n      <td>sv</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>23101</td>\n      <td>B</td>\n      <td>and i think a lot of their friends feel the sa...</td>\n      <td>2709.txt</td>\n      <td>432.140875</td>\n      <td>434.443250</td>\n      <td>0.098493</td>\n      <td>0.476925</td>\n      <td>0.364297</td>\n      <td>0.851667</td>\n      <td>0.897019</td>\n      <td>0.761266</td>\n      <td>sd</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>27063</td>\n      <td>B</td>\n      <td>well now so if you were going to have a dinner...</td>\n      <td>3506.txt</td>\n      <td>3.879375</td>\n      <td>7.575625</td>\n      <td>0.119491</td>\n      <td>0.390849</td>\n      <td>0.420487</td>\n      <td>0.963859</td>\n      <td>0.968053</td>\n      <td>0.937679</td>\n      <td>qw</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20702</td>\n      <td>A</td>\n      <td>they uh</td>\n      <td>3228.txt</td>\n      <td>55.163000</td>\n      <td>55.805875</td>\n      <td>-0.028226</td>\n      <td>0.336798</td>\n      <td>0.912177</td>\n      <td>0.927696</td>\n      <td>0.921769</td>\n      <td>0.901077</td>\n      <td>%</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5976</td>\n      <td>B</td>\n      <td>i i dont feel like theyre a benefit to society...</td>\n      <td>3247.txt</td>\n      <td>40.875875</td>\n      <td>44.615500</td>\n      <td>0.110553</td>\n      <td>0.543148</td>\n      <td>1.124164</td>\n      <td>0.841859</td>\n      <td>0.877972</td>\n      <td>0.821876</td>\n      <td>sv</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_aligned_train_features.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "   Unnamed: 0 Channel                                     Utterance      file  \\\n0        7246       A                            and one is uh four  3457.txt   \n1        1323       B                                          yeah  2349.txt   \n2       33043       A                                          okay  2608.txt   \n3       26715       A                 regarding uh taxes i you know  4725.txt   \n4       23466       A  and that was actually after the war was over  2253.txt   \n\n        start         end    pitch0    pitch1    pitch2      fb00      fb01  \\\n0    5.724875    7.149125  0.079450  0.745006  0.446980  0.812936  0.790438   \n1   18.924125   19.216375 -0.029854  0.196361  0.130172  0.704110  0.687018   \n2    5.790500    6.150500 -0.012475  0.378718  2.083808  0.816360  0.851317   \n3    0.650000    1.900000  0.033159  0.370226  0.428816  0.890855  0.886848   \n4  291.411000  292.961000  0.051122  0.643769  0.504700  0.912026  0.881303   \n\n       fb02            Label  \n0  0.581930               sd  \n1  0.650901                b  \n2  0.702781               aa  \n3  0.928268  fo_o_fw_\"_by_bc  \n4  0.915515               sd  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Channel</th>\n      <th>Utterance</th>\n      <th>file</th>\n      <th>start</th>\n      <th>end</th>\n      <th>pitch0</th>\n      <th>pitch1</th>\n      <th>pitch2</th>\n      <th>fb00</th>\n      <th>fb01</th>\n      <th>fb02</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7246</td>\n      <td>A</td>\n      <td>and one is uh four</td>\n      <td>3457.txt</td>\n      <td>5.724875</td>\n      <td>7.149125</td>\n      <td>0.079450</td>\n      <td>0.745006</td>\n      <td>0.446980</td>\n      <td>0.812936</td>\n      <td>0.790438</td>\n      <td>0.581930</td>\n      <td>sd</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1323</td>\n      <td>B</td>\n      <td>yeah</td>\n      <td>2349.txt</td>\n      <td>18.924125</td>\n      <td>19.216375</td>\n      <td>-0.029854</td>\n      <td>0.196361</td>\n      <td>0.130172</td>\n      <td>0.704110</td>\n      <td>0.687018</td>\n      <td>0.650901</td>\n      <td>b</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>33043</td>\n      <td>A</td>\n      <td>okay</td>\n      <td>2608.txt</td>\n      <td>5.790500</td>\n      <td>6.150500</td>\n      <td>-0.012475</td>\n      <td>0.378718</td>\n      <td>2.083808</td>\n      <td>0.816360</td>\n      <td>0.851317</td>\n      <td>0.702781</td>\n      <td>aa</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>26715</td>\n      <td>A</td>\n      <td>regarding uh taxes i you know</td>\n      <td>4725.txt</td>\n      <td>0.650000</td>\n      <td>1.900000</td>\n      <td>0.033159</td>\n      <td>0.370226</td>\n      <td>0.428816</td>\n      <td>0.890855</td>\n      <td>0.886848</td>\n      <td>0.928268</td>\n      <td>fo_o_fw_\"_by_bc</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>23466</td>\n      <td>A</td>\n      <td>and that was actually after the war was over</td>\n      <td>2253.txt</td>\n      <td>291.411000</td>\n      <td>292.961000</td>\n      <td>0.051122</td>\n      <td>0.643769</td>\n      <td>0.504700</td>\n      <td>0.912026</td>\n      <td>0.881303</td>\n      <td>0.915515</td>\n      <td>sd</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_aligned_test_features.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "# Early fusion for Multimodal model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "                                            features label\n0  [0.5043218731880188, 0.46324023604393005, -0.0...    sv\n1  [0.19753038883209229, 0.22390338778495789, 0.0...    sd\n2  [0.2557651102542877, 0.1047050952911377, -3.99...    qw\n3  [0.16346515715122223, 0.31099289655685425, 0.2...     %\n4  [0.40566009283065796, 0.505189836025238, -0.06...    sv",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>features</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[0.5043218731880188, 0.46324023604393005, -0.0...</td>\n      <td>sv</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[0.19753038883209229, 0.22390338778495789, 0.0...</td>\n      <td>sd</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[0.2557651102542877, 0.1047050952911377, -3.99...</td>\n      <td>qw</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[0.16346515715122223, 0.31099289655685425, 0.2...</td>\n      <td>%</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[0.40566009283065796, 0.505189836025238, -0.06...</td>\n      <td>sv</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multimodal_data_train = pd.DataFrame(columns=['features', 'label'])\n",
    "for index, row in audio_aligned_train_features.iterrows():\n",
    "    multimodal_feature = np.concatenate((\n",
    "        text_aligned_train_features[index],\n",
    "        audio_aligned_train_features.iloc[index][6:-1].to_numpy()),\n",
    "        axis=0)\n",
    "    label = audio_aligned_train_features.iloc[index]['Label']\n",
    "    temp = {'features': multimodal_feature, 'label': label}\n",
    "    multimodal_data_train = pd.concat([multimodal_data_train, pd.DataFrame([temp])], ignore_index=True)\n",
    "\n",
    "multimodal_data_train.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "                                            features label  da\n0  [0.5043218731880188, 0.46324023604393005, -0.0...    sv  38\n1  [0.19753038883209229, 0.22390338778495789, 0.0...    sd  37\n2  [0.2557651102542877, 0.1047050952911377, -3.99...    qw  33\n3  [0.16346515715122223, 0.31099289655685425, 0.2...     %   0\n4  [0.40566009283065796, 0.505189836025238, -0.06...    sv  38",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>features</th>\n      <th>label</th>\n      <th>da</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[0.5043218731880188, 0.46324023604393005, -0.0...</td>\n      <td>sv</td>\n      <td>38</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[0.19753038883209229, 0.22390338778495789, 0.0...</td>\n      <td>sd</td>\n      <td>37</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[0.2557651102542877, 0.1047050952911377, -3.99...</td>\n      <td>qw</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[0.16346515715122223, 0.31099289655685425, 0.2...</td>\n      <td>%</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[0.40566009283065796, 0.505189836025238, -0.06...</td>\n      <td>sv</td>\n      <td>38</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(multimodal_data_train['label'])\n",
    "train_encoded_labels = le.transform(multimodal_data_train['label'])\n",
    "\n",
    "multimodal_data_train['da'] = train_encoded_labels\n",
    "multimodal_data_train.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "data": {
      "text/plain": "                                            features            label\n0  [0.042744144797325134, -0.017152801156044006, ...               sd\n1  [-0.13518153131008148, 0.24333220720291138, 0....                b\n2  [-0.05714867264032364, 0.11694010347127914, 0....               aa\n3  [0.008160506375133991, 0.3085247278213501, -0....  fo_o_fw_\"_by_bc\n4  [0.023708511143922806, 0.05153898894786835, -0...               sd",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>features</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[0.042744144797325134, -0.017152801156044006, ...</td>\n      <td>sd</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[-0.13518153131008148, 0.24333220720291138, 0....</td>\n      <td>b</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[-0.05714867264032364, 0.11694010347127914, 0....</td>\n      <td>aa</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[0.008160506375133991, 0.3085247278213501, -0....</td>\n      <td>fo_o_fw_\"_by_bc</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[0.023708511143922806, 0.05153898894786835, -0...</td>\n      <td>sd</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multimodal_data_test = pd.DataFrame(columns=['features', 'label'])\n",
    "for index, row in audio_aligned_test_features.iterrows():\n",
    "    multimodal_feature = np.concatenate((\n",
    "        text_aligned_test_features[index],\n",
    "        audio_aligned_test_features.iloc[0][6:-1].to_numpy()),\n",
    "        axis=0)\n",
    "    label = audio_aligned_test_features.iloc[index]['Label']\n",
    "    temp = {'features': multimodal_feature, 'label': label}\n",
    "    multimodal_data_test = pd.concat([multimodal_data_test, pd.DataFrame([temp])], ignore_index=True)\n",
    "\n",
    "multimodal_data_test.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "                                            features            label  da\n0  [0.042744144797325134, -0.017152801156044006, ...               sd  37\n1  [-0.13518153131008148, 0.24333220720291138, 0....                b  10\n2  [-0.05714867264032364, 0.11694010347127914, 0....               aa   5\n3  [0.008160506375133991, 0.3085247278213501, -0....  fo_o_fw_\"_by_bc  20\n4  [0.023708511143922806, 0.05153898894786835, -0...               sd  37",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>features</th>\n      <th>label</th>\n      <th>da</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[0.042744144797325134, -0.017152801156044006, ...</td>\n      <td>sd</td>\n      <td>37</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[-0.13518153131008148, 0.24333220720291138, 0....</td>\n      <td>b</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[-0.05714867264032364, 0.11694010347127914, 0....</td>\n      <td>aa</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[0.008160506375133991, 0.3085247278213501, -0....</td>\n      <td>fo_o_fw_\"_by_bc</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[0.023708511143922806, 0.05153898894786835, -0...</td>\n      <td>sd</td>\n      <td>37</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_encoded_labels = le.transform(multimodal_data_test['label'])\n",
    "\n",
    "multimodal_data_test['da'] = test_encoded_labels\n",
    "multimodal_data_test.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "# Deep DA Classification"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "x_train_torch = torch.Tensor(list(multimodal_data_train['features']))\n",
    "x_test_torch = torch.Tensor(list(multimodal_data_test['features']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "y_train = torch.LongTensor(list(multimodal_data_train['da']))\n",
    "y_test = torch.LongTensor(list(multimodal_data_test['da']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "zipper = lambda x,y : list(zip(x,y))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "x_train_torch = zipper(x_train_torch,y_train)\n",
    "x_test_torch = zipper(x_test_torch,y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_classes = 41\n",
    "learning_rate = 0.05\n",
    "batch_size = 16\n",
    "num_epochs = 100\n",
    "num_workers = 1\n",
    "input_size = 774"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(x_train_torch, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(x_test_torch, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "## Model 01: FNN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.feedforwardNN = nn.Sequential(\n",
    "        nn.Linear(input_size, 500),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(500, 200),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(200, 100),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(100, num_classes),\n",
    "        nn.Softmax()\n",
    "    )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.feedforwardNN(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        Y_pred = self.forward(X)\n",
    "        return Y_pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "feedforwardModel = FeedForwardNeuralNetwork(input_size = input_size,\n",
    "                                            num_classes = num_classes).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "# Specify criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(feedforwardModel.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "# Function to get accuracy scores\n",
    "def accuracy(y_pred, y_test):\n",
    "    pred = torch.argmax(y_pred, dim=1)\n",
    "    return (pred == y_test).float().mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "# Function to get accuracy scores\n",
    "def f1_score_NN(y_pred, y_test):\n",
    "    pred = torch.argmax(y_pred, dim=1).cpu()\n",
    "    y_test_cpu = y_test.cpu()\n",
    "    return f1_score(y_test_cpu, pred, average='micro')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "### Training Feedforward NN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "# Start Training\n",
    "def train_FF(model, num_epochs, train_loader, test_loader, modelName):\n",
    "    test_loss_min = np.Inf\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        train_loss = 0.0\n",
    "        test_loss = 0.0\n",
    "\n",
    "        # Prepare the model for training\n",
    "        model.train()\n",
    "\n",
    "        for data, target in train_loader:\n",
    "            data = data.to(device=device)\n",
    "            target = target.type(torch.LongTensor).to(device=device)\n",
    "\n",
    "            data = data.reshape(data.shape[0], -1)\n",
    "\n",
    "            # forward pass: compute predicted outputs by passing\n",
    "            # inputs to the model\n",
    "            scores = model(data)\n",
    "\n",
    "            # calculate the loss\n",
    "            loss = criterion(scores, target)\n",
    "\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # backward pass: compute gradient of the loss\n",
    "            # with respect to model parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "\n",
    "        # Prepare model for evaluation\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data = data.to(device=device)\n",
    "                target = target.type(torch.LongTensor).to(device=device)\n",
    "\n",
    "                scores = model(data)\n",
    "                loss = criterion(scores, target)\n",
    "\n",
    "                test_loss += loss.item()*data.size(0)\n",
    "\n",
    "        train_loss = train_loss/len(train_loader.dataset)\n",
    "        test_loss = test_loss/len(test_loader.dataset)\n",
    "\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tTest Loss: {:.6f}'.format(\n",
    "            epoch+1,\n",
    "            train_loss,\n",
    "            test_loss\n",
    "            ))\n",
    "\n",
    "        # save model if validation loss has decreased\n",
    "        if test_loss <= test_loss_min:\n",
    "            print('Test loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            test_loss_min,\n",
    "            test_loss))\n",
    "            torch.save(model.state_dict(), modelName + '.pt')\n",
    "            test_loss_min = test_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 3.398824 \tTest Loss: 3.333682\n",
      "Test loss decreased (inf --> 3.333682).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 3.333351 \tTest Loss: 3.333670\n",
      "Test loss decreased (3.333682 --> 3.333670).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 3.333344 \tTest Loss: 3.333667\n",
      "Test loss decreased (3.333670 --> 3.333667).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 3.333341 \tTest Loss: 3.333665\n",
      "Test loss decreased (3.333667 --> 3.333665).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 3.333340 \tTest Loss: 3.333665\n",
      "Test loss decreased (3.333665 --> 3.333665).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 3.333339 \tTest Loss: 3.333664\n",
      "Test loss decreased (3.333665 --> 3.333664).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 3.333339 \tTest Loss: 3.333664\n",
      "Test loss decreased (3.333664 --> 3.333664).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 3.333338 \tTest Loss: 3.333664\n",
      "Test loss decreased (3.333664 --> 3.333664).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 3.333338 \tTest Loss: 3.333663\n",
      "Test loss decreased (3.333664 --> 3.333663).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 3.333338 \tTest Loss: 3.333663\n",
      "Test loss decreased (3.333663 --> 3.333663).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 3.333338 \tTest Loss: 3.333663\n",
      "Test loss decreased (3.333663 --> 3.333663).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 3.333337 \tTest Loss: 3.333663\n",
      "Test loss decreased (3.333663 --> 3.333663).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 3.333337 \tTest Loss: 3.333663\n",
      "Test loss decreased (3.333663 --> 3.333663).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 3.333337 \tTest Loss: 3.333663\n",
      "Test loss decreased (3.333663 --> 3.333663).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 3.333337 \tTest Loss: 3.333663\n",
      "Test loss decreased (3.333663 --> 3.333663).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 3.333337 \tTest Loss: 3.333663\n",
      "Test loss decreased (3.333663 --> 3.333663).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 3.333337 \tTest Loss: 3.333663\n",
      "Test loss decreased (3.333663 --> 3.333663).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 3.333337 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333663 --> 3.333662).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 3.333337 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 3.333337 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 3.333337 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 3.333337 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 3.333337 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 3.333337 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 3.333337 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 3.333337 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 3.333337 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 3.333337 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 3.333337 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 3.333337 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 3.333337 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 3.333337 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 3.333337 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 3.333337 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 3.333337 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 3.333337 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 3.333337 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 3.333337 \tTest Loss: 3.333662\n",
      "Epoch: 39 \tTraining Loss: 3.333337 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 3.333337 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 3.333337 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 3.333337 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 3.333337 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 3.333337 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 45 \tTraining Loss: 3.333337 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 3.333337 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 3.333337 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 3.333337 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 3.333337 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 50 \tTraining Loss: 3.333337 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 51 \tTraining Loss: 3.333337 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 52 \tTraining Loss: 3.333337 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 53 \tTraining Loss: 3.333337 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 54 \tTraining Loss: 3.333337 \tTest Loss: 3.333662\n",
      "Epoch: 55 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Epoch: 56 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 57 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 58 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 59 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 60 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 61 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 62 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 63 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 64 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 65 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 66 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 67 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 68 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 69 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 70 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 71 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 72 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 73 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 74 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Epoch: 75 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 76 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 77 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 78 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 79 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 80 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 81 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 82 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 83 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 84 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 85 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 86 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 87 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 88 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 89 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 90 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 91 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 92 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 93 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 94 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Epoch: 95 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 96 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 97 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 98 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 99 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n",
      "Epoch: 100 \tTraining Loss: 3.333336 \tTest Loss: 3.333662\n",
      "Test loss decreased (3.333662 --> 3.333662).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "train_FF(feedforwardModel, num_epochs=num_epochs, train_loader=train_loader, test_loader=test_loader, modelName='feedforwardModel')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model with best test loss\n",
    "feedforwardModel.load_state_dict(torch.load('feedforwardModel.pt'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [],
   "source": [
    "# Reference:\n",
    "# https://www.kaggle.com/code/tauseef6462/simple-feedforward-neural-network-using-pytorch\n",
    "def prepare_for_accuracy(model,\n",
    "                         test_dataset_tensor,\n",
    "                         test_target_tensor):\n",
    "    test_dataset_tensor = test_dataset_tensor.to(device=device)\n",
    "    Y_pred_test = model.predict(test_dataset_tensor)\n",
    "\n",
    "    Y_pred_test = Y_pred_test.to(device=device)\n",
    "    test_target_tensor = test_target_tensor.to(device=device)\n",
    "    accuracy_test = accuracy(Y_pred_test, test_target_tensor)\n",
    "    f1_score_val = f1_score_NN(Y_pred_test, test_target_tensor)\n",
    "    print(\"Test accuracy of Network\",(accuracy_test))\n",
    "    print(\"F1 score of Network\",(f1_score_val))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "x_train_torch = torch.Tensor(list(multimodal_data_train['features']))\n",
    "x_test_torch = torch.Tensor(list(multimodal_data_test['features']))\n",
    "y_train = torch.LongTensor(list(multimodal_data_train['da']))\n",
    "y_test = torch.LongTensor(list(multimodal_data_test['da']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for FeedForward Network :: \n",
      "Test accuracy of Network tensor(0.4210, device='cuda:0')\n",
      "F1 score of Network 0.4209650582362729\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy for FeedForward Network :: ')\n",
    "prepare_for_accuracy(feedforwardModel,\n",
    "                     test_dataset_tensor=x_test_torch,\n",
    "                     test_target_tensor=y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "### Using VGGish embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "cleaned_train_data__with_embeddings = pd.read_csv('train_features_audio_aligned__with_wav_files__embeddings.csv')\n",
    "cleaned_test_data__with_embeddings = pd.read_csv('test_features_audio_aligned__with_wav_files__embeddings.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "# Early fusion for Multimodal model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "data": {
      "text/plain": "                                            features label\n0  [0.5043218731880188, 0.46324023604393005, -0.0...    sv\n1  [0.19753038883209229, 0.22390338778495789, 0.0...    sd\n2  [0.2557651102542877, 0.1047050952911377, -3.99...    ^q\n3  [0.16346515715122223, 0.31099289655685425, 0.2...    sd\n4  [0.40566009283065796, 0.505189836025238, -0.06...    sd",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>features</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[0.5043218731880188, 0.46324023604393005, -0.0...</td>\n      <td>sv</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[0.19753038883209229, 0.22390338778495789, 0.0...</td>\n      <td>sd</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[0.2557651102542877, 0.1047050952911377, -3.99...</td>\n      <td>^q</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[0.16346515715122223, 0.31099289655685425, 0.2...</td>\n      <td>sd</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[0.40566009283065796, 0.505189836025238, -0.06...</td>\n      <td>sd</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multimodal_data_train = pd.DataFrame(columns=['features', 'label'])\n",
    "for index, row in cleaned_train_data__with_embeddings.iterrows():\n",
    "    audio_embeddings = list()\n",
    "    with open('temp.npy', encoding='utf-8', mode='w') as npyf:\n",
    "        temp = cleaned_train_data__with_embeddings.iloc[index]['audio_embeddings']\n",
    "        temp = temp.replace('\\n', '')[1: -1]\n",
    "        npyf.write(temp)\n",
    "    with open('temp.npy', encoding='utf-8', mode='r') as npyf:\n",
    "        lines = npyf.readlines()[0].split(' ')\n",
    "        for line in lines:\n",
    "            if len(line) > 0:\n",
    "                audio_embeddings.append(float(line))\n",
    "    multimodal_feature = np.concatenate((\n",
    "        text_aligned_train_features[index],\n",
    "        audio_embeddings),\n",
    "        axis=0)\n",
    "    label = cleaned_train_data__with_embeddings.iloc[index]['Label']\n",
    "    temp = {'features': multimodal_feature, 'label': label}\n",
    "    multimodal_data_train = pd.concat([multimodal_data_train, pd.DataFrame([temp])], ignore_index=True)\n",
    "\n",
    "multimodal_data_train.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "data": {
      "text/plain": "                                            features            label\n0  [0.042744144797325134, -0.017152801156044006, ...               sd\n1  [-0.13518153131008148, 0.24333220720291138, 0....  fo_o_fw_\"_by_bc\n2  [-0.05714867264032364, 0.11694010347127914, 0....               sd\n3  [0.008160506375133991, 0.3085247278213501, -0....               sd\n4  [0.023708511143922806, 0.05153898894786835, -0...               sv",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>features</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[0.042744144797325134, -0.017152801156044006, ...</td>\n      <td>sd</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[-0.13518153131008148, 0.24333220720291138, 0....</td>\n      <td>fo_o_fw_\"_by_bc</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[-0.05714867264032364, 0.11694010347127914, 0....</td>\n      <td>sd</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[0.008160506375133991, 0.3085247278213501, -0....</td>\n      <td>sd</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[0.023708511143922806, 0.05153898894786835, -0...</td>\n      <td>sv</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multimodal_data_test = pd.DataFrame(columns=['features', 'label'])\n",
    "for index, row in cleaned_test_data__with_embeddings.iterrows():\n",
    "    audio_embeddings = list()\n",
    "    with open('temp.npy', encoding='utf-8', mode='w') as npyf:\n",
    "        temp = cleaned_test_data__with_embeddings.iloc[index]['audio_embeddings']\n",
    "        temp = temp.replace('\\n', '')[1: -1]\n",
    "        npyf.write(temp)\n",
    "    with open('temp.npy', encoding='utf-8', mode='r') as npyf:\n",
    "        lines = npyf.readlines()[0].split(' ')\n",
    "        for line in lines:\n",
    "            if len(line) > 0:\n",
    "                audio_embeddings.append(float(line))\n",
    "    multimodal_feature = np.concatenate((\n",
    "        text_aligned_test_features[index],\n",
    "        audio_embeddings),\n",
    "        axis=0)\n",
    "    label = cleaned_test_data__with_embeddings.iloc[index]['Label']\n",
    "    temp = {'features': multimodal_feature, 'label': label}\n",
    "    multimodal_data_test = pd.concat([multimodal_data_test, pd.DataFrame([temp])], ignore_index=True)\n",
    "\n",
    "multimodal_data_test.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "data": {
      "text/plain": "                                            features label  da\n0  [0.5043218731880188, 0.46324023604393005, -0.0...    sv  37\n1  [0.19753038883209229, 0.22390338778495789, 0.0...    sd  36\n2  [0.2557651102542877, 0.1047050952911377, -3.99...    ^q   3\n3  [0.16346515715122223, 0.31099289655685425, 0.2...    sd  36\n4  [0.40566009283065796, 0.505189836025238, -0.06...    sd  36",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>features</th>\n      <th>label</th>\n      <th>da</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[0.5043218731880188, 0.46324023604393005, -0.0...</td>\n      <td>sv</td>\n      <td>37</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[0.19753038883209229, 0.22390338778495789, 0.0...</td>\n      <td>sd</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[0.2557651102542877, 0.1047050952911377, -3.99...</td>\n      <td>^q</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[0.16346515715122223, 0.31099289655685425, 0.2...</td>\n      <td>sd</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[0.40566009283065796, 0.505189836025238, -0.06...</td>\n      <td>sd</td>\n      <td>36</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(multimodal_data_train['label'])\n",
    "train_encoded_labels = le.transform(multimodal_data_train['label'])\n",
    "\n",
    "multimodal_data_train['da'] = train_encoded_labels\n",
    "multimodal_data_train.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "data": {
      "text/plain": "                                            features            label\n0  [0.042744144797325134, -0.017152801156044006, ...               sd\n1  [-0.13518153131008148, 0.24333220720291138, 0....  fo_o_fw_\"_by_bc\n2  [-0.05714867264032364, 0.11694010347127914, 0....               sd\n3  [0.008160506375133991, 0.3085247278213501, -0....               sd\n4  [0.023708511143922806, 0.05153898894786835, -0...               sv",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>features</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[0.042744144797325134, -0.017152801156044006, ...</td>\n      <td>sd</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[-0.13518153131008148, 0.24333220720291138, 0....</td>\n      <td>fo_o_fw_\"_by_bc</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[-0.05714867264032364, 0.11694010347127914, 0....</td>\n      <td>sd</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[0.008160506375133991, 0.3085247278213501, -0....</td>\n      <td>sd</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[0.023708511143922806, 0.05153898894786835, -0...</td>\n      <td>sv</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multimodal_data_test = multimodal_data_test[multimodal_data_test['label'] != '^g']\n",
    "multimodal_data_test.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "data": {
      "text/plain": "                                            features            label  da\n0  [0.042744144797325134, -0.017152801156044006, ...               sd  36\n1  [-0.13518153131008148, 0.24333220720291138, 0....  fo_o_fw_\"_by_bc  19\n2  [-0.05714867264032364, 0.11694010347127914, 0....               sd  36\n3  [0.008160506375133991, 0.3085247278213501, -0....               sd  36\n4  [0.023708511143922806, 0.05153898894786835, -0...               sv  37",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>features</th>\n      <th>label</th>\n      <th>da</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[0.042744144797325134, -0.017152801156044006, ...</td>\n      <td>sd</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[-0.13518153131008148, 0.24333220720291138, 0....</td>\n      <td>fo_o_fw_\"_by_bc</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[-0.05714867264032364, 0.11694010347127914, 0....</td>\n      <td>sd</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[0.008160506375133991, 0.3085247278213501, -0....</td>\n      <td>sd</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[0.023708511143922806, 0.05153898894786835, -0...</td>\n      <td>sv</td>\n      <td>37</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_encoded_labels = le.transform(multimodal_data_test['label'])\n",
    "\n",
    "multimodal_data_test['da'] = test_encoded_labels\n",
    "multimodal_data_test.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "# Deep DA Classification"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [],
   "source": [
    "x_train_torch = torch.Tensor(list(multimodal_data_train['features']))\n",
    "x_test_torch = torch.Tensor(list(multimodal_data_test['features']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [],
   "source": [
    "y_train = torch.LongTensor(list(multimodal_data_train['da']))\n",
    "y_test = torch.LongTensor(list(multimodal_data_test['da']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [],
   "source": [
    "zipper = lambda x,y : list(zip(x,y))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [],
   "source": [
    "x_train_torch = zipper(x_train_torch,y_train)\n",
    "x_test_torch = zipper(x_test_torch,y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(x_train_torch, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(x_test_torch, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [],
   "source": [
    "## Model 01: FNN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "# num_classes = 41\n",
    "num_classes = 40\n",
    "learning_rate = 0.05\n",
    "batch_size = 16\n",
    "num_epochs = 500\n",
    "num_workers = 1\n",
    "input_size = 896"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "vggishFeedforwardModel = FeedForwardNeuralNetwork(input_size = input_size,\n",
    "                                            num_classes = num_classes).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [],
   "source": [
    "# Specify criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(feedforwardModel.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (inf --> 3.689347).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 45 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 50 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 51 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 52 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 53 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 54 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 55 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 56 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 57 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 58 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 59 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 60 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 61 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 62 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 63 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 64 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 65 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 66 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 67 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 68 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 69 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 70 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 71 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 72 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 73 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 74 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 75 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 76 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 77 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 78 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 79 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 80 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 81 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 82 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 83 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 84 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 85 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 86 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 87 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 88 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 89 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 90 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 91 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 92 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 93 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 94 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 95 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 96 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 97 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 98 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 99 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 100 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 101 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 102 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 103 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 104 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 105 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 106 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 107 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 108 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 109 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 110 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 111 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 112 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 113 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 114 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 115 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 116 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 117 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 118 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 119 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 120 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 121 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 122 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 123 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 124 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 125 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 126 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 127 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 128 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 129 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 130 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 131 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 132 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 133 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 134 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 135 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 136 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 137 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 138 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 139 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 140 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 141 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 142 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 143 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 144 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 145 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 146 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 147 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 148 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 149 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 150 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 151 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 152 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 153 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 154 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 155 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 156 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 157 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 158 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 159 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 160 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 161 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 162 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 163 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 164 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 165 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 166 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 167 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 168 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 169 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 170 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 171 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 172 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 173 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 174 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 175 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 176 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 177 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 178 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 179 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 180 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 181 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 182 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 183 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 184 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 185 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 186 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 187 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 188 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 189 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 190 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 191 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 192 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 193 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 194 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 195 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 196 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 197 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 198 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 199 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 200 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 201 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 202 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 203 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 204 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 205 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 206 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 207 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 208 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 209 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 210 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 211 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 212 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 213 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 214 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 215 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 216 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 217 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 218 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 219 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 220 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 221 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 222 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 223 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 224 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 225 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 226 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 227 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 228 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 229 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 230 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 231 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 232 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 233 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 234 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 235 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 236 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 237 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 238 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 239 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 240 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 241 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 242 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 243 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 244 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 245 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 246 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 247 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 248 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 249 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 250 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 251 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 252 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 253 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 254 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 255 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 256 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 257 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 258 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 259 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 260 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 261 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 262 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 263 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 264 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 265 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 266 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 267 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 268 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 269 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 270 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 271 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 272 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 273 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 274 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 275 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 276 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 277 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 278 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 279 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 280 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 281 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 282 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 283 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 284 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 285 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 286 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 287 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 288 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 289 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 290 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 291 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 292 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 293 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 294 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 295 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 296 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 297 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 298 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 299 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 300 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 301 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 302 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 303 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 304 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 305 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 306 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 307 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 308 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 309 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 310 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 311 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 312 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 313 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 314 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 315 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 316 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 317 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 318 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 319 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 320 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 321 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 322 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 323 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 324 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 325 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 326 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 327 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 328 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 329 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 330 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 331 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 332 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 333 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 334 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 335 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 336 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 337 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 338 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 339 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 340 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 341 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 342 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 343 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 344 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 345 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 346 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 347 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 348 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 349 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 350 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 351 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 352 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 353 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 354 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 355 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 356 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 357 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 358 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 359 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 360 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 361 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 362 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 363 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 364 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 365 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 366 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 367 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 368 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 369 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 370 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 371 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 372 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 373 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 374 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 375 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 376 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 377 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 378 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 379 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 380 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 381 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 382 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 383 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 384 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 385 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 386 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 387 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 388 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 389 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 390 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 391 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 392 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 393 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 394 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 395 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 396 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 397 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 398 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 399 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 400 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 401 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 402 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 403 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 404 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 405 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 406 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 407 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 408 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 409 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 410 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 411 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 412 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 413 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 414 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 415 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 416 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 417 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 418 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 419 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 420 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 421 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 422 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 423 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 424 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 425 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 426 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 427 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 428 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 429 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 430 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 431 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 432 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 433 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 434 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 435 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 436 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 437 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 438 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 439 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 440 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 441 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 442 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 443 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 444 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 445 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 446 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 447 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 448 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 449 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 450 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 451 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 452 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 453 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 454 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 455 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 456 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 457 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 458 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 459 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 460 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 461 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 462 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 463 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 464 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 465 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 466 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 467 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 468 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 469 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 470 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 471 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 472 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 473 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 474 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 475 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 476 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 477 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 478 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 479 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 480 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 481 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 482 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 483 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 484 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 485 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 486 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 487 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 488 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 489 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 490 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 491 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 492 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 493 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 494 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 495 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 496 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 497 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 498 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 499 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n",
      "Epoch: 500 \tTraining Loss: 3.689340 \tTest Loss: 3.689347\n",
      "Test loss decreased (3.689347 --> 3.689347).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "train_FF(vggishFeedforwardModel, num_epochs=num_epochs, train_loader=train_loader, test_loader=test_loader, modelName='vggishFeedforwardModel')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model with best test loss\n",
    "vggishFeedforwardModel.load_state_dict(torch.load('vggishFeedforwardModel.pt'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "outputs": [],
   "source": [
    "x_train_torch = torch.Tensor(list(multimodal_data_train['features']))\n",
    "x_test_torch = torch.Tensor(list(multimodal_data_test['features']))\n",
    "y_train = torch.LongTensor(list(multimodal_data_train['da']))\n",
    "y_test = torch.LongTensor(list(multimodal_data_test['da']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for FeedForward Network :: \n",
      "Test accuracy of Network tensor(0.4835, device='cuda:0')\n",
      "F1 score of Network 0.48348348348348347\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy for FeedForward Network :: ')\n",
    "prepare_for_accuracy(vggishFeedforwardModel,\n",
    "                     test_dataset_tensor=x_test_torch,\n",
    "                     test_target_tensor=y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "### Model 02: A different FFN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "# num_classes = 41\n",
    "num_classes = 40\n",
    "learning_rate = 0.08\n",
    "batch_size = 16\n",
    "num_epochs = 100\n",
    "num_workers = 1\n",
    "input_size = 1000\n",
    "audio_size = 128\n",
    "text_size = 768"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork_2(nn.Module):\n",
    "    def __init__(self, audio_size, text_size, input_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.feedforwardNN_text = nn.Sequential(\n",
    "            nn.Linear(text_size, 500)\n",
    "        )\n",
    "        self.feedforwardNN_audio = nn.Sequential(\n",
    "            nn.Linear(audio_size, 500)\n",
    "        )\n",
    "\n",
    "        self.feedforwardNN = nn.Sequential(\n",
    "        nn.Linear(input_size, 500),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(500, 200),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(200, 100),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(100, num_classes),\n",
    "        nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, text_embds, audio_embds):\n",
    "        text_weights = self.feedforwardNN_text(text_embds)\n",
    "        audio_weights = self.feedforwardNN_audio(audio_embds)\n",
    "        merged = torch.concat((text_weights, audio_weights), dim=1)\n",
    "        return self.feedforwardNN(merged)\n",
    "\n",
    "    def predict(self, text_embds, audio_embds):\n",
    "        text_weights = self.feedforwardNN_text(text_embds)\n",
    "        audio_weights = self.feedforwardNN_audio(audio_embds)\n",
    "        merged = torch.concat((text_weights, audio_weights) ,dim=1)\n",
    "        Y_pred = self.feedforwardNN(merged)\n",
    "        return Y_pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [
    "train_audio_embeddings = list()\n",
    "train_text_embeddings = list()\n",
    "for index, row in cleaned_train_data__with_embeddings.iterrows():\n",
    "    temp2 = list()\n",
    "    with open('temp.npy', encoding='utf-8', mode='w') as npyf:\n",
    "        temp = cleaned_train_data__with_embeddings.iloc[index]['audio_embeddings']\n",
    "        temp = temp.replace('\\n', '')[1: -1]\n",
    "        npyf.write(temp)\n",
    "    with open('temp.npy', encoding='utf-8', mode='r') as npyf:\n",
    "        lines = npyf.readlines()[0].split(' ')\n",
    "        for line in lines:\n",
    "            if len(line) > 0:\n",
    "                temp2.append(float(line))\n",
    "    train_text_embeddings.append(text_aligned_train_features[index])\n",
    "    train_audio_embeddings.append(temp2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [],
   "source": [
    "test_audio_embeddings = list()\n",
    "test_text_embeddings = list()\n",
    "for index, row in cleaned_test_data__with_embeddings.iterrows():\n",
    "    temp2 = list()\n",
    "    with open('temp.npy', encoding='utf-8', mode='w') as npyf:\n",
    "        temp = cleaned_test_data__with_embeddings.iloc[index]['audio_embeddings']\n",
    "        temp = temp.replace('\\n', '')[1: -1]\n",
    "        npyf.write(temp)\n",
    "    with open('temp.npy', encoding='utf-8', mode='r') as npyf:\n",
    "        lines = npyf.readlines()[0].split(' ')\n",
    "        for line in lines:\n",
    "            if len(line) > 0:\n",
    "                temp2.append(float(line))\n",
    "    if row['Label'] == '^g':\n",
    "        continue\n",
    "    test_text_embeddings.append(text_aligned_test_features[index])\n",
    "    test_audio_embeddings.append(temp2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "# x_train_torch_text = torch.Tensor(list(text_aligned_train_features))\n",
    "# x_test_torch_text = torch.Tensor(list(text_aligned_test_features))\n",
    "\n",
    "x_train_torch_text = torch.Tensor(train_text_embeddings)\n",
    "x_test_torch_text = torch.Tensor(test_text_embeddings)\n",
    "\n",
    "x_train_torch_audio = torch.Tensor(train_audio_embeddings)\n",
    "x_test_torch_audio = torch.Tensor(test_audio_embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "data": {
      "text/plain": "   Unnamed: 0.2  Unnamed: 0.1  Unnamed: 0 Channel  \\\n0             0             0       24551       A   \n1             1            12        1375       B   \n2             2            14       28359       B   \n3             3            15        5278       A   \n4             4            20         310       A   \n\n                        Utterance      file       start         end    pitch0  \\\n0  otherwise theyre pretty smelly  4019.txt  220.071750  221.542875  0.058766   \n1          she got the treatments  3057.txt  238.462375  239.773500  0.049801   \n2             this is the reality  2418.txt  207.996125  209.286125  0.091640   \n3     so we kind of looked around  3252.txt  126.830125  128.693375  0.062573   \n4      hes in in florida jail now  3334.txt  156.163125  157.872375  0.136232   \n\n     pitch1    pitch2      fb00      fb01      fb02 Label  \\\n0  0.731051  0.638110  0.899390  0.884808  0.852887    sv   \n1  0.327706  0.288991  0.833201  0.821875  0.723110    sd   \n2  0.485693  0.503992  0.841941  0.762634  0.675776    ^q   \n3  0.717063  0.323278  0.864236  0.876196  0.871295    sd   \n4  0.202110  0.324408  0.822553  0.860201  0.623045    sd   \n\n                             raw_audio_file_loc  \\\n0  Audio\\swb1_LDC97S62\\swb1_d4\\data\\sw04019.sph   \n1  Audio\\swb1_LDC97S62\\swb1_d2\\data\\sw03057.sph   \n2  Audio\\swb1_LDC97S62\\swb1_d1\\data\\sw02418.sph   \n3  Audio\\swb1_LDC97S62\\swb1_d2\\data\\sw03252.sph   \n4  Audio\\swb1_LDC97S62\\swb1_d2\\data\\sw03334.sph   \n\n                                split_audio_file_loc  \\\n0  Audio\\swb1_LDC97S62\\swb1_d4\\data\\sw04019.sph_0...   \n1  Audio\\swb1_LDC97S62\\swb1_d2\\data\\sw03057.sph_1...   \n2  Audio\\swb1_LDC97S62\\swb1_d1\\data\\sw02418.sph_1...   \n3  Audio\\swb1_LDC97S62\\swb1_d2\\data\\sw03252.sph_0...   \n4  Audio\\swb1_LDC97S62\\swb1_d2\\data\\sw03334.sph_0...   \n\n                                    audio_embeddings  da  \n0  [-7.4466288e-01 -5.5393863e-01 -5.2604270e-01 ...  37  \n1  [-0.7556931  -0.18065795 -0.32506776 -0.808879...  36  \n2  [-5.1421905e-01 -5.2362168e-01 -4.5810127e-01 ...   3  \n3  [-0.82808685 -0.5073712  -0.84148455 -0.899432...  36  \n4  [-1.27045846e+00 -5.35477519e-01 -2.04725593e-...  36  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0.2</th>\n      <th>Unnamed: 0.1</th>\n      <th>Unnamed: 0</th>\n      <th>Channel</th>\n      <th>Utterance</th>\n      <th>file</th>\n      <th>start</th>\n      <th>end</th>\n      <th>pitch0</th>\n      <th>pitch1</th>\n      <th>pitch2</th>\n      <th>fb00</th>\n      <th>fb01</th>\n      <th>fb02</th>\n      <th>Label</th>\n      <th>raw_audio_file_loc</th>\n      <th>split_audio_file_loc</th>\n      <th>audio_embeddings</th>\n      <th>da</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>24551</td>\n      <td>A</td>\n      <td>otherwise theyre pretty smelly</td>\n      <td>4019.txt</td>\n      <td>220.071750</td>\n      <td>221.542875</td>\n      <td>0.058766</td>\n      <td>0.731051</td>\n      <td>0.638110</td>\n      <td>0.899390</td>\n      <td>0.884808</td>\n      <td>0.852887</td>\n      <td>sv</td>\n      <td>Audio\\swb1_LDC97S62\\swb1_d4\\data\\sw04019.sph</td>\n      <td>Audio\\swb1_LDC97S62\\swb1_d4\\data\\sw04019.sph_0...</td>\n      <td>[-7.4466288e-01 -5.5393863e-01 -5.2604270e-01 ...</td>\n      <td>37</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>12</td>\n      <td>1375</td>\n      <td>B</td>\n      <td>she got the treatments</td>\n      <td>3057.txt</td>\n      <td>238.462375</td>\n      <td>239.773500</td>\n      <td>0.049801</td>\n      <td>0.327706</td>\n      <td>0.288991</td>\n      <td>0.833201</td>\n      <td>0.821875</td>\n      <td>0.723110</td>\n      <td>sd</td>\n      <td>Audio\\swb1_LDC97S62\\swb1_d2\\data\\sw03057.sph</td>\n      <td>Audio\\swb1_LDC97S62\\swb1_d2\\data\\sw03057.sph_1...</td>\n      <td>[-0.7556931  -0.18065795 -0.32506776 -0.808879...</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>14</td>\n      <td>28359</td>\n      <td>B</td>\n      <td>this is the reality</td>\n      <td>2418.txt</td>\n      <td>207.996125</td>\n      <td>209.286125</td>\n      <td>0.091640</td>\n      <td>0.485693</td>\n      <td>0.503992</td>\n      <td>0.841941</td>\n      <td>0.762634</td>\n      <td>0.675776</td>\n      <td>^q</td>\n      <td>Audio\\swb1_LDC97S62\\swb1_d1\\data\\sw02418.sph</td>\n      <td>Audio\\swb1_LDC97S62\\swb1_d1\\data\\sw02418.sph_1...</td>\n      <td>[-5.1421905e-01 -5.2362168e-01 -4.5810127e-01 ...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>15</td>\n      <td>5278</td>\n      <td>A</td>\n      <td>so we kind of looked around</td>\n      <td>3252.txt</td>\n      <td>126.830125</td>\n      <td>128.693375</td>\n      <td>0.062573</td>\n      <td>0.717063</td>\n      <td>0.323278</td>\n      <td>0.864236</td>\n      <td>0.876196</td>\n      <td>0.871295</td>\n      <td>sd</td>\n      <td>Audio\\swb1_LDC97S62\\swb1_d2\\data\\sw03252.sph</td>\n      <td>Audio\\swb1_LDC97S62\\swb1_d2\\data\\sw03252.sph_0...</td>\n      <td>[-0.82808685 -0.5073712  -0.84148455 -0.899432...</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>20</td>\n      <td>310</td>\n      <td>A</td>\n      <td>hes in in florida jail now</td>\n      <td>3334.txt</td>\n      <td>156.163125</td>\n      <td>157.872375</td>\n      <td>0.136232</td>\n      <td>0.202110</td>\n      <td>0.324408</td>\n      <td>0.822553</td>\n      <td>0.860201</td>\n      <td>0.623045</td>\n      <td>sd</td>\n      <td>Audio\\swb1_LDC97S62\\swb1_d2\\data\\sw03334.sph</td>\n      <td>Audio\\swb1_LDC97S62\\swb1_d2\\data\\sw03334.sph_0...</td>\n      <td>[-1.27045846e+00 -5.35477519e-01 -2.04725593e-...</td>\n      <td>36</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(cleaned_train_data__with_embeddings['Label'])\n",
    "train_encoded_labels = le.transform(cleaned_train_data__with_embeddings['Label'])\n",
    "\n",
    "cleaned_train_data__with_embeddings['da'] = train_encoded_labels\n",
    "\n",
    "cleaned_test_data__with_embeddings = cleaned_test_data__with_embeddings[cleaned_test_data__with_embeddings['Label'] != '^g']\n",
    "test_encoded_labels = le.transform(cleaned_test_data__with_embeddings['Label'])\n",
    "\n",
    "cleaned_test_data__with_embeddings['da'] = test_encoded_labels\n",
    "cleaned_train_data__with_embeddings.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "data": {
      "text/plain": "   Unnamed: 0.2  Unnamed: 0.1  Unnamed: 0 Channel  \\\n0             0             0        7246       A   \n1             1             3       26715       A   \n2             2             4       23466       A   \n3             3             7       30504       A   \n4             4             9        8670       B   \n\n                                      Utterance      file       start  \\\n0                            and one is uh four  3457.txt    5.724875   \n1                 regarding uh taxes i you know  4725.txt    0.650000   \n2  and that was actually after the war was over  2253.txt  291.411000   \n3                   oh well he made it at fifty  2623.txt  577.624750   \n4                     and another thing is cost  4649.txt  135.940125   \n\n          end    pitch0    pitch1    pitch2      fb00      fb01      fb02  \\\n0    7.149125  0.079450  0.745006  0.446980  0.812936  0.790438  0.581930   \n1    1.900000  0.033159  0.370226  0.428816  0.890855  0.886848  0.928268   \n2  292.961000  0.051122  0.643769  0.504700  0.912026  0.881303  0.915515   \n3  579.525250  0.117541  0.304582  0.497606  0.903913  0.885161  0.875586   \n4  137.436000  0.075767  0.416207  0.655162  0.908320  0.937884  0.858942   \n\n             Label                            raw_audio_file_loc  \\\n0               sd  Audio\\swb1_LDC97S62\\swb1_d1\\data\\sw03457.sph   \n1  fo_o_fw_\"_by_bc  Audio\\swb1_LDC97S62\\swb1_d3\\data\\sw04725.sph   \n2               sd  Audio\\swb1_LDC97S62\\swb1_d1\\data\\sw02253.sph   \n3               sd  Audio\\swb1_LDC97S62\\swb1_d2\\data\\sw02623.sph   \n4               sv  Audio\\swb1_LDC97S62\\swb1_d4\\data\\sw04649.sph   \n\n                                split_audio_file_loc  \\\n0  Audio\\swb1_LDC97S62\\swb1_d1\\data\\sw03457.sph_0...   \n1  Audio\\swb1_LDC97S62\\swb1_d3\\data\\sw04725.sph_0...   \n2  Audio\\swb1_LDC97S62\\swb1_d1\\data\\sw02253.sph_0...   \n3  Audio\\swb1_LDC97S62\\swb1_d2\\data\\sw02623.sph_0...   \n4  Audio\\swb1_LDC97S62\\swb1_d4\\data\\sw04649.sph_1...   \n\n                                    audio_embeddings  da  \n0  [-0.8501383  -0.6682229  -0.7700393  -0.899509...  36  \n1  [-0.29266676 -0.39997765 -0.05619848 -0.805616...  19  \n2  [-0.57869494 -0.02115735 -0.22905743 -1.103605...  36  \n3  [-7.7571785e-01 -3.5373911e-01 -3.1975806e-01 ...  36  \n4  [-0.6750778  -0.38885894 -0.5354514  -0.978235...  37  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0.2</th>\n      <th>Unnamed: 0.1</th>\n      <th>Unnamed: 0</th>\n      <th>Channel</th>\n      <th>Utterance</th>\n      <th>file</th>\n      <th>start</th>\n      <th>end</th>\n      <th>pitch0</th>\n      <th>pitch1</th>\n      <th>pitch2</th>\n      <th>fb00</th>\n      <th>fb01</th>\n      <th>fb02</th>\n      <th>Label</th>\n      <th>raw_audio_file_loc</th>\n      <th>split_audio_file_loc</th>\n      <th>audio_embeddings</th>\n      <th>da</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>7246</td>\n      <td>A</td>\n      <td>and one is uh four</td>\n      <td>3457.txt</td>\n      <td>5.724875</td>\n      <td>7.149125</td>\n      <td>0.079450</td>\n      <td>0.745006</td>\n      <td>0.446980</td>\n      <td>0.812936</td>\n      <td>0.790438</td>\n      <td>0.581930</td>\n      <td>sd</td>\n      <td>Audio\\swb1_LDC97S62\\swb1_d1\\data\\sw03457.sph</td>\n      <td>Audio\\swb1_LDC97S62\\swb1_d1\\data\\sw03457.sph_0...</td>\n      <td>[-0.8501383  -0.6682229  -0.7700393  -0.899509...</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>3</td>\n      <td>26715</td>\n      <td>A</td>\n      <td>regarding uh taxes i you know</td>\n      <td>4725.txt</td>\n      <td>0.650000</td>\n      <td>1.900000</td>\n      <td>0.033159</td>\n      <td>0.370226</td>\n      <td>0.428816</td>\n      <td>0.890855</td>\n      <td>0.886848</td>\n      <td>0.928268</td>\n      <td>fo_o_fw_\"_by_bc</td>\n      <td>Audio\\swb1_LDC97S62\\swb1_d3\\data\\sw04725.sph</td>\n      <td>Audio\\swb1_LDC97S62\\swb1_d3\\data\\sw04725.sph_0...</td>\n      <td>[-0.29266676 -0.39997765 -0.05619848 -0.805616...</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>4</td>\n      <td>23466</td>\n      <td>A</td>\n      <td>and that was actually after the war was over</td>\n      <td>2253.txt</td>\n      <td>291.411000</td>\n      <td>292.961000</td>\n      <td>0.051122</td>\n      <td>0.643769</td>\n      <td>0.504700</td>\n      <td>0.912026</td>\n      <td>0.881303</td>\n      <td>0.915515</td>\n      <td>sd</td>\n      <td>Audio\\swb1_LDC97S62\\swb1_d1\\data\\sw02253.sph</td>\n      <td>Audio\\swb1_LDC97S62\\swb1_d1\\data\\sw02253.sph_0...</td>\n      <td>[-0.57869494 -0.02115735 -0.22905743 -1.103605...</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>7</td>\n      <td>30504</td>\n      <td>A</td>\n      <td>oh well he made it at fifty</td>\n      <td>2623.txt</td>\n      <td>577.624750</td>\n      <td>579.525250</td>\n      <td>0.117541</td>\n      <td>0.304582</td>\n      <td>0.497606</td>\n      <td>0.903913</td>\n      <td>0.885161</td>\n      <td>0.875586</td>\n      <td>sd</td>\n      <td>Audio\\swb1_LDC97S62\\swb1_d2\\data\\sw02623.sph</td>\n      <td>Audio\\swb1_LDC97S62\\swb1_d2\\data\\sw02623.sph_0...</td>\n      <td>[-7.7571785e-01 -3.5373911e-01 -3.1975806e-01 ...</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>9</td>\n      <td>8670</td>\n      <td>B</td>\n      <td>and another thing is cost</td>\n      <td>4649.txt</td>\n      <td>135.940125</td>\n      <td>137.436000</td>\n      <td>0.075767</td>\n      <td>0.416207</td>\n      <td>0.655162</td>\n      <td>0.908320</td>\n      <td>0.937884</td>\n      <td>0.858942</td>\n      <td>sv</td>\n      <td>Audio\\swb1_LDC97S62\\swb1_d4\\data\\sw04649.sph</td>\n      <td>Audio\\swb1_LDC97S62\\swb1_d4\\data\\sw04649.sph_1...</td>\n      <td>[-0.6750778  -0.38885894 -0.5354514  -0.978235...</td>\n      <td>37</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_test_data__with_embeddings.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [],
   "source": [
    "y_train = torch.LongTensor(list(cleaned_train_data__with_embeddings['da']))\n",
    "y_test = torch.LongTensor(list(cleaned_test_data__with_embeddings['da']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [],
   "source": [
    "zipper = lambda x,y,z : list(zip(x,y,z))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "x_train_torch = zipper(x_train_torch_text, x_train_torch_audio, y_train)\n",
    "x_test_torch = zipper(x_test_torch_text, x_test_torch_audio, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(x_train_torch, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(x_test_torch, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "feedforwardModel_multi = FeedForwardNeuralNetwork_2(\n",
    "    audio_size=audio_size,\n",
    "    text_size=text_size,\n",
    "    input_size = input_size,\n",
    "    num_classes = num_classes).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [],
   "source": [
    "# Specify criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(feedforwardModel_multi.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [],
   "source": [
    "# Function to get accuracy scores\n",
    "def accuracy(y_pred, y_test):\n",
    "    pred = torch.argmax(y_pred, dim=1)\n",
    "    return (pred == y_test).float().mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [],
   "source": [
    "### Training Feedforward NN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [],
   "source": [
    "# Start Training\n",
    "def train_FF_Multi(model, num_epochs, train_loader, test_loader, modelName):\n",
    "    test_loss_min = np.Inf\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        train_loss = 0.0\n",
    "        test_loss = 0.0\n",
    "\n",
    "        # Prepare the model for training\n",
    "        model.train()\n",
    "\n",
    "        for text_data, audio_data, target in train_loader:\n",
    "            text_data = text_data.to(device=device)\n",
    "            audio_data = audio_data.to(device=device)\n",
    "            target = target.type(torch.LongTensor).to(device=device)\n",
    "\n",
    "            text_data = text_data.reshape(text_data.shape[0], -1)\n",
    "            audio_data = audio_data.reshape(audio_data.shape[0], -1)\n",
    "\n",
    "            # forward pass: compute predicted outputs by passing\n",
    "            # inputs to the model\n",
    "            scores = model(text_data, audio_data)\n",
    "\n",
    "            # calculate the loss\n",
    "            loss = criterion(scores, target)\n",
    "\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # backward pass: compute gradient of the loss\n",
    "            # with respect to model parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "\n",
    "            # train_loss += loss.item()*data.size(0)\n",
    "            train_loss += loss.item() * 16 # Batch size\n",
    "\n",
    "        # Prepare model for evaluation\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for text_data, audio_data, target in test_loader:\n",
    "                text_data = text_data.to(device=device)\n",
    "                audio_data = audio_data.to(device=device)\n",
    "                target = target.type(torch.LongTensor).to(device=device)\n",
    "\n",
    "                scores = model(text_data, audio_data)\n",
    "                loss = criterion(scores, target)\n",
    "\n",
    "                # test_loss += loss.item()*data.size(0)\n",
    "                test_loss += loss.item() * 16 # Batch size\n",
    "\n",
    "        train_loss = train_loss/len(train_loader.dataset)\n",
    "        test_loss = test_loss/len(test_loader.dataset)\n",
    "\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tTest Loss: {:.6f}'.format(\n",
    "            epoch+1,\n",
    "            train_loss,\n",
    "            test_loss\n",
    "            ))\n",
    "\n",
    "        # save model if validation loss has decreased\n",
    "        if test_loss <= test_loss_min:\n",
    "            print('Test loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            test_loss_min,\n",
    "            test_loss))\n",
    "            torch.save(model.state_dict(), modelName + '.pt')\n",
    "            test_loss_min = test_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 3.572009 \tTest Loss: 3.276504\n",
      "Test loss decreased (inf --> 3.276504).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 3.223663 \tTest Loss: 3.276261\n",
      "Test loss decreased (3.276504 --> 3.276261).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 3.223590 \tTest Loss: 3.276239\n",
      "Test loss decreased (3.276261 --> 3.276239).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 3.223577 \tTest Loss: 3.276232\n",
      "Test loss decreased (3.276239 --> 3.276232).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 3.223572 \tTest Loss: 3.276228\n",
      "Test loss decreased (3.276232 --> 3.276228).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 3.223568 \tTest Loss: 3.276226\n",
      "Test loss decreased (3.276228 --> 3.276226).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 3.223567 \tTest Loss: 3.276224\n",
      "Test loss decreased (3.276226 --> 3.276224).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 3.223565 \tTest Loss: 3.276223\n",
      "Test loss decreased (3.276224 --> 3.276223).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 3.223564 \tTest Loss: 3.276222\n",
      "Test loss decreased (3.276223 --> 3.276222).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 3.223564 \tTest Loss: 3.276222\n",
      "Test loss decreased (3.276222 --> 3.276222).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 3.223563 \tTest Loss: 3.276221\n",
      "Test loss decreased (3.276222 --> 3.276221).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 3.223563 \tTest Loss: 3.276221\n",
      "Test loss decreased (3.276221 --> 3.276221).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 3.223562 \tTest Loss: 3.276220\n",
      "Test loss decreased (3.276221 --> 3.276220).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 3.223562 \tTest Loss: 3.276220\n",
      "Test loss decreased (3.276220 --> 3.276220).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 3.223562 \tTest Loss: 3.276220\n",
      "Test loss decreased (3.276220 --> 3.276220).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 3.223561 \tTest Loss: 3.276220\n",
      "Test loss decreased (3.276220 --> 3.276220).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 3.223561 \tTest Loss: 3.276219\n",
      "Test loss decreased (3.276220 --> 3.276219).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 3.223561 \tTest Loss: 3.276219\n",
      "Test loss decreased (3.276219 --> 3.276219).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 3.223561 \tTest Loss: 3.276219\n",
      "Test loss decreased (3.276219 --> 3.276219).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 3.223561 \tTest Loss: 3.276219\n",
      "Test loss decreased (3.276219 --> 3.276219).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 3.223561 \tTest Loss: 3.276219\n",
      "Test loss decreased (3.276219 --> 3.276219).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 3.223561 \tTest Loss: 3.276219\n",
      "Test loss decreased (3.276219 --> 3.276219).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 3.223560 \tTest Loss: 3.276219\n",
      "Test loss decreased (3.276219 --> 3.276219).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 3.223560 \tTest Loss: 3.276219\n",
      "Test loss decreased (3.276219 --> 3.276219).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 3.223560 \tTest Loss: 3.276219\n",
      "Test loss decreased (3.276219 --> 3.276219).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 3.223560 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276219 --> 3.276218).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 3.223560 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 3.223560 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 3.223560 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 3.223560 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 3.223560 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 3.223560 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 3.223560 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 3.223560 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 3.223560 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 3.223560 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 3.223560 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 3.223560 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 3.223560 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 3.223560 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 3.223559 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 3.223559 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 3.223559 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 3.223559 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 45 \tTraining Loss: 3.223559 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 3.223559 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 3.223559 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 3.223559 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 3.223559 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 50 \tTraining Loss: 3.223559 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 51 \tTraining Loss: 3.223559 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 52 \tTraining Loss: 3.223559 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 53 \tTraining Loss: 3.223559 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 54 \tTraining Loss: 3.223559 \tTest Loss: 3.276218\n",
      "Epoch: 55 \tTraining Loss: 3.223559 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 56 \tTraining Loss: 3.223559 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 57 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276218 --> 3.276217).  Saving model ...\n",
      "Epoch: 58 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 59 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 60 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 61 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Epoch: 62 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 63 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 64 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 65 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 66 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Epoch: 67 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 68 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 69 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 70 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 71 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 72 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 73 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 74 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 75 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 76 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 77 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 78 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 79 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Epoch: 80 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Epoch: 81 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 82 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 83 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 84 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 85 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 86 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 87 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 88 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 89 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 90 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 91 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 92 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 93 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 94 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 95 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 96 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 97 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 98 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 99 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 100 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "train_FF_Multi(feedforwardModel_multi, num_epochs=num_epochs, train_loader=train_loader, test_loader=test_loader, modelName='feedforwardModel__Multi')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model with best test loss\n",
    "feedforwardModel_multi.load_state_dict(torch.load('feedforwardModel__Multi.pt'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [],
   "source": [
    "# Reference:\n",
    "# https://www.kaggle.com/code/tauseef6462/simple-feedforward-neural-network-using-pytorch\n",
    "def prepare_for_accuracy(model,\n",
    "                         test_dataset_tensor_text,\n",
    "                         test_dataset_tensor_audio,\n",
    "                         test_target_tensor):\n",
    "    test_dataset_tensor_text = test_dataset_tensor_text.to(device=device)\n",
    "    test_dataset_tensor_audio = test_dataset_tensor_audio.to(device=device)\n",
    "    Y_pred_test = model.predict(test_dataset_tensor_text, test_dataset_tensor_audio)\n",
    "\n",
    "    Y_pred_test = Y_pred_test.to(device=device)\n",
    "    test_target_tensor = test_target_tensor.to(device=device)\n",
    "    accuracy_test = accuracy(Y_pred_test, test_target_tensor)\n",
    "    f1_score_val = f1_score_NN(Y_pred_test, test_target_tensor)\n",
    "    print(\"Test accuracy of Network\",(accuracy_test))\n",
    "    print(\"F1 score of Network\",(f1_score_val))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [],
   "source": [
    "# x_test_torch = torch.Tensor(list(multimodal_data_test['features']))\n",
    "# y_test = torch.LongTensor(list(multimodal_data_test['da']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for FeedForward Network :: \n",
      "Test accuracy of Network tensor(0.4835, device='cuda:0')\n",
      "F1 score of Network 0.48348348348348347\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy for FeedForward Network :: ')\n",
    "prepare_for_accuracy(feedforwardModel_multi,\n",
    "                     test_dataset_tensor_audio=x_test_torch_audio,\n",
    "                     test_dataset_tensor_text=x_test_torch_text,\n",
    "                     test_target_tensor=y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [],
   "source": [
    "## With audio features such as pitch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "# num_classes = 41\n",
    "num_classes = 40\n",
    "learning_rate = 0.08\n",
    "batch_size = 16\n",
    "num_epochs = 100\n",
    "num_workers = 1\n",
    "input_size = 506\n",
    "audio_size = 6\n",
    "text_size = 768"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork_2(nn.Module):\n",
    "    def __init__(self, audio_size, text_size, input_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.feedforwardNN_text = nn.Sequential(\n",
    "            nn.Linear(text_size, 500)\n",
    "        )\n",
    "        self.feedforwardNN_audio = nn.Sequential(\n",
    "            nn.Linear(audio_size, 6)\n",
    "        )\n",
    "\n",
    "        self.feedforwardNN = nn.Sequential(\n",
    "        nn.Linear(input_size, 500),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(500, 200),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(200, 100),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(100, num_classes),\n",
    "        nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, text_embds, audio_embds):\n",
    "        text_weights = self.feedforwardNN_text(text_embds)\n",
    "        audio_weights = self.feedforwardNN_audio(audio_embds)\n",
    "        merged = torch.concat((text_weights, audio_weights), dim=1)\n",
    "        return self.feedforwardNN(merged)\n",
    "\n",
    "    def predict(self, text_embds, audio_embds):\n",
    "        text_weights = self.feedforwardNN_text(text_embds)\n",
    "        audio_weights = self.feedforwardNN_audio(audio_embds)\n",
    "        merged = torch.concat((text_weights, audio_weights) ,dim=1)\n",
    "        Y_pred = self.feedforwardNN(merged)\n",
    "        return Y_pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [],
   "source": [
    "train_audio_embeddings = list()\n",
    "train_text_embeddings = list()\n",
    "for index, row in cleaned_train_data__with_embeddings.iterrows():\n",
    "    train_text_embeddings.append(text_aligned_train_features[index])\n",
    "    train_audio_embeddings.append(cleaned_train_data__with_embeddings.iloc[index][8:14].to_numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [],
   "source": [
    "test_audio_embeddings = list()\n",
    "test_text_embeddings = list()\n",
    "for index, row in cleaned_test_data__with_embeddings.iterrows():\n",
    "    if row['Label'] == '^g':\n",
    "        continue\n",
    "    test_text_embeddings.append(text_aligned_test_features[index])\n",
    "    test_audio_embeddings.append(row[8: 14].to_numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [],
   "source": [
    "# x_train_torch_text = torch.Tensor(list(text_aligned_train_features))\n",
    "# x_test_torch_text = torch.Tensor(list(text_aligned_test_features))\n",
    "\n",
    "x_train_torch_text = torch.Tensor(train_text_embeddings)\n",
    "x_test_torch_text = torch.Tensor(test_text_embeddings)\n",
    "\n",
    "x_train_torch_audio = torch.Tensor(train_audio_embeddings)\n",
    "x_test_torch_audio = torch.Tensor(test_audio_embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [],
   "source": [
    "y_train = torch.LongTensor(list(cleaned_train_data__with_embeddings['da']))\n",
    "y_test = torch.LongTensor(list(cleaned_test_data__with_embeddings['da']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [],
   "source": [
    "zipper = lambda x,y,z : list(zip(x,y,z))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [],
   "source": [
    "x_train_torch = zipper(x_train_torch_text, x_train_torch_audio, y_train)\n",
    "x_test_torch = zipper(x_test_torch_text, x_test_torch_audio, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(x_train_torch, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(x_test_torch, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "feedforwardModel_multi = FeedForwardNeuralNetwork_2(\n",
    "    audio_size=audio_size,\n",
    "    text_size=text_size,\n",
    "    input_size = input_size,\n",
    "    num_classes = num_classes).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [],
   "source": [
    "# Specify criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(feedforwardModel_multi.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [],
   "source": [
    "# Function to get accuracy scores\n",
    "def accuracy(y_pred, y_test):\n",
    "    pred = torch.argmax(y_pred, dim=1)\n",
    "    return (pred == y_test).float().mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 3.505320 \tTest Loss: 3.276381\n",
      "Test loss decreased (inf --> 3.276381).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 3.223637 \tTest Loss: 3.276253\n",
      "Test loss decreased (3.276381 --> 3.276253).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 3.223588 \tTest Loss: 3.276236\n",
      "Test loss decreased (3.276253 --> 3.276236).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 3.223576 \tTest Loss: 3.276230\n",
      "Test loss decreased (3.276236 --> 3.276230).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 3.223571 \tTest Loss: 3.276226\n",
      "Test loss decreased (3.276230 --> 3.276226).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 3.223568 \tTest Loss: 3.276224\n",
      "Test loss decreased (3.276226 --> 3.276224).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 3.223566 \tTest Loss: 3.276223\n",
      "Test loss decreased (3.276224 --> 3.276223).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 3.223565 \tTest Loss: 3.276222\n",
      "Test loss decreased (3.276223 --> 3.276222).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 3.223564 \tTest Loss: 3.276221\n",
      "Test loss decreased (3.276222 --> 3.276221).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 3.223564 \tTest Loss: 3.276221\n",
      "Test loss decreased (3.276221 --> 3.276221).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 3.223563 \tTest Loss: 3.276220\n",
      "Test loss decreased (3.276221 --> 3.276220).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 3.223563 \tTest Loss: 3.276220\n",
      "Test loss decreased (3.276220 --> 3.276220).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 3.223562 \tTest Loss: 3.276220\n",
      "Test loss decreased (3.276220 --> 3.276220).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 3.223562 \tTest Loss: 3.276219\n",
      "Test loss decreased (3.276220 --> 3.276219).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 3.223562 \tTest Loss: 3.276219\n",
      "Test loss decreased (3.276219 --> 3.276219).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 3.223561 \tTest Loss: 3.276219\n",
      "Test loss decreased (3.276219 --> 3.276219).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 3.223561 \tTest Loss: 3.276219\n",
      "Test loss decreased (3.276219 --> 3.276219).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 3.223561 \tTest Loss: 3.276219\n",
      "Test loss decreased (3.276219 --> 3.276219).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 3.223561 \tTest Loss: 3.276219\n",
      "Test loss decreased (3.276219 --> 3.276219).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 3.223561 \tTest Loss: 3.276219\n",
      "Test loss decreased (3.276219 --> 3.276219).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 3.223561 \tTest Loss: 3.276219\n",
      "Test loss decreased (3.276219 --> 3.276219).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 3.223561 \tTest Loss: 3.276219\n",
      "Test loss decreased (3.276219 --> 3.276219).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 3.223561 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276219 --> 3.276218).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 3.223560 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 3.223560 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 3.223560 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 3.223560 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 3.223560 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 3.223560 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 3.223560 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 3.223560 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 3.223560 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 3.223560 \tTest Loss: 3.276218\n",
      "Epoch: 34 \tTraining Loss: 3.223560 \tTest Loss: 3.276218\n",
      "Epoch: 35 \tTraining Loss: 3.223560 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 3.223560 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 3.223560 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 3.223560 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 3.223560 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 3.223560 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 3.223560 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 3.223560 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 3.223560 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 3.223560 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 45 \tTraining Loss: 3.223560 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 3.223559 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 3.223559 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 3.223559 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 3.223559 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 50 \tTraining Loss: 3.223559 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 51 \tTraining Loss: 3.223559 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 52 \tTraining Loss: 3.223559 \tTest Loss: 3.276218\n",
      "Test loss decreased (3.276218 --> 3.276218).  Saving model ...\n",
      "Epoch: 53 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276218 --> 3.276217).  Saving model ...\n",
      "Epoch: 54 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 55 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 56 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 57 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 58 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 59 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 60 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 61 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 62 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 63 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 64 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Epoch: 65 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 66 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 67 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Epoch: 68 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Epoch: 69 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Epoch: 70 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Epoch: 71 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Epoch: 72 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 73 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 74 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 75 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 76 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 77 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 78 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 79 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 80 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 81 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 82 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 83 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 84 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 85 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Epoch: 86 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 87 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 88 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 89 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 90 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n",
      "Epoch: 91 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Epoch: 92 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Epoch: 93 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Epoch: 94 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Epoch: 95 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Epoch: 96 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Epoch: 97 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Epoch: 98 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Epoch: 99 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Epoch: 100 \tTraining Loss: 3.223559 \tTest Loss: 3.276217\n",
      "Test loss decreased (3.276217 --> 3.276217).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "train_FF_Multi(feedforwardModel_multi, num_epochs=num_epochs, train_loader=train_loader, test_loader=test_loader, modelName='feedforwardModel__Multi_pitch')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model with best test loss\n",
    "feedforwardModel_multi.load_state_dict(torch.load('feedforwardModel__Multi_pitch.pt'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for FeedForward Network :: \n",
      "Test accuracy of Network tensor(0.4835, device='cuda:0')\n",
      "F1 score of Network 0.48348348348348347\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy for FeedForward Network :: ')\n",
    "prepare_for_accuracy(feedforwardModel_multi,\n",
    "                     test_dataset_tensor_audio=x_test_torch_audio,\n",
    "                     test_dataset_tensor_text=x_test_torch_text,\n",
    "                     test_target_tensor=y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [],
   "source": [
    "### Late Fusion"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 5.329866  ,  0.2828937 , -1.5951992 , ...,  4.385116  ,\n        -1.2062347 , -0.7415899 ],\n       [ 3.1725254 ,  0.7356104 , -1.9743685 , ...,  3.4874892 ,\n        -1.576004  , -0.4736683 ],\n       [-0.52005005,  1.0725409 , -1.5971717 , ...,  5.938665  ,\n        -1.2389355 , -1.245855  ],\n       ...,\n       [-0.11853296,  1.2770503 , -1.637255  , ...,  4.3278975 ,\n        -1.4869914 , -0.7658975 ],\n       [ 4.426511  ,  2.9620278 , -1.4006051 , ...,  6.0137463 ,\n        -1.6051922 , -0.77653944],\n       [ 3.9398794 , -1.5151525 , -2.057766  , ...,  5.8977346 ,\n        -1.2377753 , -0.78777707]], dtype=float32)"
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_probs = np.load('test_audio_probabilities_text.npy')\n",
    "text_probs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 5.329866  ,  0.2828937 , -0.43800756, ...,  4.385116  ,\n        -1.2062347 , -0.7415899 ],\n       [ 3.1725254 ,  0.7356104 ,  0.37866458, ...,  3.4874892 ,\n        -1.576004  , -0.4736683 ],\n       [-0.52005005,  1.0725409 ,  0.813379  , ...,  5.938665  ,\n        -1.2389355 , -1.245855  ],\n       ...,\n       [-0.11853296,  1.2770503 , -0.11502191, ...,  4.3278975 ,\n        -1.4869914 , -0.7658975 ],\n       [ 4.426511  ,  2.9620278 ,  0.2917903 , ...,  6.0137463 ,\n        -1.6051922 , -0.77653944],\n       [ 3.9398794 , -1.5151525 ,  1.1465577 , ...,  5.8977346 ,\n        -1.2377753 , -0.78777707]], dtype=float32)"
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_probs = np.delete(text_probs, 2, axis=1)\n",
    "text_probs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.06666667, 0.        , 0.        , ..., 0.04444444, 0.        ,\n        0.        ],\n       [0.08888889, 0.02222222, 0.        , ..., 0.11111111, 0.        ,\n        0.        ],\n       [0.06666667, 0.02222222, 0.        , ..., 0.11111111, 0.        ,\n        0.        ],\n       ...,\n       [0.08888889, 0.        , 0.        , ..., 0.11111111, 0.        ,\n        0.        ],\n       [0.02222222, 0.        , 0.        , ..., 0.15555556, 0.        ,\n        0.        ],\n       [0.02222222, 0.04444444, 0.        , ..., 0.08888889, 0.        ,\n        0.        ]])"
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_probs = pd.read_csv('test_audio_probabilities_audio.csv', header=None).to_numpy()\n",
    "audio_probs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 5.3965326 ,  0.28289369, -0.43800756, ...,  4.42956054,\n        -1.20623469, -0.7415899 ],\n       [ 3.26141429,  0.75783265,  0.37866458, ...,  3.59860033,\n        -1.57600403, -0.47366831],\n       [-0.45338338,  1.0947631 ,  0.81337899, ...,  6.04977602,\n        -1.23893547, -1.24585497],\n       ...,\n       [-0.02964407,  1.27705026, -0.11502191, ...,  4.43900866,\n        -1.48699141, -0.76589751],\n       [ 4.44873303,  2.96202779,  0.29179031, ...,  6.16930182,\n        -1.60519218, -0.77653944],\n       [ 3.96210164, -1.47070801,  1.14655769, ...,  5.98662353,\n        -1.23777533, -0.78777707]])"
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "late_fusion = text_probs + audio_probs\n",
    "late_fusion"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [
    {
     "data": {
      "text/plain": "array([36, 36, 36, 36, 36, 11, 36, 36, 36, 36, 36, 22, 37, 36, 36, 36, 36,\n       36, 37, 34, 36, 36,  4, 36, 36, 36,  0, 11,  0, 36, 36, 11, 36, 36,\n       37, 37, 37, 36, 36, 36, 36,  0, 36, 36, 36, 36, 34, 36, 36, 36, 36,\n       36, 36, 36, 36, 11, 36, 37, 36, 36, 36, 36, 32, 36, 36, 36, 36, 36,\n       36,  0, 36, 32, 36, 36, 36,  0, 36, 30,  4, 36, 36, 36, 36, 36, 36,\n       36, 11,  0,  0, 37, 36, 36, 36, 15, 36, 37, 36, 36, 36, 37, 30, 36,\n       34, 15, 36, 36, 36, 36, 36, 36, 37, 36, 36, 34, 36, 34,  0,  0, 36,\n       14, 34, 36, 36, 36,  9, 36, 36, 36, 36, 32, 36, 36, 34, 36, 32,  9,\n       34, 36, 34, 36, 36, 37, 36,  4, 36, 34, 36, 32, 37,  0, 37,  0, 36,\n       37, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 37,  0,  9, 36, 36, 34,\n       36, 36, 36, 36, 36, 37, 32, 36, 36, 36, 36, 36,  0, 36, 36, 34, 36,\n       36, 37, 36, 36,  0, 36, 37, 36, 36, 37, 36, 36, 37, 27, 36, 36, 36,\n       36, 34, 36, 36, 36, 36, 37, 36, 36, 36, 36, 36, 36, 36, 36, 36, 18,\n       18, 36, 36, 36,  0, 36, 36, 11, 36,  9, 36, 34, 11, 37,  0, 36, 36,\n       36, 37, 34, 36, 34, 36, 37, 36, 37, 37, 36, 36, 32, 36, 36, 36, 37,\n       36, 36, 34, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36,  9, 34, 32,\n       36, 36, 36, 36, 36, 36,  9, 36,  0, 32, 36, 36, 36, 37, 37, 36, 36,\n       36, 36, 37,  0, 36, 11, 36, 36, 32, 36, 25, 36, 36, 36, 37, 36, 36,\n       36, 36, 36,  4, 36, 36, 31, 36, 36, 34, 36, 36, 36, 36, 36, 36, 36,\n       36, 34, 36, 18, 36, 36, 36, 36, 36, 37], dtype=int64)"
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds = np.argmax(late_fusion, axis=1)\n",
    "test_preds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [
    {
     "data": {
      "text/plain": "0      36\n1      19\n2      36\n3      36\n4      37\n       ..\n329    37\n330    36\n331    36\n332    36\n333     0\nName: da, Length: 333, dtype: int32"
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_test_labels = cleaned_test_data__with_embeddings['da']\n",
    "real_test_labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [
    {
     "data": {
      "text/plain": "0.6636636636636637"
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(real_test_labels, test_preds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "outputs": [
    {
     "data": {
      "text/plain": "0.6636636636636637"
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(real_test_labels, test_preds, average='micro')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [
    {
     "data": {
      "text/plain": "              precision    recall  f1-score     support\n0              0.500000  0.473684  0.486486   19.000000\n1              0.000000  0.000000  0.000000    1.000000\n2              0.000000  0.000000  0.000000    2.000000\n3              0.000000  0.000000  0.000000    5.000000\n4              0.500000  0.400000  0.444444    5.000000\n6              0.000000  0.000000  0.000000    4.000000\n9              0.666667  0.666667  0.666667    6.000000\n10             0.000000  0.000000  0.000000    1.000000\n11             0.750000  0.666667  0.705882    9.000000\n13             0.000000  0.000000  0.000000    4.000000\n14             0.000000  0.000000  0.000000    2.000000\n15             0.500000  0.500000  0.500000    2.000000\n17             0.000000  0.000000  0.000000    1.000000\n18             0.666667  0.666667  0.666667    3.000000\n19             0.000000  0.000000  0.000000    2.000000\n20             0.000000  0.000000  0.000000    1.000000\n22             1.000000  0.333333  0.500000    3.000000\n23             0.000000  0.000000  0.000000    5.000000\n24             0.000000  0.000000  0.000000    3.000000\n25             0.000000  0.000000  0.000000    0.000000\n26             0.000000  0.000000  0.000000    1.000000\n27             1.000000  1.000000  1.000000    1.000000\n29             0.000000  0.000000  0.000000    1.000000\n30             0.500000  0.500000  0.500000    2.000000\n31             0.000000  0.000000  0.000000    0.000000\n32             0.900000  0.900000  0.900000   10.000000\n34             0.650000  0.722222  0.684211   18.000000\n35             0.000000  0.000000  0.000000    3.000000\n36             0.668161  0.925466  0.776042  161.000000\n37             0.718750  0.403509  0.516854   57.000000\n38             0.000000  0.000000  0.000000    1.000000\naccuracy       0.663664  0.663664  0.663664    0.663664\nmacro avg      0.290976  0.263168  0.269266  333.000000\nweighted avg   0.600579  0.663664  0.612725  333.000000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>precision</th>\n      <th>recall</th>\n      <th>f1-score</th>\n      <th>support</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.500000</td>\n      <td>0.473684</td>\n      <td>0.486486</td>\n      <td>19.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>5.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.500000</td>\n      <td>0.400000</td>\n      <td>0.444444</td>\n      <td>5.000000</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>4.000000</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.666667</td>\n      <td>0.666667</td>\n      <td>0.666667</td>\n      <td>6.000000</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.750000</td>\n      <td>0.666667</td>\n      <td>0.705882</td>\n      <td>9.000000</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>4.000000</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.666667</td>\n      <td>0.666667</td>\n      <td>0.666667</td>\n      <td>3.000000</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>1.000000</td>\n      <td>0.333333</td>\n      <td>0.500000</td>\n      <td>3.000000</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>5.000000</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>3.000000</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>0.900000</td>\n      <td>0.900000</td>\n      <td>0.900000</td>\n      <td>10.000000</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>0.650000</td>\n      <td>0.722222</td>\n      <td>0.684211</td>\n      <td>18.000000</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>3.000000</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>0.668161</td>\n      <td>0.925466</td>\n      <td>0.776042</td>\n      <td>161.000000</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>0.718750</td>\n      <td>0.403509</td>\n      <td>0.516854</td>\n      <td>57.000000</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>accuracy</th>\n      <td>0.663664</td>\n      <td>0.663664</td>\n      <td>0.663664</td>\n      <td>0.663664</td>\n    </tr>\n    <tr>\n      <th>macro avg</th>\n      <td>0.290976</td>\n      <td>0.263168</td>\n      <td>0.269266</td>\n      <td>333.000000</td>\n    </tr>\n    <tr>\n      <th>weighted avg</th>\n      <td>0.600579</td>\n      <td>0.663664</td>\n      <td>0.612725</td>\n      <td>333.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = classification_report(real_test_labels, test_preds, output_dict=True)\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "report_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}