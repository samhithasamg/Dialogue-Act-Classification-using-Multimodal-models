{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e5b6e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/geetshingi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/geetshingi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/geetshingi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sklearn as sk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import platform \n",
    "import torch\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf0bc063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Platform: macOS-13.2.1-arm64-arm-64bit\n",
      "PyTorch Version: 2.0.0.dev20230212\n",
      "\n",
      "Python 3.9.16 | packaged by conda-forge | (main, Feb  1 2023, 21:38:11) \n",
      "[Clang 14.0.6 ]\n",
      "Pandas 1.5.3\n",
      "Scikit-Learn 1.2.1\n",
      "GPU is NOT AVAILABLE\n",
      "MPS (Apple Metal) is AVAILABLE\n",
      "Target device is mps\n"
     ]
    }
   ],
   "source": [
    "has_gpu = torch.cuda.is_available()\n",
    "has_mps = getattr(torch,'has_mps',False)\n",
    "device = \"mps\" if getattr(torch,'has_mps',False) \\\n",
    "    else \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Python Platform: {platform.platform()}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "print(\"GPU is\", \"available\" if has_gpu else \"NOT AVAILABLE\")\n",
    "print(\"MPS (Apple Metal) is\", \"AVAILABLE\" if has_mps else \"NOT AVAILABLE\")\n",
    "print(f\"Target device is {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b311663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('text_data_nathan.csv', usecols = ['Utterance', 'Label'])\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c05e433f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>otherwise theyre pretty smelly</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>she got the treatments</td>\n",
       "      <td>sd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this is the reality</td>\n",
       "      <td>^q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>so we kind of looked around</td>\n",
       "      <td>sd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hes in in florida jail now</td>\n",
       "      <td>sd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Utterance Label\n",
       "0  otherwise theyre pretty smelly    sv\n",
       "1          she got the treatments    sd\n",
       "2             this is the reality    ^q\n",
       "3     so we kind of looked around    sd\n",
       "4      hes in in florida jail now    sd"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('train_features_audio_aligned__with_wav_files__embeddings.csv', usecols = ['Utterance', 'Label'])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "606a16b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6305, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd8f4b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and one is uh four</td>\n",
       "      <td>sd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>regarding uh taxes i you know</td>\n",
       "      <td>fo_o_fw_\"_by_bc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and that was actually after the war was over</td>\n",
       "      <td>sd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oh well he made it at fifty</td>\n",
       "      <td>sd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and another thing is cost</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Utterance            Label\n",
       "0                            and one is uh four               sd\n",
       "1                 regarding uh taxes i you know  fo_o_fw_\"_by_bc\n",
       "2  and that was actually after the war was over               sd\n",
       "3                   oh well he made it at fifty               sd\n",
       "4                     and another thing is cost               sv"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('test_features_audio_aligned__with_wav_files__embeddings.csv', usecols = ['Utterance', 'Label'])\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "053eb375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and one is uh four</td>\n",
       "      <td>sd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>regarding uh taxes i you know</td>\n",
       "      <td>fo_o_fw_\"_by_bc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and that was actually after the war was over</td>\n",
       "      <td>sd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oh well he made it at fifty</td>\n",
       "      <td>sd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and another thing is cost</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Utterance            Label\n",
       "0                            and one is uh four               sd\n",
       "1                 regarding uh taxes i you know  fo_o_fw_\"_by_bc\n",
       "2  and that was actually after the war was over               sd\n",
       "3                   oh well he made it at fifty               sd\n",
       "4                     and another thing is cost               sv"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('test_features_audio_aligned__with_wav_files__embeddings.csv', usecols = ['Utterance', 'Label'])\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "599ae88d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6305, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "06ca6ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(333, 2)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0ed54dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df[test_df['Label'] != '^g']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c12d2a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Utterance'] = df['Utterance'].str.lower()\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a23658d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>otherwise theyre pretty smelly</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>she got the treatments</td>\n",
       "      <td>sd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this is the reality</td>\n",
       "      <td>^q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>so we kind of looked around</td>\n",
       "      <td>sd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hes in in florida jail now</td>\n",
       "      <td>sd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Utterance Label\n",
       "0  otherwise theyre pretty smelly    sv\n",
       "1          she got the treatments    sd\n",
       "2             this is the reality    ^q\n",
       "3     so we kind of looked around    sd\n",
       "4      hes in in florida jail now    sd"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Utterance'] = train_df['Utterance'].str.lower()\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ee7a24f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and one is uh four</td>\n",
       "      <td>sd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>regarding uh taxes i you know</td>\n",
       "      <td>fo_o_fw_\"_by_bc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and that was actually after the war was over</td>\n",
       "      <td>sd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oh well he made it at fifty</td>\n",
       "      <td>sd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and another thing is cost</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Utterance            Label\n",
       "0                            and one is uh four               sd\n",
       "1                 regarding uh taxes i you know  fo_o_fw_\"_by_bc\n",
       "2  and that was actually after the war was over               sd\n",
       "3                   oh well he made it at fifty               sd\n",
       "4                     and another thing is cost               sv"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['Utterance'] = test_df['Utterance'].str.lower()\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "771fc4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(x):\n",
    "    return BeautifulSoup(x,'html.parser').get_text(separator=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a70d2197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Utterance'] = df['Utterance'].apply(lambda x: remove_html(x))\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85de6a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>otherwise theyre pretty smelly</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>she got the treatments</td>\n",
       "      <td>sd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this is the reality</td>\n",
       "      <td>^q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>so we kind of looked around</td>\n",
       "      <td>sd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hes in in florida jail now</td>\n",
       "      <td>sd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Utterance Label\n",
       "0  otherwise theyre pretty smelly    sv\n",
       "1          she got the treatments    sd\n",
       "2             this is the reality    ^q\n",
       "3     so we kind of looked around    sd\n",
       "4      hes in in florida jail now    sd"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Utterance'] = train_df['Utterance'].apply(lambda x: remove_html(x))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a1d65d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and one is uh four</td>\n",
       "      <td>sd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>regarding uh taxes i you know</td>\n",
       "      <td>fo_o_fw_\"_by_bc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and that was actually after the war was over</td>\n",
       "      <td>sd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oh well he made it at fifty</td>\n",
       "      <td>sd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and another thing is cost</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Utterance            Label\n",
       "0                            and one is uh four               sd\n",
       "1                 regarding uh taxes i you know  fo_o_fw_\"_by_bc\n",
       "2  and that was actually after the war was over               sd\n",
       "3                   oh well he made it at fifty               sd\n",
       "4                     and another thing is cost               sv"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['Utterance'] = test_df['Utterance'].apply(lambda x: remove_html(x))\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c99f7e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Utterance'] = df['Utterance'].str\\\n",
    "#                     .replace('[^a-zA-Z ]', '',regex=True)\n",
    "# df['Utterance'] = df['Utterance'].str\\\n",
    "#                     .replace('http\\S+|www.\\S+', '',regex=True)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "446edc16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>otherwise theyre pretty smelly</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>she got the treatments</td>\n",
       "      <td>sd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this is the reality</td>\n",
       "      <td>^q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>so we kind of looked around</td>\n",
       "      <td>sd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hes in in florida jail now</td>\n",
       "      <td>sd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Utterance Label\n",
       "0  otherwise theyre pretty smelly    sv\n",
       "1          she got the treatments    sd\n",
       "2             this is the reality    ^q\n",
       "3     so we kind of looked around    sd\n",
       "4      hes in in florida jail now    sd"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Utterance'] = train_df['Utterance'].str\\\n",
    "                    .replace('[^a-zA-Z ]', '',regex=True)\n",
    "train_df['Utterance'] = train_df['Utterance'].str\\\n",
    "                    .replace('http\\S+|www.\\S+', '',regex=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e90c6d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and one is uh four</td>\n",
       "      <td>sd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>regarding uh taxes i you know</td>\n",
       "      <td>fo_o_fw_\"_by_bc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and that was actually after the war was over</td>\n",
       "      <td>sd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oh well he made it at fifty</td>\n",
       "      <td>sd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and another thing is cost</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Utterance            Label\n",
       "0                            and one is uh four               sd\n",
       "1                 regarding uh taxes i you know  fo_o_fw_\"_by_bc\n",
       "2  and that was actually after the war was over               sd\n",
       "3                   oh well he made it at fifty               sd\n",
       "4                     and another thing is cost               sv"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['Utterance'] = test_df['Utterance'].str\\\n",
    "                    .replace('[^a-zA-Z ]', '',regex=True)\n",
    "test_df['Utterance'] = test_df['Utterance'].str\\\n",
    "                    .replace('http\\S+|www.\\S+', '',regex=True)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d49a0df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Utterance'] = df['Utterance'].apply(lambda x:contractions.fix(x))\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b953d117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>otherwise they are pretty smelly</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>she got the treatments</td>\n",
       "      <td>sd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this is the reality</td>\n",
       "      <td>^q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>so we kind of looked around</td>\n",
       "      <td>sd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hes in in florida jail now</td>\n",
       "      <td>sd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Utterance Label\n",
       "0  otherwise they are pretty smelly    sv\n",
       "1            she got the treatments    sd\n",
       "2               this is the reality    ^q\n",
       "3       so we kind of looked around    sd\n",
       "4        hes in in florida jail now    sd"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Utterance'] = train_df['Utterance'].apply(lambda x:contractions.fix(x))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6695b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and one is uh four</td>\n",
       "      <td>sd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>regarding uh taxes i you know</td>\n",
       "      <td>fo_o_fw_\"_by_bc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and that was actually after the war was over</td>\n",
       "      <td>sd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oh well he made it at fifty</td>\n",
       "      <td>sd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and another thing is cost</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Utterance            Label\n",
       "0                            and one is uh four               sd\n",
       "1                 regarding uh taxes i you know  fo_o_fw_\"_by_bc\n",
       "2  and that was actually after the war was over               sd\n",
       "3                   oh well he made it at fifty               sd\n",
       "4                     and another thing is cost               sv"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['Utterance'] = test_df['Utterance'].apply(lambda x:contractions.fix(x))\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "16b44297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sd                 161\n",
       "sv                  57\n",
       "%                   19\n",
       "qy                  18\n",
       "qw                  10\n",
       "ba                   9\n",
       "b                    6\n",
       "^q                   5\n",
       "na                   5\n",
       "aa                   5\n",
       "ad                   4\n",
       "bf                   4\n",
       "fc                   3\n",
       "qy^d                 3\n",
       "ng                   3\n",
       "h                    3\n",
       "fo_o_fw_\"_by_bc      2\n",
       "qo                   2\n",
       "^h                   2\n",
       "bk                   2\n",
       "bh                   2\n",
       "^2                   1\n",
       "fp                   1\n",
       "qh                   1\n",
       "fa                   1\n",
       "b^m                  1\n",
       "no                   1\n",
       "ny                   1\n",
       "t1                   1\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c7cb9e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_df['Utterance']\n",
    "y_train = train_df['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9cc7f919",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test_df['Utterance']\n",
    "y_test = test_df['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ca7e21b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.utils import shuffle\n",
    "\n",
    "# shuffled_df = shuffle(df)\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(\n",
    "#                     shuffled_df['Utterance'], shuffled_df['Label'], \n",
    "#                     train_size = 0.95, test_size = 0.05, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08b0c6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df, shuffled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a18c6573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "from tqdm import tqdm\n",
    "def extract_bert(list_of_sentences=[], batch_size=32):\n",
    "    cls_hidden_states = []\n",
    "    if len(list_of_sentences)<batch_size: #go longer ones we should batch\n",
    "        tknzed= tokenizer(list_of_sentences, return_tensors=\"pt\",padding=True)\n",
    "        b=model(**tknzed)\n",
    "        cls_hidden_states = torch.squeeze(b.last_hidden_state[:,0,:])\n",
    "    else:\n",
    "        for i in tqdm(range(0,len(list_of_sentences),batch_size)):    \n",
    "            tknzed= tokenizer(list_of_sentences[i:min(len(list_of_sentences),i+batch_size)], return_tensors=\"pt\",padding=True)\n",
    "            b=model(**tknzed)\n",
    "            cls_hidden_states.append(torch.squeeze(b.last_hidden_state[:,0,:]))\n",
    "    return cls_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a1522df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_bert = extract_bert(list(x_train))\n",
    "# x_train_torch = torch.cat(x_train_bert)\n",
    "# x_test_bert = extract_bert(list(x_test))\n",
    "# x_test_torch = torch.cat(x_test_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8d3194d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 32/32 [00:05<00:00,  5.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(1000, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 32/32 [00:05<00:00,  5.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "(2000, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 32/32 [00:04<00:00,  6.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "(3000, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 32/32 [00:04<00:00,  6.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "(4000, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 32/32 [00:04<00:00,  6.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n",
      "(5000, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 32/32 [00:05<00:00,  5.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "(6000, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "(6305, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(x_train), 1000):\n",
    "    reduced_x_train = x_train[i: i + 1000]\n",
    "    reduced_x_train_bert = extract_bert(list(reduced_x_train))\n",
    "    reduced_x_train_torch = torch.cat(reduced_x_train_bert)\n",
    "    redcued_x_train_features = reduced_x_train_torch.cpu().detach().numpy()\n",
    "    print(i)\n",
    "    if i != 0:\n",
    "        x_train_features = np.concatenate((x_train_features, redcued_x_train_features), axis = 0)\n",
    "    else:\n",
    "        x_train_features = redcued_x_train_features\n",
    "#     reduced_y_train = np.asarray(reduced_y_train)\n",
    "    print(x_train_features.shape)\n",
    "    del reduced_x_train, reduced_x_train_bert, reduced_x_train_torch, redcued_x_train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d5ebd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced_x_train_bert = extract_bert(list(x_train[189000:-1]))\n",
    "# print(len(reduced_x_train_bert[0]))\n",
    "# reduced_x_train_torch = torch.cat(reduced_x_train_bert)\n",
    "# redcued_x_train_features = reduced_x_train_torch.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87f8076a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_features = np.concatenate((x_train_features, redcued_x_train_features), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c63da38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del reduced_x_train, reduced_x_train_bert, reduced_x_train_torch, redcued_x_train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14decaf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6305, 768)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7db3340a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6305,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "71cdd147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 11/11 [00:01<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(333, 768)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(x_test), 1000):\n",
    "    reduced_x_test = x_test[i: i + 1000]\n",
    "    reduced_x_test_bert = extract_bert(list(reduced_x_test))\n",
    "    reduced_x_test_torch = torch.cat(reduced_x_test_bert)\n",
    "    redcued_x_test_features = reduced_x_test_torch.cpu().detach().numpy()\n",
    "    print(i)\n",
    "    if i != 0:\n",
    "        x_test_features = np.concatenate((x_test_features, redcued_x_test_features), axis = 0)\n",
    "    else:\n",
    "        x_test_features = redcued_x_test_features\n",
    "#     reduced_y_train = np.asarray(reduced_y_train)\n",
    "    print(x_test_features.shape)\n",
    "    del reduced_x_test, reduced_x_test_bert, reduced_x_test_torch, redcued_x_test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d66a5c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6305, 768)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f7feb36d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(333, 768)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "beee97a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "38a2210e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6305,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "28e7fcb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(333,)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6733f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d1969f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('features/train_features_roberta_audio_aligned.npy', 'wb') as f:\n",
    "    np.save(f, x_train_features)\n",
    "    \n",
    "with open('features/train_labels_roberta_audio_aligned.npy', 'wb') as f:\n",
    "    np.save(f, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a23b0d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('features/test_features_roberta_audio_aligned.npy', 'wb') as f:\n",
    "    np.save(f, x_test_features)\n",
    "    \n",
    "with open('features/test_labels_roberta_audio_aligned.npy', 'wb') as f:\n",
    "    np.save(f, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "23684d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('features/train_features_roberta.npy', allow_pickle=True)\n",
    "y_train = np.load('features/train_labels_roberta.npy', allow_pickle=True)\n",
    "\n",
    "x_test = np.load('features/test_features_roberta.npy', allow_pickle=True)\n",
    "y_test = np.load('features/test_labels_roberta.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "40c5e22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_audio = np.load('features/test_features_roberta_audio_aligned.npy', allow_pickle=True)\n",
    "y_test_audio = np.load('features/test_labels_roberta_audio_aligned.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "65f22f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(333, 768)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "050321bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(333,)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4ae54ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(189753, 768)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5a0d5be2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(189753,)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5f767427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9987, 768)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1e900949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9987,)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "63fb4e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(x_train)\n",
    "simple_x_train = scaler.transform(x_train)\n",
    "\n",
    "simple_x_test =scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0d4e78d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'%': 0, '^2': 1, '^g': 2, '^h': 3, '^q': 4, 'aa': 5, 'aap_am': 6, 'ad': 7, 'ar': 8, 'arp_nd': 9, 'b': 10, 'b^m': 11, 'ba': 12, 'bd': 13, 'bf': 14, 'bh': 15, 'bk': 16, 'br': 17, 'fa': 18, 'fc': 19, 'fo_o_fw_\"_by_bc': 20, 'fp': 21, 'ft': 22, 'h': 23, 'na': 24, 'ng': 25, 'nn': 26, 'no': 27, 'ny': 28, 'oo_co_cc': 29, 'qh': 30, 'qo': 31, 'qrr': 32, 'qw': 33, 'qw^d': 34, 'qy': 35, 'qy^d': 36, 'sd': 37, 'sv': 38, 't1': 39, 't3': 40}\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_train)\n",
    "le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(le_name_mapping)\n",
    "y_train = le.transform(y_train)\n",
    "y_test = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5dad743f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_audio = le.transform(y_test_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e60f9850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(189753,)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "158c4468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.593463</td>\n",
       "      <td>0.754545</td>\n",
       "      <td>0.664380</td>\n",
       "      <td>770.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.270833</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.329114</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.606061</td>\n",
       "      <td>0.172117</td>\n",
       "      <td>0.268097</td>\n",
       "      <td>581.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.393939</td>\n",
       "      <td>0.302326</td>\n",
       "      <td>0.342105</td>\n",
       "      <td>43.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.807154</td>\n",
       "      <td>0.548626</td>\n",
       "      <td>0.653241</td>\n",
       "      <td>1892.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.089744</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.128440</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.577358</td>\n",
       "      <td>0.624490</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>245.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.065934</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.081081</td>\n",
       "      <td>57.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.362069</td>\n",
       "      <td>0.396226</td>\n",
       "      <td>0.378378</td>\n",
       "      <td>53.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>62.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.609756</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.030769</td>\n",
       "      <td>37.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.584906</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.704545</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.109842</td>\n",
       "      <td>0.816993</td>\n",
       "      <td>0.193648</td>\n",
       "      <td>153.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.211765</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>29.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.278351</td>\n",
       "      <td>0.421875</td>\n",
       "      <td>97.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>224.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.016393</td>\n",
       "      <td>0.022989</td>\n",
       "      <td>61.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.763999</td>\n",
       "      <td>0.839008</td>\n",
       "      <td>0.799748</td>\n",
       "      <td>3789.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.613551</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.465493</td>\n",
       "      <td>1304.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.613498</td>\n",
       "      <td>0.613498</td>\n",
       "      <td>0.613498</td>\n",
       "      <td>0.613498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.315631</td>\n",
       "      <td>0.329908</td>\n",
       "      <td>0.295241</td>\n",
       "      <td>9987.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.675725</td>\n",
       "      <td>0.613498</td>\n",
       "      <td>0.620255</td>\n",
       "      <td>9987.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "0              0.593463  0.754545  0.664380   770.000000\n",
       "1              0.000000  0.000000  0.000000    41.000000\n",
       "2              0.000000  0.000000  0.000000     1.000000\n",
       "3              0.270833  0.419355  0.329114    31.000000\n",
       "4              0.035714  0.024390  0.028986    41.000000\n",
       "5              0.606061  0.172117  0.268097   581.000000\n",
       "6              0.000000  0.000000  0.000000     7.000000\n",
       "7              0.393939  0.302326  0.342105    43.000000\n",
       "8              0.000000  0.000000  0.000000    14.000000\n",
       "9              0.000000  0.000000  0.000000     9.000000\n",
       "10             0.807154  0.548626  0.653241  1892.000000\n",
       "11             0.089744  0.225806  0.128440    31.000000\n",
       "12             0.577358  0.624490  0.600000   245.000000\n",
       "13             0.250000  0.333333  0.285714     3.000000\n",
       "14             0.065934  0.105263  0.081081    57.000000\n",
       "15             0.362069  0.396226  0.378378    53.000000\n",
       "16             0.580645  0.290323  0.387097    62.000000\n",
       "17             0.166667  0.250000  0.200000     8.000000\n",
       "18             1.000000  0.833333  0.909091     6.000000\n",
       "19             0.722222  0.433333  0.541667   120.000000\n",
       "20             0.000000  0.000000  0.000000    40.000000\n",
       "21             0.428571  0.461538  0.444444    13.000000\n",
       "22             0.200000  0.600000  0.300000     5.000000\n",
       "23             0.609756  0.714286  0.657895    70.000000\n",
       "24             0.035714  0.027027  0.030769    37.000000\n",
       "25             0.133333  0.133333  0.133333    15.000000\n",
       "26             0.584906  0.885714  0.704545    70.000000\n",
       "27             0.000000  0.000000  0.000000    10.000000\n",
       "28             0.109842  0.816993  0.193648   153.000000\n",
       "29             0.111111  0.200000  0.142857     5.000000\n",
       "30             0.211765  0.620690  0.315789    29.000000\n",
       "31             0.500000  0.444444  0.470588    27.000000\n",
       "32             0.333333  0.400000  0.363636     5.000000\n",
       "33             0.870968  0.278351  0.421875    97.000000\n",
       "34             0.045455  0.142857  0.068966     7.000000\n",
       "35             0.772727  0.607143  0.680000   224.000000\n",
       "36             0.038462  0.016393  0.022989    61.000000\n",
       "37             0.763999  0.839008  0.799748  3789.000000\n",
       "38             0.613551  0.375000  0.465493  1304.000000\n",
       "39             0.000000  0.000000  0.000000     7.000000\n",
       "40             0.055556  0.250000  0.090909     4.000000\n",
       "accuracy       0.613498  0.613498  0.613498     0.613498\n",
       "macro avg      0.315631  0.329908  0.295241  9987.000000\n",
       "weighted avg   0.675725  0.613498  0.620255  9987.000000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Perceptron()\n",
    "\n",
    "model.fit(simple_x_train, y_train)\n",
    "pred = model.predict(simple_x_test)\n",
    "pred = classification_report(y_test, pred, output_dict=True)\n",
    "report = pd.DataFrame(pred).transpose()\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4739b6a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.768212</td>\n",
       "      <td>0.753247</td>\n",
       "      <td>0.760656</td>\n",
       "      <td>770.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.548476</td>\n",
       "      <td>0.340792</td>\n",
       "      <td>0.420382</td>\n",
       "      <td>581.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.325581</td>\n",
       "      <td>0.459016</td>\n",
       "      <td>43.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.745964</td>\n",
       "      <td>0.928118</td>\n",
       "      <td>0.827131</td>\n",
       "      <td>1892.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.704225</td>\n",
       "      <td>0.612245</td>\n",
       "      <td>0.655022</td>\n",
       "      <td>245.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>57.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.491525</td>\n",
       "      <td>0.547170</td>\n",
       "      <td>0.517857</td>\n",
       "      <td>53.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.370968</td>\n",
       "      <td>0.464646</td>\n",
       "      <td>62.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.610837</td>\n",
       "      <td>120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.758065</td>\n",
       "      <td>0.671429</td>\n",
       "      <td>0.712121</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.580952</td>\n",
       "      <td>0.871429</td>\n",
       "      <td>0.697143</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.013072</td>\n",
       "      <td>0.025478</td>\n",
       "      <td>153.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.324324</td>\n",
       "      <td>29.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.612245</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.721739</td>\n",
       "      <td>0.855670</td>\n",
       "      <td>0.783019</td>\n",
       "      <td>97.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>224.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>61.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.778694</td>\n",
       "      <td>0.897070</td>\n",
       "      <td>0.833701</td>\n",
       "      <td>3789.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.641202</td>\n",
       "      <td>0.572853</td>\n",
       "      <td>0.605103</td>\n",
       "      <td>1304.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.737459</td>\n",
       "      <td>0.737459</td>\n",
       "      <td>0.737459</td>\n",
       "      <td>0.737459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.429673</td>\n",
       "      <td>0.305925</td>\n",
       "      <td>0.327413</td>\n",
       "      <td>9987.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.702414</td>\n",
       "      <td>0.737459</td>\n",
       "      <td>0.707468</td>\n",
       "      <td>9987.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "0              0.768212  0.753247  0.760656   770.000000\n",
       "1              0.333333  0.024390  0.045455    41.000000\n",
       "2              0.000000  0.000000  0.000000     1.000000\n",
       "3              0.588235  0.322581  0.416667    31.000000\n",
       "4              0.000000  0.000000  0.000000    41.000000\n",
       "5              0.548476  0.340792  0.420382   581.000000\n",
       "6              0.000000  0.000000  0.000000     7.000000\n",
       "7              0.777778  0.325581  0.459016    43.000000\n",
       "8              0.000000  0.000000  0.000000    14.000000\n",
       "9              0.000000  0.000000  0.000000     9.000000\n",
       "10             0.745964  0.928118  0.827131  1892.000000\n",
       "11             0.000000  0.000000  0.000000    31.000000\n",
       "12             0.704225  0.612245  0.655022   245.000000\n",
       "13             0.333333  0.333333  0.333333     3.000000\n",
       "14             0.000000  0.000000  0.000000    57.000000\n",
       "15             0.491525  0.547170  0.517857    53.000000\n",
       "16             0.621622  0.370968  0.464646    62.000000\n",
       "17             1.000000  0.125000  0.222222     8.000000\n",
       "18             1.000000  0.833333  0.909091     6.000000\n",
       "19             0.746988  0.516667  0.610837   120.000000\n",
       "20             0.500000  0.075000  0.130435    40.000000\n",
       "21             0.777778  0.538462  0.636364    13.000000\n",
       "22             0.000000  0.000000  0.000000     5.000000\n",
       "23             0.758065  0.671429  0.712121    70.000000\n",
       "24             0.000000  0.000000  0.000000    37.000000\n",
       "25             1.000000  0.133333  0.235294    15.000000\n",
       "26             0.580952  0.871429  0.697143    70.000000\n",
       "27             0.000000  0.000000  0.000000    10.000000\n",
       "28             0.500000  0.013072  0.025478   153.000000\n",
       "29             0.000000  0.000000  0.000000     5.000000\n",
       "30             0.750000  0.206897  0.324324    29.000000\n",
       "31             0.681818  0.555556  0.612245    27.000000\n",
       "32             0.500000  0.400000  0.444444     5.000000\n",
       "33             0.721739  0.855670  0.783019    97.000000\n",
       "34             0.000000  0.000000  0.000000     7.000000\n",
       "35             0.766667  0.718750  0.741935   224.000000\n",
       "36             0.000000  0.000000  0.000000    61.000000\n",
       "37             0.778694  0.897070  0.833701  3789.000000\n",
       "38             0.641202  0.572853  0.605103  1304.000000\n",
       "39             0.000000  0.000000  0.000000     7.000000\n",
       "40             0.000000  0.000000  0.000000     4.000000\n",
       "accuracy       0.737459  0.737459  0.737459     0.737459\n",
       "macro avg      0.429673  0.305925  0.327413  9987.000000\n",
       "weighted avg   0.702414  0.737459  0.707468  9987.000000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "\n",
    "\n",
    "# pipeline = imbpipeline(steps = [['classifier', LinearSVC()]])\n",
    "\n",
    "# param_grid = {'classifier__C': [0.01, 0.1, 1, 10, 100, 1000],\n",
    "#               'classifier__penalty': ['l1', 'l2'],\n",
    "#               'classifier__loss': ['hinge', 'squared_hinge'],\n",
    "#              'classifier__class_weight': [None]}\n",
    "\n",
    "\n",
    "svc = LinearSVC()\n",
    "\n",
    "svc.fit(x_train, y_train)\n",
    "pred = svc.predict(x_test)\n",
    "pred = classification_report(y_test, pred, output_dict=True)\n",
    "report = pd.DataFrame(pred).transpose()\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "28b78f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.440499</td>\n",
       "      <td>0.596104</td>\n",
       "      <td>0.506623</td>\n",
       "      <td>770.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.302158</td>\n",
       "      <td>0.144578</td>\n",
       "      <td>0.195576</td>\n",
       "      <td>581.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.710494</td>\n",
       "      <td>0.919662</td>\n",
       "      <td>0.801659</td>\n",
       "      <td>1892.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.156716</td>\n",
       "      <td>245.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>57.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>53.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>62.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>153.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>97.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.395349</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.288952</td>\n",
       "      <td>224.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>61.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.685006</td>\n",
       "      <td>0.897070</td>\n",
       "      <td>0.776826</td>\n",
       "      <td>3789.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.468297</td>\n",
       "      <td>0.396472</td>\n",
       "      <td>0.429402</td>\n",
       "      <td>1304.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.627916</td>\n",
       "      <td>0.627916</td>\n",
       "      <td>0.627916</td>\n",
       "      <td>0.627916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.095484</td>\n",
       "      <td>0.079690</td>\n",
       "      <td>0.076970</td>\n",
       "      <td>9987.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.538439</td>\n",
       "      <td>0.627916</td>\n",
       "      <td>0.563424</td>\n",
       "      <td>9987.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "0              0.440499  0.596104  0.506623   770.000000\n",
       "1              0.000000  0.000000  0.000000    41.000000\n",
       "2              0.000000  0.000000  0.000000     1.000000\n",
       "3              0.000000  0.000000  0.000000    31.000000\n",
       "4              0.000000  0.000000  0.000000    41.000000\n",
       "5              0.302158  0.144578  0.195576   581.000000\n",
       "6              0.000000  0.000000  0.000000     7.000000\n",
       "7              0.000000  0.000000  0.000000    43.000000\n",
       "8              0.000000  0.000000  0.000000    14.000000\n",
       "9              0.000000  0.000000  0.000000     9.000000\n",
       "10             0.710494  0.919662  0.801659  1892.000000\n",
       "11             0.000000  0.000000  0.000000    31.000000\n",
       "12             0.913043  0.085714  0.156716   245.000000\n",
       "13             0.000000  0.000000  0.000000     3.000000\n",
       "14             0.000000  0.000000  0.000000    57.000000\n",
       "15             0.000000  0.000000  0.000000    53.000000\n",
       "16             0.000000  0.000000  0.000000    62.000000\n",
       "17             0.000000  0.000000  0.000000     8.000000\n",
       "18             0.000000  0.000000  0.000000     6.000000\n",
       "19             0.000000  0.000000  0.000000   120.000000\n",
       "20             0.000000  0.000000  0.000000    40.000000\n",
       "21             0.000000  0.000000  0.000000    13.000000\n",
       "22             0.000000  0.000000  0.000000     5.000000\n",
       "23             0.000000  0.000000  0.000000    70.000000\n",
       "24             0.000000  0.000000  0.000000    37.000000\n",
       "25             0.000000  0.000000  0.000000    15.000000\n",
       "26             0.000000  0.000000  0.000000    70.000000\n",
       "27             0.000000  0.000000  0.000000    10.000000\n",
       "28             0.000000  0.000000  0.000000   153.000000\n",
       "29             0.000000  0.000000  0.000000     5.000000\n",
       "30             0.000000  0.000000  0.000000    29.000000\n",
       "31             0.000000  0.000000  0.000000    27.000000\n",
       "32             0.000000  0.000000  0.000000     5.000000\n",
       "33             0.000000  0.000000  0.000000    97.000000\n",
       "34             0.000000  0.000000  0.000000     7.000000\n",
       "35             0.395349  0.227679  0.288952   224.000000\n",
       "36             0.000000  0.000000  0.000000    61.000000\n",
       "37             0.685006  0.897070  0.776826  3789.000000\n",
       "38             0.468297  0.396472  0.429402  1304.000000\n",
       "39             0.000000  0.000000  0.000000     7.000000\n",
       "40             0.000000  0.000000  0.000000     4.000000\n",
       "accuracy       0.627916  0.627916  0.627916     0.627916\n",
       "macro avg      0.095484  0.079690  0.076970  9987.000000\n",
       "weighted avg   0.538439  0.627916  0.563424  9987.000000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from imblearn.pipeline import Pipeline as imbpipeline\n",
    "\n",
    "\n",
    "# pipeline = imbpipeline(steps = [['classifier', LogisticRegression()]])\n",
    "\n",
    "# param_grid = {'classifier__C': np.logspace(-4, 4, 50),\n",
    "#               'classifier__penalty': ['l1', 'l2'],\n",
    "#              'classifier__class_weight': [None]}\n",
    "\n",
    "lg = LogisticRegression()\n",
    "# lg = GridSearchCV(estimator=pipeline,\n",
    "#                            param_grid=param_grid,\n",
    "#                            scoring='f1_weighted',\n",
    "#                            cv=10,\n",
    "#                            n_jobs=-1,\n",
    "#                             verbose = 1,\n",
    "#                             refit = True)\n",
    "\n",
    "lg.fit(x_train, y_train)\n",
    "pred = lg.predict(x_test)\n",
    "pred = classification_report(y_test, pred, output_dict=True)\n",
    "report = pd.DataFrame(pred).transpose()\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b23051e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# clf = LogisticRegression()\n",
    "# clf.fit(x_train, y_train)\n",
    "# clf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13e2aea5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_y_pred = clf.predict(x_train)\n",
    "\n",
    "# train_result = classification_report(train_y_pred, y_train)\n",
    "# print(train_result)\n",
    "\n",
    "# y_pred = clf.predict(x_test)\n",
    "\n",
    "# result = classification_report(y_pred, y_test)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5ab9b436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MultinomialNB()\n",
    "# model.fit(x_train, y_train)\n",
    "# pred = model.predict(x_test)\n",
    "# pred = classification_report(y_test, pred, output_dict=True)\n",
    "# report = pd.DataFrame(pred).transpose()\n",
    "# report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8c7541f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.865854</td>\n",
       "      <td>0.554688</td>\n",
       "      <td>0.676190</td>\n",
       "      <td>128.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.396040</td>\n",
       "      <td>0.522876</td>\n",
       "      <td>101.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.747253</td>\n",
       "      <td>160.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.425532</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>33.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>60.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.647373</td>\n",
       "      <td>0.957839</td>\n",
       "      <td>0.772582</td>\n",
       "      <td>759.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.229091</td>\n",
       "      <td>0.337802</td>\n",
       "      <td>275.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.667776</td>\n",
       "      <td>0.667776</td>\n",
       "      <td>0.667776</td>\n",
       "      <td>0.667776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.487773</td>\n",
       "      <td>0.312751</td>\n",
       "      <td>0.348167</td>\n",
       "      <td>1803.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.664281</td>\n",
       "      <td>0.667776</td>\n",
       "      <td>0.622081</td>\n",
       "      <td>1803.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "0              0.865854  0.554688  0.676190   128.000000\n",
       "1              0.000000  0.000000  0.000000     7.000000\n",
       "2              0.000000  0.000000  0.000000     1.000000\n",
       "3              1.000000  0.166667  0.285714     6.000000\n",
       "4              0.000000  0.000000  0.000000    10.000000\n",
       "5              0.769231  0.396040  0.522876   101.000000\n",
       "6              1.000000  1.000000  1.000000     1.000000\n",
       "7              1.000000  0.272727  0.428571    11.000000\n",
       "8              0.000000  0.000000  0.000000     4.000000\n",
       "9              0.000000  0.000000  0.000000     3.000000\n",
       "10             0.666667  0.850000  0.747253   160.000000\n",
       "11             0.000000  0.000000  0.000000     7.000000\n",
       "12             0.774194  0.585366  0.666667    41.000000\n",
       "13             1.000000  0.500000  0.666667     2.000000\n",
       "14             0.000000  0.000000  0.000000     8.000000\n",
       "15             0.555556  0.555556  0.555556     9.000000\n",
       "16             0.833333  0.384615  0.526316    13.000000\n",
       "17             1.000000  0.250000  0.400000     4.000000\n",
       "18             0.000000  0.000000  0.000000     1.000000\n",
       "19             0.714286  0.555556  0.625000     9.000000\n",
       "20             0.425532  0.740741  0.540541    27.000000\n",
       "21             1.000000  0.444444  0.615385     9.000000\n",
       "22             0.000000  0.000000  0.000000     1.000000\n",
       "23             1.000000  0.583333  0.736842    12.000000\n",
       "24             1.000000  0.090909  0.166667    11.000000\n",
       "25             1.000000  0.200000  0.333333     5.000000\n",
       "26             0.700000  1.000000  0.823529    14.000000\n",
       "27             0.000000  0.000000  0.000000     3.000000\n",
       "28             0.578947  0.392857  0.468085    28.000000\n",
       "29             0.000000  0.000000  0.000000     2.000000\n",
       "30             0.000000  0.000000  0.000000     6.000000\n",
       "31             0.846154  0.687500  0.758621    16.000000\n",
       "32             0.000000  0.000000  0.000000     2.000000\n",
       "33             0.782609  0.545455  0.642857    33.000000\n",
       "34             0.000000  0.000000  0.000000     1.000000\n",
       "35             0.708333  0.566667  0.629630    60.000000\n",
       "36             0.000000  0.000000  0.000000    12.000000\n",
       "37             0.647373  0.957839  0.772582   759.000000\n",
       "38             0.642857  0.229091  0.337802   275.000000\n",
       "39             0.000000  0.000000  0.000000     1.000000\n",
       "accuracy       0.667776  0.667776  0.667776     0.667776\n",
       "macro avg      0.487773  0.312751  0.348167  1803.000000\n",
       "weighted avg   0.664281  0.667776  0.622081  1803.000000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "pred = model.predict(x_test)\n",
    "pred = classification_report(y_test, pred, output_dict=True)\n",
    "report = pd.DataFrame(pred).transpose()\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4a8edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LinearSVC()\n",
    "\n",
    "# model.fit(x_train, y_train)\n",
    "# pred = model.predict(x_test)\n",
    "# pred = classification_report(y_test, pred)\n",
    "# print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5f9e6b7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.632812</td>\n",
       "      <td>0.726457</td>\n",
       "      <td>128.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.396040</td>\n",
       "      <td>0.522876</td>\n",
       "      <td>101.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.661765</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>0.741758</td>\n",
       "      <td>160.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.658537</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.425532</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>0.622951</td>\n",
       "      <td>33.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>60.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.730727</td>\n",
       "      <td>0.886693</td>\n",
       "      <td>0.801190</td>\n",
       "      <td>759.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.575397</td>\n",
       "      <td>0.527273</td>\n",
       "      <td>0.550285</td>\n",
       "      <td>275.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.696617</td>\n",
       "      <td>0.696617</td>\n",
       "      <td>0.696617</td>\n",
       "      <td>0.696617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.479478</td>\n",
       "      <td>0.337426</td>\n",
       "      <td>0.364168</td>\n",
       "      <td>1803.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.684044</td>\n",
       "      <td>0.696617</td>\n",
       "      <td>0.673473</td>\n",
       "      <td>1803.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "0              0.852632  0.632812  0.726457   128.000000\n",
       "1              0.000000  0.000000  0.000000     7.000000\n",
       "2              0.000000  0.000000  0.000000     1.000000\n",
       "3              1.000000  0.166667  0.285714     6.000000\n",
       "4              0.000000  0.000000  0.000000    10.000000\n",
       "5              0.769231  0.396040  0.522876   101.000000\n",
       "6              1.000000  1.000000  1.000000     1.000000\n",
       "7              0.555556  0.454545  0.500000    11.000000\n",
       "8              0.000000  0.000000  0.000000     4.000000\n",
       "9              0.000000  0.000000  0.000000     3.000000\n",
       "10             0.661765  0.843750  0.741758   160.000000\n",
       "11             0.000000  0.000000  0.000000     7.000000\n",
       "12             0.794118  0.658537  0.720000    41.000000\n",
       "13             1.000000  0.500000  0.666667     2.000000\n",
       "14             0.000000  0.000000  0.000000     8.000000\n",
       "15             0.555556  0.555556  0.555556     9.000000\n",
       "16             0.833333  0.384615  0.526316    13.000000\n",
       "17             1.000000  0.250000  0.400000     4.000000\n",
       "18             0.000000  0.000000  0.000000     1.000000\n",
       "19             0.625000  0.555556  0.588235     9.000000\n",
       "20             0.425532  0.740741  0.540541    27.000000\n",
       "21             1.000000  0.555556  0.714286     9.000000\n",
       "22             0.000000  0.000000  0.000000     1.000000\n",
       "23             0.875000  0.583333  0.700000    12.000000\n",
       "24             1.000000  0.090909  0.166667    11.000000\n",
       "25             1.000000  0.200000  0.333333     5.000000\n",
       "26             0.700000  1.000000  0.823529    14.000000\n",
       "27             0.000000  0.000000  0.000000     3.000000\n",
       "28             0.550000  0.392857  0.458333    28.000000\n",
       "29             0.000000  0.000000  0.000000     2.000000\n",
       "30             0.000000  0.000000  0.000000     6.000000\n",
       "31             0.812500  0.812500  0.812500    16.000000\n",
       "32             0.000000  0.000000  0.000000     2.000000\n",
       "33             0.678571  0.575758  0.622951    33.000000\n",
       "34             0.000000  0.000000  0.000000     1.000000\n",
       "35             0.684211  0.650000  0.666667    60.000000\n",
       "36             0.500000  0.083333  0.142857    12.000000\n",
       "37             0.730727  0.886693  0.801190   759.000000\n",
       "38             0.575397  0.527273  0.550285   275.000000\n",
       "39             0.000000  0.000000  0.000000     1.000000\n",
       "accuracy       0.696617  0.696617  0.696617     0.696617\n",
       "macro avg      0.479478  0.337426  0.364168  1803.000000\n",
       "weighted avg   0.684044  0.696617  0.673473  1803.000000"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "pred = model.predict(x_test)\n",
    "pred = classification_report(y_test, pred, output_dict=True)\n",
    "report = pd.DataFrame(pred).transpose()\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "35548fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_torch = torch.from_numpy(x_train)\n",
    "x_test_torch = torch.from_numpy(x_test)\n",
    "x_test_audio_torch = torch.from_numpy(x_test_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "22d90e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([333, 768])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_audio_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "368f0bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(333,)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cf226dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipper = lambda x,y : list(zip(x,list(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a5b9a3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_torch = zipper(x_train_torch, y_train)\n",
    "test_torch = zipper(x_test_torch, y_test)\n",
    "test_audio_torch = zipper(x_test_audio_torch, y_test_audio)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_torch, batch_size=256)\n",
    "test_loader = torch.utils.data.DataLoader(test_torch, batch_size=256)\n",
    "test_audio_loader = torch.utils.data.DataLoader(test_audio_torch, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "93b94870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module): \n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__() \n",
    "#         self.gru = nn.GRU(input_size=768, hidden_size=32, batch_first=True) \n",
    "        self.fc1 = nn.Linear(768,41)\n",
    "#         self.fc2 = nn.Linear(256, 64)\n",
    "#         self.fc3 = nn.Linear(128, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "#         self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "#         out, hn = self.gru(x, torch.randn(1, len(x), 32))\n",
    "#         out = self.fc1(self.relu(x))\n",
    "#         out = self.fc2(self.relu(out))\n",
    "        out = self.fc1(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7145a20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "805c8a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=768, out_features=41, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "linear_model = Net()\n",
    "print(linear_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "da7e6fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer = torch.optim.SGD(linear_model.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e05d3dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 2.051567 \tValidation Loss: 1.986048\n",
      "Validation loss decreased (inf --> 1.986048).         Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.913626 \tValidation Loss: 1.879416\n",
      "Validation loss decreased (1.986048 --> 1.879416).         Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 1.827345 \tValidation Loss: 1.806525\n",
      "Validation loss decreased (1.879416 --> 1.806525).         Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 1.765834 \tValidation Loss: 1.752761\n",
      "Validation loss decreased (1.806525 --> 1.752761).         Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 1.718950 \tValidation Loss: 1.710723\n",
      "Validation loss decreased (1.752761 --> 1.710723).         Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 1.681424 \tValidation Loss: 1.676437\n",
      "Validation loss decreased (1.710723 --> 1.676437).         Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 1.650290 \tValidation Loss: 1.647591\n",
      "Validation loss decreased (1.676437 --> 1.647591).         Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 1.623753 \tValidation Loss: 1.622739\n",
      "Validation loss decreased (1.647591 --> 1.622739).         Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 1.600657 \tValidation Loss: 1.600928\n",
      "Validation loss decreased (1.622739 --> 1.600928).         Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 1.580223 \tValidation Loss: 1.581501\n",
      "Validation loss decreased (1.600928 --> 1.581501).         Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 1.561902 \tValidation Loss: 1.563987\n",
      "Validation loss decreased (1.581501 --> 1.563987).         Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 1.545297 \tValidation Loss: 1.548041\n",
      "Validation loss decreased (1.563987 --> 1.548041).         Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 1.530109 \tValidation Loss: 1.533399\n",
      "Validation loss decreased (1.548041 --> 1.533399).         Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 1.516112 \tValidation Loss: 1.519861\n",
      "Validation loss decreased (1.533399 --> 1.519861).         Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 1.503128 \tValidation Loss: 1.507267\n",
      "Validation loss decreased (1.519861 --> 1.507267).         Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 1.491017 \tValidation Loss: 1.495490\n",
      "Validation loss decreased (1.507267 --> 1.495490).         Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 1.479665 \tValidation Loss: 1.484427\n",
      "Validation loss decreased (1.495490 --> 1.484427).         Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 1.468980 \tValidation Loss: 1.473996\n",
      "Validation loss decreased (1.484427 --> 1.473996).         Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 1.458886 \tValidation Loss: 1.464125\n",
      "Validation loss decreased (1.473996 --> 1.464125).         Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 1.449319 \tValidation Loss: 1.454755\n",
      "Validation loss decreased (1.464125 --> 1.454755).         Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 1.440226 \tValidation Loss: 1.445838\n",
      "Validation loss decreased (1.454755 --> 1.445838).         Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 1.431560 \tValidation Loss: 1.437332\n",
      "Validation loss decreased (1.445838 --> 1.437332).         Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 1.423284 \tValidation Loss: 1.429198\n",
      "Validation loss decreased (1.437332 --> 1.429198).         Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 1.415363 \tValidation Loss: 1.421407\n",
      "Validation loss decreased (1.429198 --> 1.421407).         Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 1.407768 \tValidation Loss: 1.413931\n",
      "Validation loss decreased (1.421407 --> 1.413931).         Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 1.400474 \tValidation Loss: 1.406745\n",
      "Validation loss decreased (1.413931 --> 1.406745).         Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 1.393457 \tValidation Loss: 1.399829\n",
      "Validation loss decreased (1.406745 --> 1.399829).         Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 1.386698 \tValidation Loss: 1.393163\n",
      "Validation loss decreased (1.399829 --> 1.393163).         Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 1.380179 \tValidation Loss: 1.386730\n",
      "Validation loss decreased (1.393163 --> 1.386730).         Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 1.373885 \tValidation Loss: 1.380515\n",
      "Validation loss decreased (1.386730 --> 1.380515).         Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 1.367800 \tValidation Loss: 1.374505\n",
      "Validation loss decreased (1.380515 --> 1.374505).         Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 1.361913 \tValidation Loss: 1.368688\n",
      "Validation loss decreased (1.374505 --> 1.368688).         Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 1.356211 \tValidation Loss: 1.363052\n",
      "Validation loss decreased (1.368688 --> 1.363052).         Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 1.350684 \tValidation Loss: 1.357586\n",
      "Validation loss decreased (1.363052 --> 1.357586).         Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 1.345322 \tValidation Loss: 1.352283\n",
      "Validation loss decreased (1.357586 --> 1.352283).         Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 1.340117 \tValidation Loss: 1.347132\n",
      "Validation loss decreased (1.352283 --> 1.347132).         Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 1.335060 \tValidation Loss: 1.342127\n",
      "Validation loss decreased (1.347132 --> 1.342127).         Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 1.330144 \tValidation Loss: 1.337261\n",
      "Validation loss decreased (1.342127 --> 1.337261).         Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 1.325362 \tValidation Loss: 1.332525\n",
      "Validation loss decreased (1.337261 --> 1.332525).         Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 1.320708 \tValidation Loss: 1.327915\n",
      "Validation loss decreased (1.332525 --> 1.327915).         Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 1.316175 \tValidation Loss: 1.323424\n",
      "Validation loss decreased (1.327915 --> 1.323424).         Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 1.311759 \tValidation Loss: 1.319048\n",
      "Validation loss decreased (1.323424 --> 1.319048).         Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 1.307453 \tValidation Loss: 1.314781\n",
      "Validation loss decreased (1.319048 --> 1.314781).         Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 1.303254 \tValidation Loss: 1.310618\n",
      "Validation loss decreased (1.314781 --> 1.310618).         Saving model ...\n",
      "Epoch: 45 \tTraining Loss: 1.299157 \tValidation Loss: 1.306555\n",
      "Validation loss decreased (1.310618 --> 1.306555).         Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 1.295157 \tValidation Loss: 1.302588\n",
      "Validation loss decreased (1.306555 --> 1.302588).         Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 1.291251 \tValidation Loss: 1.298714\n",
      "Validation loss decreased (1.302588 --> 1.298714).         Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 1.287435 \tValidation Loss: 1.294928\n",
      "Validation loss decreased (1.298714 --> 1.294928).         Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 1.283706 \tValidation Loss: 1.291228\n",
      "Validation loss decreased (1.294928 --> 1.291228).         Saving model ...\n",
      "Epoch: 50 \tTraining Loss: 1.280060 \tValidation Loss: 1.287609\n",
      "Validation loss decreased (1.291228 --> 1.287609).         Saving model ...\n",
      "Epoch: 51 \tTraining Loss: 1.276494 \tValidation Loss: 1.284069\n",
      "Validation loss decreased (1.287609 --> 1.284069).         Saving model ...\n",
      "Epoch: 52 \tTraining Loss: 1.273005 \tValidation Loss: 1.280606\n",
      "Validation loss decreased (1.284069 --> 1.280606).         Saving model ...\n",
      "Epoch: 53 \tTraining Loss: 1.269591 \tValidation Loss: 1.277215\n",
      "Validation loss decreased (1.280606 --> 1.277215).         Saving model ...\n",
      "Epoch: 54 \tTraining Loss: 1.266248 \tValidation Loss: 1.273896\n",
      "Validation loss decreased (1.277215 --> 1.273896).         Saving model ...\n",
      "Epoch: 55 \tTraining Loss: 1.262975 \tValidation Loss: 1.270645\n",
      "Validation loss decreased (1.273896 --> 1.270645).         Saving model ...\n",
      "Epoch: 56 \tTraining Loss: 1.259769 \tValidation Loss: 1.267460\n",
      "Validation loss decreased (1.270645 --> 1.267460).         Saving model ...\n",
      "Epoch: 57 \tTraining Loss: 1.256627 \tValidation Loss: 1.264339\n",
      "Validation loss decreased (1.267460 --> 1.264339).         Saving model ...\n",
      "Epoch: 58 \tTraining Loss: 1.253548 \tValidation Loss: 1.261279\n",
      "Validation loss decreased (1.264339 --> 1.261279).         Saving model ...\n",
      "Epoch: 59 \tTraining Loss: 1.250530 \tValidation Loss: 1.258279\n",
      "Validation loss decreased (1.261279 --> 1.258279).         Saving model ...\n",
      "Epoch: 60 \tTraining Loss: 1.247570 \tValidation Loss: 1.255337\n",
      "Validation loss decreased (1.258279 --> 1.255337).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61 \tTraining Loss: 1.244667 \tValidation Loss: 1.252452\n",
      "Validation loss decreased (1.255337 --> 1.252452).         Saving model ...\n",
      "Epoch: 62 \tTraining Loss: 1.241819 \tValidation Loss: 1.249620\n",
      "Validation loss decreased (1.252452 --> 1.249620).         Saving model ...\n",
      "Epoch: 63 \tTraining Loss: 1.239025 \tValidation Loss: 1.246841\n",
      "Validation loss decreased (1.249620 --> 1.246841).         Saving model ...\n",
      "Epoch: 64 \tTraining Loss: 1.236282 \tValidation Loss: 1.244113\n",
      "Validation loss decreased (1.246841 --> 1.244113).         Saving model ...\n",
      "Epoch: 65 \tTraining Loss: 1.233589 \tValidation Loss: 1.241435\n",
      "Validation loss decreased (1.244113 --> 1.241435).         Saving model ...\n",
      "Epoch: 66 \tTraining Loss: 1.230945 \tValidation Loss: 1.238804\n",
      "Validation loss decreased (1.241435 --> 1.238804).         Saving model ...\n",
      "Epoch: 67 \tTraining Loss: 1.228348 \tValidation Loss: 1.236221\n",
      "Validation loss decreased (1.238804 --> 1.236221).         Saving model ...\n",
      "Epoch: 68 \tTraining Loss: 1.225797 \tValidation Loss: 1.233683\n",
      "Validation loss decreased (1.236221 --> 1.233683).         Saving model ...\n",
      "Epoch: 69 \tTraining Loss: 1.223290 \tValidation Loss: 1.231188\n",
      "Validation loss decreased (1.233683 --> 1.231188).         Saving model ...\n",
      "Epoch: 70 \tTraining Loss: 1.220827 \tValidation Loss: 1.228737\n",
      "Validation loss decreased (1.231188 --> 1.228737).         Saving model ...\n",
      "Epoch: 71 \tTraining Loss: 1.218407 \tValidation Loss: 1.226327\n",
      "Validation loss decreased (1.228737 --> 1.226327).         Saving model ...\n",
      "Epoch: 72 \tTraining Loss: 1.216027 \tValidation Loss: 1.223958\n",
      "Validation loss decreased (1.226327 --> 1.223958).         Saving model ...\n",
      "Epoch: 73 \tTraining Loss: 1.213687 \tValidation Loss: 1.221629\n",
      "Validation loss decreased (1.223958 --> 1.221629).         Saving model ...\n",
      "Epoch: 74 \tTraining Loss: 1.211385 \tValidation Loss: 1.219337\n",
      "Validation loss decreased (1.221629 --> 1.219337).         Saving model ...\n",
      "Epoch: 75 \tTraining Loss: 1.209122 \tValidation Loss: 1.217083\n",
      "Validation loss decreased (1.219337 --> 1.217083).         Saving model ...\n",
      "Epoch: 76 \tTraining Loss: 1.206895 \tValidation Loss: 1.214866\n",
      "Validation loss decreased (1.217083 --> 1.214866).         Saving model ...\n",
      "Epoch: 77 \tTraining Loss: 1.204705 \tValidation Loss: 1.212684\n",
      "Validation loss decreased (1.214866 --> 1.212684).         Saving model ...\n",
      "Epoch: 78 \tTraining Loss: 1.202549 \tValidation Loss: 1.210536\n",
      "Validation loss decreased (1.212684 --> 1.210536).         Saving model ...\n",
      "Epoch: 79 \tTraining Loss: 1.200427 \tValidation Loss: 1.208423\n",
      "Validation loss decreased (1.210536 --> 1.208423).         Saving model ...\n",
      "Epoch: 80 \tTraining Loss: 1.198338 \tValidation Loss: 1.206342\n",
      "Validation loss decreased (1.208423 --> 1.206342).         Saving model ...\n",
      "Epoch: 81 \tTraining Loss: 1.196282 \tValidation Loss: 1.204293\n",
      "Validation loss decreased (1.206342 --> 1.204293).         Saving model ...\n",
      "Epoch: 82 \tTraining Loss: 1.194257 \tValidation Loss: 1.202275\n",
      "Validation loss decreased (1.204293 --> 1.202275).         Saving model ...\n",
      "Epoch: 83 \tTraining Loss: 1.192263 \tValidation Loss: 1.200288\n",
      "Validation loss decreased (1.202275 --> 1.200288).         Saving model ...\n",
      "Epoch: 84 \tTraining Loss: 1.190300 \tValidation Loss: 1.198331\n",
      "Validation loss decreased (1.200288 --> 1.198331).         Saving model ...\n",
      "Epoch: 85 \tTraining Loss: 1.188365 \tValidation Loss: 1.196403\n",
      "Validation loss decreased (1.198331 --> 1.196403).         Saving model ...\n",
      "Epoch: 86 \tTraining Loss: 1.186459 \tValidation Loss: 1.194503\n",
      "Validation loss decreased (1.196403 --> 1.194503).         Saving model ...\n",
      "Epoch: 87 \tTraining Loss: 1.184581 \tValidation Loss: 1.192631\n",
      "Validation loss decreased (1.194503 --> 1.192631).         Saving model ...\n",
      "Epoch: 88 \tTraining Loss: 1.182731 \tValidation Loss: 1.190786\n",
      "Validation loss decreased (1.192631 --> 1.190786).         Saving model ...\n",
      "Epoch: 89 \tTraining Loss: 1.180907 \tValidation Loss: 1.188967\n",
      "Validation loss decreased (1.190786 --> 1.188967).         Saving model ...\n",
      "Epoch: 90 \tTraining Loss: 1.179109 \tValidation Loss: 1.187174\n",
      "Validation loss decreased (1.188967 --> 1.187174).         Saving model ...\n",
      "Epoch: 91 \tTraining Loss: 1.177336 \tValidation Loss: 1.185407\n",
      "Validation loss decreased (1.187174 --> 1.185407).         Saving model ...\n",
      "Epoch: 92 \tTraining Loss: 1.175589 \tValidation Loss: 1.183664\n",
      "Validation loss decreased (1.185407 --> 1.183664).         Saving model ...\n",
      "Epoch: 93 \tTraining Loss: 1.173866 \tValidation Loss: 1.181946\n",
      "Validation loss decreased (1.183664 --> 1.181946).         Saving model ...\n",
      "Epoch: 94 \tTraining Loss: 1.172166 \tValidation Loss: 1.180251\n",
      "Validation loss decreased (1.181946 --> 1.180251).         Saving model ...\n",
      "Epoch: 95 \tTraining Loss: 1.170490 \tValidation Loss: 1.178579\n",
      "Validation loss decreased (1.180251 --> 1.178579).         Saving model ...\n",
      "Epoch: 96 \tTraining Loss: 1.168837 \tValidation Loss: 1.176930\n",
      "Validation loss decreased (1.178579 --> 1.176930).         Saving model ...\n",
      "Epoch: 97 \tTraining Loss: 1.167206 \tValidation Loss: 1.175303\n",
      "Validation loss decreased (1.176930 --> 1.175303).         Saving model ...\n",
      "Epoch: 98 \tTraining Loss: 1.165597 \tValidation Loss: 1.173697\n",
      "Validation loss decreased (1.175303 --> 1.173697).         Saving model ...\n",
      "Epoch: 99 \tTraining Loss: 1.164009 \tValidation Loss: 1.172113\n",
      "Validation loss decreased (1.173697 --> 1.172113).         Saving model ...\n",
      "Epoch: 100 \tTraining Loss: 1.162442 \tValidation Loss: 1.170550\n",
      "Validation loss decreased (1.172113 --> 1.170550).         Saving model ...\n",
      "Epoch: 101 \tTraining Loss: 1.160896 \tValidation Loss: 1.169007\n",
      "Validation loss decreased (1.170550 --> 1.169007).         Saving model ...\n",
      "Epoch: 102 \tTraining Loss: 1.159369 \tValidation Loss: 1.167483\n",
      "Validation loss decreased (1.169007 --> 1.167483).         Saving model ...\n",
      "Epoch: 103 \tTraining Loss: 1.157863 \tValidation Loss: 1.165980\n",
      "Validation loss decreased (1.167483 --> 1.165980).         Saving model ...\n",
      "Epoch: 104 \tTraining Loss: 1.156375 \tValidation Loss: 1.164495\n",
      "Validation loss decreased (1.165980 --> 1.164495).         Saving model ...\n",
      "Epoch: 105 \tTraining Loss: 1.154906 \tValidation Loss: 1.163029\n",
      "Validation loss decreased (1.164495 --> 1.163029).         Saving model ...\n",
      "Epoch: 106 \tTraining Loss: 1.153456 \tValidation Loss: 1.161582\n",
      "Validation loss decreased (1.163029 --> 1.161582).         Saving model ...\n",
      "Epoch: 107 \tTraining Loss: 1.152023 \tValidation Loss: 1.160152\n",
      "Validation loss decreased (1.161582 --> 1.160152).         Saving model ...\n",
      "Epoch: 108 \tTraining Loss: 1.150609 \tValidation Loss: 1.158740\n",
      "Validation loss decreased (1.160152 --> 1.158740).         Saving model ...\n",
      "Epoch: 109 \tTraining Loss: 1.149212 \tValidation Loss: 1.157346\n",
      "Validation loss decreased (1.158740 --> 1.157346).         Saving model ...\n",
      "Epoch: 110 \tTraining Loss: 1.147831 \tValidation Loss: 1.155968\n",
      "Validation loss decreased (1.157346 --> 1.155968).         Saving model ...\n",
      "Epoch: 111 \tTraining Loss: 1.146468 \tValidation Loss: 1.154607\n",
      "Validation loss decreased (1.155968 --> 1.154607).         Saving model ...\n",
      "Epoch: 112 \tTraining Loss: 1.145121 \tValidation Loss: 1.153262\n",
      "Validation loss decreased (1.154607 --> 1.153262).         Saving model ...\n",
      "Epoch: 113 \tTraining Loss: 1.143790 \tValidation Loss: 1.151933\n",
      "Validation loss decreased (1.153262 --> 1.151933).         Saving model ...\n",
      "Epoch: 114 \tTraining Loss: 1.142474 \tValidation Loss: 1.150620\n",
      "Validation loss decreased (1.151933 --> 1.150620).         Saving model ...\n",
      "Epoch: 115 \tTraining Loss: 1.141174 \tValidation Loss: 1.149322\n",
      "Validation loss decreased (1.150620 --> 1.149322).         Saving model ...\n",
      "Epoch: 116 \tTraining Loss: 1.139889 \tValidation Loss: 1.148039\n",
      "Validation loss decreased (1.149322 --> 1.148039).         Saving model ...\n",
      "Epoch: 117 \tTraining Loss: 1.138619 \tValidation Loss: 1.146771\n",
      "Validation loss decreased (1.148039 --> 1.146771).         Saving model ...\n",
      "Epoch: 118 \tTraining Loss: 1.137364 \tValidation Loss: 1.145518\n",
      "Validation loss decreased (1.146771 --> 1.145518).         Saving model ...\n",
      "Epoch: 119 \tTraining Loss: 1.136123 \tValidation Loss: 1.144279\n",
      "Validation loss decreased (1.145518 --> 1.144279).         Saving model ...\n",
      "Epoch: 120 \tTraining Loss: 1.134896 \tValidation Loss: 1.143054\n",
      "Validation loss decreased (1.144279 --> 1.143054).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 121 \tTraining Loss: 1.133683 \tValidation Loss: 1.141842\n",
      "Validation loss decreased (1.143054 --> 1.141842).         Saving model ...\n",
      "Epoch: 122 \tTraining Loss: 1.132483 \tValidation Loss: 1.140644\n",
      "Validation loss decreased (1.141842 --> 1.140644).         Saving model ...\n",
      "Epoch: 123 \tTraining Loss: 1.131296 \tValidation Loss: 1.139460\n",
      "Validation loss decreased (1.140644 --> 1.139460).         Saving model ...\n",
      "Epoch: 124 \tTraining Loss: 1.130123 \tValidation Loss: 1.138288\n",
      "Validation loss decreased (1.139460 --> 1.138288).         Saving model ...\n",
      "Epoch: 125 \tTraining Loss: 1.128962 \tValidation Loss: 1.137129\n",
      "Validation loss decreased (1.138288 --> 1.137129).         Saving model ...\n",
      "Epoch: 126 \tTraining Loss: 1.127814 \tValidation Loss: 1.135983\n",
      "Validation loss decreased (1.137129 --> 1.135983).         Saving model ...\n",
      "Epoch: 127 \tTraining Loss: 1.126679 \tValidation Loss: 1.134849\n",
      "Validation loss decreased (1.135983 --> 1.134849).         Saving model ...\n",
      "Epoch: 128 \tTraining Loss: 1.125555 \tValidation Loss: 1.133727\n",
      "Validation loss decreased (1.134849 --> 1.133727).         Saving model ...\n",
      "Epoch: 129 \tTraining Loss: 1.124444 \tValidation Loss: 1.132617\n",
      "Validation loss decreased (1.133727 --> 1.132617).         Saving model ...\n",
      "Epoch: 130 \tTraining Loss: 1.123344 \tValidation Loss: 1.131519\n",
      "Validation loss decreased (1.132617 --> 1.131519).         Saving model ...\n",
      "Epoch: 131 \tTraining Loss: 1.122256 \tValidation Loss: 1.130433\n",
      "Validation loss decreased (1.131519 --> 1.130433).         Saving model ...\n",
      "Epoch: 132 \tTraining Loss: 1.121179 \tValidation Loss: 1.129357\n",
      "Validation loss decreased (1.130433 --> 1.129357).         Saving model ...\n",
      "Epoch: 133 \tTraining Loss: 1.120113 \tValidation Loss: 1.128293\n",
      "Validation loss decreased (1.129357 --> 1.128293).         Saving model ...\n",
      "Epoch: 134 \tTraining Loss: 1.119058 \tValidation Loss: 1.127240\n",
      "Validation loss decreased (1.128293 --> 1.127240).         Saving model ...\n",
      "Epoch: 135 \tTraining Loss: 1.118014 \tValidation Loss: 1.126197\n",
      "Validation loss decreased (1.127240 --> 1.126197).         Saving model ...\n",
      "Epoch: 136 \tTraining Loss: 1.116980 \tValidation Loss: 1.125165\n",
      "Validation loss decreased (1.126197 --> 1.125165).         Saving model ...\n",
      "Epoch: 137 \tTraining Loss: 1.115957 \tValidation Loss: 1.124144\n",
      "Validation loss decreased (1.125165 --> 1.124144).         Saving model ...\n",
      "Epoch: 138 \tTraining Loss: 1.114944 \tValidation Loss: 1.123132\n",
      "Validation loss decreased (1.124144 --> 1.123132).         Saving model ...\n",
      "Epoch: 139 \tTraining Loss: 1.113941 \tValidation Loss: 1.122131\n",
      "Validation loss decreased (1.123132 --> 1.122131).         Saving model ...\n",
      "Epoch: 140 \tTraining Loss: 1.112948 \tValidation Loss: 1.121140\n",
      "Validation loss decreased (1.122131 --> 1.121140).         Saving model ...\n",
      "Epoch: 141 \tTraining Loss: 1.111965 \tValidation Loss: 1.120158\n",
      "Validation loss decreased (1.121140 --> 1.120158).         Saving model ...\n",
      "Epoch: 142 \tTraining Loss: 1.110992 \tValidation Loss: 1.119186\n",
      "Validation loss decreased (1.120158 --> 1.119186).         Saving model ...\n",
      "Epoch: 143 \tTraining Loss: 1.110027 \tValidation Loss: 1.118223\n",
      "Validation loss decreased (1.119186 --> 1.118223).         Saving model ...\n",
      "Epoch: 144 \tTraining Loss: 1.109072 \tValidation Loss: 1.117270\n",
      "Validation loss decreased (1.118223 --> 1.117270).         Saving model ...\n",
      "Epoch: 145 \tTraining Loss: 1.108127 \tValidation Loss: 1.116326\n",
      "Validation loss decreased (1.117270 --> 1.116326).         Saving model ...\n",
      "Epoch: 146 \tTraining Loss: 1.107190 \tValidation Loss: 1.115390\n",
      "Validation loss decreased (1.116326 --> 1.115390).         Saving model ...\n",
      "Epoch: 147 \tTraining Loss: 1.106262 \tValidation Loss: 1.114464\n",
      "Validation loss decreased (1.115390 --> 1.114464).         Saving model ...\n",
      "Epoch: 148 \tTraining Loss: 1.105342 \tValidation Loss: 1.113546\n",
      "Validation loss decreased (1.114464 --> 1.113546).         Saving model ...\n",
      "Epoch: 149 \tTraining Loss: 1.104431 \tValidation Loss: 1.112637\n",
      "Validation loss decreased (1.113546 --> 1.112637).         Saving model ...\n",
      "Epoch: 150 \tTraining Loss: 1.103529 \tValidation Loss: 1.111736\n",
      "Validation loss decreased (1.112637 --> 1.111736).         Saving model ...\n",
      "Epoch: 151 \tTraining Loss: 1.102635 \tValidation Loss: 1.110844\n",
      "Validation loss decreased (1.111736 --> 1.110844).         Saving model ...\n",
      "Epoch: 152 \tTraining Loss: 1.101749 \tValidation Loss: 1.109960\n",
      "Validation loss decreased (1.110844 --> 1.109960).         Saving model ...\n",
      "Epoch: 153 \tTraining Loss: 1.100871 \tValidation Loss: 1.109083\n",
      "Validation loss decreased (1.109960 --> 1.109083).         Saving model ...\n",
      "Epoch: 154 \tTraining Loss: 1.100001 \tValidation Loss: 1.108215\n",
      "Validation loss decreased (1.109083 --> 1.108215).         Saving model ...\n",
      "Epoch: 155 \tTraining Loss: 1.099139 \tValidation Loss: 1.107355\n",
      "Validation loss decreased (1.108215 --> 1.107355).         Saving model ...\n",
      "Epoch: 156 \tTraining Loss: 1.098285 \tValidation Loss: 1.106502\n",
      "Validation loss decreased (1.107355 --> 1.106502).         Saving model ...\n",
      "Epoch: 157 \tTraining Loss: 1.097438 \tValidation Loss: 1.105657\n",
      "Validation loss decreased (1.106502 --> 1.105657).         Saving model ...\n",
      "Epoch: 158 \tTraining Loss: 1.096598 \tValidation Loss: 1.104819\n",
      "Validation loss decreased (1.105657 --> 1.104819).         Saving model ...\n",
      "Epoch: 159 \tTraining Loss: 1.095766 \tValidation Loss: 1.103989\n",
      "Validation loss decreased (1.104819 --> 1.103989).         Saving model ...\n",
      "Epoch: 160 \tTraining Loss: 1.094941 \tValidation Loss: 1.103166\n",
      "Validation loss decreased (1.103989 --> 1.103166).         Saving model ...\n",
      "Epoch: 161 \tTraining Loss: 1.094124 \tValidation Loss: 1.102350\n",
      "Validation loss decreased (1.103166 --> 1.102350).         Saving model ...\n",
      "Epoch: 162 \tTraining Loss: 1.093313 \tValidation Loss: 1.101541\n",
      "Validation loss decreased (1.102350 --> 1.101541).         Saving model ...\n",
      "Epoch: 163 \tTraining Loss: 1.092509 \tValidation Loss: 1.100739\n",
      "Validation loss decreased (1.101541 --> 1.100739).         Saving model ...\n",
      "Epoch: 164 \tTraining Loss: 1.091712 \tValidation Loss: 1.099944\n",
      "Validation loss decreased (1.100739 --> 1.099944).         Saving model ...\n",
      "Epoch: 165 \tTraining Loss: 1.090922 \tValidation Loss: 1.099155\n",
      "Validation loss decreased (1.099944 --> 1.099155).         Saving model ...\n",
      "Epoch: 166 \tTraining Loss: 1.090138 \tValidation Loss: 1.098373\n",
      "Validation loss decreased (1.099155 --> 1.098373).         Saving model ...\n",
      "Epoch: 167 \tTraining Loss: 1.089361 \tValidation Loss: 1.097598\n",
      "Validation loss decreased (1.098373 --> 1.097598).         Saving model ...\n",
      "Epoch: 168 \tTraining Loss: 1.088590 \tValidation Loss: 1.096829\n",
      "Validation loss decreased (1.097598 --> 1.096829).         Saving model ...\n",
      "Epoch: 169 \tTraining Loss: 1.087826 \tValidation Loss: 1.096067\n",
      "Validation loss decreased (1.096829 --> 1.096067).         Saving model ...\n",
      "Epoch: 170 \tTraining Loss: 1.087068 \tValidation Loss: 1.095311\n",
      "Validation loss decreased (1.096067 --> 1.095311).         Saving model ...\n",
      "Epoch: 171 \tTraining Loss: 1.086316 \tValidation Loss: 1.094561\n",
      "Validation loss decreased (1.095311 --> 1.094561).         Saving model ...\n",
      "Epoch: 172 \tTraining Loss: 1.085570 \tValidation Loss: 1.093817\n",
      "Validation loss decreased (1.094561 --> 1.093817).         Saving model ...\n",
      "Epoch: 173 \tTraining Loss: 1.084831 \tValidation Loss: 1.093079\n",
      "Validation loss decreased (1.093817 --> 1.093079).         Saving model ...\n",
      "Epoch: 174 \tTraining Loss: 1.084097 \tValidation Loss: 1.092348\n",
      "Validation loss decreased (1.093079 --> 1.092348).         Saving model ...\n",
      "Epoch: 175 \tTraining Loss: 1.083369 \tValidation Loss: 1.091622\n",
      "Validation loss decreased (1.092348 --> 1.091622).         Saving model ...\n",
      "Epoch: 176 \tTraining Loss: 1.082646 \tValidation Loss: 1.090902\n",
      "Validation loss decreased (1.091622 --> 1.090902).         Saving model ...\n",
      "Epoch: 177 \tTraining Loss: 1.081930 \tValidation Loss: 1.090187\n",
      "Validation loss decreased (1.090902 --> 1.090187).         Saving model ...\n",
      "Epoch: 178 \tTraining Loss: 1.081219 \tValidation Loss: 1.089478\n",
      "Validation loss decreased (1.090187 --> 1.089478).         Saving model ...\n",
      "Epoch: 179 \tTraining Loss: 1.080513 \tValidation Loss: 1.088775\n",
      "Validation loss decreased (1.089478 --> 1.088775).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 180 \tTraining Loss: 1.079813 \tValidation Loss: 1.088077\n",
      "Validation loss decreased (1.088775 --> 1.088077).         Saving model ...\n",
      "Epoch: 181 \tTraining Loss: 1.079119 \tValidation Loss: 1.087384\n",
      "Validation loss decreased (1.088077 --> 1.087384).         Saving model ...\n",
      "Epoch: 182 \tTraining Loss: 1.078429 \tValidation Loss: 1.086697\n",
      "Validation loss decreased (1.087384 --> 1.086697).         Saving model ...\n",
      "Epoch: 183 \tTraining Loss: 1.077745 \tValidation Loss: 1.086016\n",
      "Validation loss decreased (1.086697 --> 1.086016).         Saving model ...\n",
      "Epoch: 184 \tTraining Loss: 1.077066 \tValidation Loss: 1.085339\n",
      "Validation loss decreased (1.086016 --> 1.085339).         Saving model ...\n",
      "Epoch: 185 \tTraining Loss: 1.076392 \tValidation Loss: 1.084667\n",
      "Validation loss decreased (1.085339 --> 1.084667).         Saving model ...\n",
      "Epoch: 186 \tTraining Loss: 1.075724 \tValidation Loss: 1.084001\n",
      "Validation loss decreased (1.084667 --> 1.084001).         Saving model ...\n",
      "Epoch: 187 \tTraining Loss: 1.075060 \tValidation Loss: 1.083339\n",
      "Validation loss decreased (1.084001 --> 1.083339).         Saving model ...\n",
      "Epoch: 188 \tTraining Loss: 1.074401 \tValidation Loss: 1.082683\n",
      "Validation loss decreased (1.083339 --> 1.082683).         Saving model ...\n",
      "Epoch: 189 \tTraining Loss: 1.073747 \tValidation Loss: 1.082031\n",
      "Validation loss decreased (1.082683 --> 1.082031).         Saving model ...\n",
      "Epoch: 190 \tTraining Loss: 1.073098 \tValidation Loss: 1.081384\n",
      "Validation loss decreased (1.082031 --> 1.081384).         Saving model ...\n",
      "Epoch: 191 \tTraining Loss: 1.072453 \tValidation Loss: 1.080742\n",
      "Validation loss decreased (1.081384 --> 1.080742).         Saving model ...\n",
      "Epoch: 192 \tTraining Loss: 1.071813 \tValidation Loss: 1.080105\n",
      "Validation loss decreased (1.080742 --> 1.080105).         Saving model ...\n",
      "Epoch: 193 \tTraining Loss: 1.071178 \tValidation Loss: 1.079472\n",
      "Validation loss decreased (1.080105 --> 1.079472).         Saving model ...\n",
      "Epoch: 194 \tTraining Loss: 1.070547 \tValidation Loss: 1.078844\n",
      "Validation loss decreased (1.079472 --> 1.078844).         Saving model ...\n",
      "Epoch: 195 \tTraining Loss: 1.069921 \tValidation Loss: 1.078220\n",
      "Validation loss decreased (1.078844 --> 1.078220).         Saving model ...\n",
      "Epoch: 196 \tTraining Loss: 1.069299 \tValidation Loss: 1.077601\n",
      "Validation loss decreased (1.078220 --> 1.077601).         Saving model ...\n",
      "Epoch: 197 \tTraining Loss: 1.068682 \tValidation Loss: 1.076986\n",
      "Validation loss decreased (1.077601 --> 1.076986).         Saving model ...\n",
      "Epoch: 198 \tTraining Loss: 1.068069 \tValidation Loss: 1.076375\n",
      "Validation loss decreased (1.076986 --> 1.076375).         Saving model ...\n",
      "Epoch: 199 \tTraining Loss: 1.067460 \tValidation Loss: 1.075769\n",
      "Validation loss decreased (1.076375 --> 1.075769).         Saving model ...\n",
      "Epoch: 200 \tTraining Loss: 1.066855 \tValidation Loss: 1.075167\n",
      "Validation loss decreased (1.075769 --> 1.075167).         Saving model ...\n",
      "Epoch: 201 \tTraining Loss: 1.066255 \tValidation Loss: 1.074569\n",
      "Validation loss decreased (1.075167 --> 1.074569).         Saving model ...\n",
      "Epoch: 202 \tTraining Loss: 1.065658 \tValidation Loss: 1.073976\n",
      "Validation loss decreased (1.074569 --> 1.073976).         Saving model ...\n",
      "Epoch: 203 \tTraining Loss: 1.065066 \tValidation Loss: 1.073386\n",
      "Validation loss decreased (1.073976 --> 1.073386).         Saving model ...\n",
      "Epoch: 204 \tTraining Loss: 1.064478 \tValidation Loss: 1.072801\n",
      "Validation loss decreased (1.073386 --> 1.072801).         Saving model ...\n",
      "Epoch: 205 \tTraining Loss: 1.063894 \tValidation Loss: 1.072219\n",
      "Validation loss decreased (1.072801 --> 1.072219).         Saving model ...\n",
      "Epoch: 206 \tTraining Loss: 1.063313 \tValidation Loss: 1.071641\n",
      "Validation loss decreased (1.072219 --> 1.071641).         Saving model ...\n",
      "Epoch: 207 \tTraining Loss: 1.062737 \tValidation Loss: 1.071068\n",
      "Validation loss decreased (1.071641 --> 1.071068).         Saving model ...\n",
      "Epoch: 208 \tTraining Loss: 1.062164 \tValidation Loss: 1.070498\n",
      "Validation loss decreased (1.071068 --> 1.070498).         Saving model ...\n",
      "Epoch: 209 \tTraining Loss: 1.061595 \tValidation Loss: 1.069932\n",
      "Validation loss decreased (1.070498 --> 1.069932).         Saving model ...\n",
      "Epoch: 210 \tTraining Loss: 1.061030 \tValidation Loss: 1.069370\n",
      "Validation loss decreased (1.069932 --> 1.069370).         Saving model ...\n",
      "Epoch: 211 \tTraining Loss: 1.060469 \tValidation Loss: 1.068811\n",
      "Validation loss decreased (1.069370 --> 1.068811).         Saving model ...\n",
      "Epoch: 212 \tTraining Loss: 1.059911 \tValidation Loss: 1.068256\n",
      "Validation loss decreased (1.068811 --> 1.068256).         Saving model ...\n",
      "Epoch: 213 \tTraining Loss: 1.059357 \tValidation Loss: 1.067705\n",
      "Validation loss decreased (1.068256 --> 1.067705).         Saving model ...\n",
      "Epoch: 214 \tTraining Loss: 1.058806 \tValidation Loss: 1.067157\n",
      "Validation loss decreased (1.067705 --> 1.067157).         Saving model ...\n",
      "Epoch: 215 \tTraining Loss: 1.058259 \tValidation Loss: 1.066613\n",
      "Validation loss decreased (1.067157 --> 1.066613).         Saving model ...\n",
      "Epoch: 216 \tTraining Loss: 1.057716 \tValidation Loss: 1.066073\n",
      "Validation loss decreased (1.066613 --> 1.066073).         Saving model ...\n",
      "Epoch: 217 \tTraining Loss: 1.057176 \tValidation Loss: 1.065536\n",
      "Validation loss decreased (1.066073 --> 1.065536).         Saving model ...\n",
      "Epoch: 218 \tTraining Loss: 1.056639 \tValidation Loss: 1.065002\n",
      "Validation loss decreased (1.065536 --> 1.065002).         Saving model ...\n",
      "Epoch: 219 \tTraining Loss: 1.056106 \tValidation Loss: 1.064472\n",
      "Validation loss decreased (1.065002 --> 1.064472).         Saving model ...\n",
      "Epoch: 220 \tTraining Loss: 1.055576 \tValidation Loss: 1.063945\n",
      "Validation loss decreased (1.064472 --> 1.063945).         Saving model ...\n",
      "Epoch: 221 \tTraining Loss: 1.055049 \tValidation Loss: 1.063421\n",
      "Validation loss decreased (1.063945 --> 1.063421).         Saving model ...\n",
      "Epoch: 222 \tTraining Loss: 1.054526 \tValidation Loss: 1.062901\n",
      "Validation loss decreased (1.063421 --> 1.062901).         Saving model ...\n",
      "Epoch: 223 \tTraining Loss: 1.054006 \tValidation Loss: 1.062384\n",
      "Validation loss decreased (1.062901 --> 1.062384).         Saving model ...\n",
      "Epoch: 224 \tTraining Loss: 1.053489 \tValidation Loss: 1.061870\n",
      "Validation loss decreased (1.062384 --> 1.061870).         Saving model ...\n",
      "Epoch: 225 \tTraining Loss: 1.052975 \tValidation Loss: 1.061360\n",
      "Validation loss decreased (1.061870 --> 1.061360).         Saving model ...\n",
      "Epoch: 226 \tTraining Loss: 1.052465 \tValidation Loss: 1.060852\n",
      "Validation loss decreased (1.061360 --> 1.060852).         Saving model ...\n",
      "Epoch: 227 \tTraining Loss: 1.051957 \tValidation Loss: 1.060348\n",
      "Validation loss decreased (1.060852 --> 1.060348).         Saving model ...\n",
      "Epoch: 228 \tTraining Loss: 1.051453 \tValidation Loss: 1.059846\n",
      "Validation loss decreased (1.060348 --> 1.059846).         Saving model ...\n",
      "Epoch: 229 \tTraining Loss: 1.050951 \tValidation Loss: 1.059348\n",
      "Validation loss decreased (1.059846 --> 1.059348).         Saving model ...\n",
      "Epoch: 230 \tTraining Loss: 1.050453 \tValidation Loss: 1.058853\n",
      "Validation loss decreased (1.059348 --> 1.058853).         Saving model ...\n",
      "Epoch: 231 \tTraining Loss: 1.049957 \tValidation Loss: 1.058361\n",
      "Validation loss decreased (1.058853 --> 1.058361).         Saving model ...\n",
      "Epoch: 232 \tTraining Loss: 1.049465 \tValidation Loss: 1.057871\n",
      "Validation loss decreased (1.058361 --> 1.057871).         Saving model ...\n",
      "Epoch: 233 \tTraining Loss: 1.048975 \tValidation Loss: 1.057385\n",
      "Validation loss decreased (1.057871 --> 1.057385).         Saving model ...\n",
      "Epoch: 234 \tTraining Loss: 1.048488 \tValidation Loss: 1.056901\n",
      "Validation loss decreased (1.057385 --> 1.056901).         Saving model ...\n",
      "Epoch: 235 \tTraining Loss: 1.048004 \tValidation Loss: 1.056421\n",
      "Validation loss decreased (1.056901 --> 1.056421).         Saving model ...\n",
      "Epoch: 236 \tTraining Loss: 1.047523 \tValidation Loss: 1.055943\n",
      "Validation loss decreased (1.056421 --> 1.055943).         Saving model ...\n",
      "Epoch: 237 \tTraining Loss: 1.047045 \tValidation Loss: 1.055468\n",
      "Validation loss decreased (1.055943 --> 1.055468).         Saving model ...\n",
      "Epoch: 238 \tTraining Loss: 1.046569 \tValidation Loss: 1.054995\n",
      "Validation loss decreased (1.055468 --> 1.054995).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 239 \tTraining Loss: 1.046096 \tValidation Loss: 1.054526\n",
      "Validation loss decreased (1.054995 --> 1.054526).         Saving model ...\n",
      "Epoch: 240 \tTraining Loss: 1.045626 \tValidation Loss: 1.054059\n",
      "Validation loss decreased (1.054526 --> 1.054059).         Saving model ...\n",
      "Epoch: 241 \tTraining Loss: 1.045158 \tValidation Loss: 1.053595\n",
      "Validation loss decreased (1.054059 --> 1.053595).         Saving model ...\n",
      "Epoch: 242 \tTraining Loss: 1.044694 \tValidation Loss: 1.053133\n",
      "Validation loss decreased (1.053595 --> 1.053133).         Saving model ...\n",
      "Epoch: 243 \tTraining Loss: 1.044231 \tValidation Loss: 1.052675\n",
      "Validation loss decreased (1.053133 --> 1.052675).         Saving model ...\n",
      "Epoch: 244 \tTraining Loss: 1.043772 \tValidation Loss: 1.052218\n",
      "Validation loss decreased (1.052675 --> 1.052218).         Saving model ...\n",
      "Epoch: 245 \tTraining Loss: 1.043314 \tValidation Loss: 1.051765\n",
      "Validation loss decreased (1.052218 --> 1.051765).         Saving model ...\n",
      "Epoch: 246 \tTraining Loss: 1.042860 \tValidation Loss: 1.051314\n",
      "Validation loss decreased (1.051765 --> 1.051314).         Saving model ...\n",
      "Epoch: 247 \tTraining Loss: 1.042408 \tValidation Loss: 1.050865\n",
      "Validation loss decreased (1.051314 --> 1.050865).         Saving model ...\n",
      "Epoch: 248 \tTraining Loss: 1.041958 \tValidation Loss: 1.050419\n",
      "Validation loss decreased (1.050865 --> 1.050419).         Saving model ...\n",
      "Epoch: 249 \tTraining Loss: 1.041511 \tValidation Loss: 1.049975\n",
      "Validation loss decreased (1.050419 --> 1.049975).         Saving model ...\n",
      "Epoch: 250 \tTraining Loss: 1.041066 \tValidation Loss: 1.049534\n",
      "Validation loss decreased (1.049975 --> 1.049534).         Saving model ...\n",
      "Epoch: 251 \tTraining Loss: 1.040624 \tValidation Loss: 1.049095\n",
      "Validation loss decreased (1.049534 --> 1.049095).         Saving model ...\n",
      "Epoch: 252 \tTraining Loss: 1.040184 \tValidation Loss: 1.048659\n",
      "Validation loss decreased (1.049095 --> 1.048659).         Saving model ...\n",
      "Epoch: 253 \tTraining Loss: 1.039747 \tValidation Loss: 1.048225\n",
      "Validation loss decreased (1.048659 --> 1.048225).         Saving model ...\n",
      "Epoch: 254 \tTraining Loss: 1.039311 \tValidation Loss: 1.047793\n",
      "Validation loss decreased (1.048225 --> 1.047793).         Saving model ...\n",
      "Epoch: 255 \tTraining Loss: 1.038879 \tValidation Loss: 1.047364\n",
      "Validation loss decreased (1.047793 --> 1.047364).         Saving model ...\n",
      "Epoch: 256 \tTraining Loss: 1.038448 \tValidation Loss: 1.046937\n",
      "Validation loss decreased (1.047364 --> 1.046937).         Saving model ...\n",
      "Epoch: 257 \tTraining Loss: 1.038020 \tValidation Loss: 1.046513\n",
      "Validation loss decreased (1.046937 --> 1.046513).         Saving model ...\n",
      "Epoch: 258 \tTraining Loss: 1.037594 \tValidation Loss: 1.046090\n",
      "Validation loss decreased (1.046513 --> 1.046090).         Saving model ...\n",
      "Epoch: 259 \tTraining Loss: 1.037170 \tValidation Loss: 1.045670\n",
      "Validation loss decreased (1.046090 --> 1.045670).         Saving model ...\n",
      "Epoch: 260 \tTraining Loss: 1.036748 \tValidation Loss: 1.045252\n",
      "Validation loss decreased (1.045670 --> 1.045252).         Saving model ...\n",
      "Epoch: 261 \tTraining Loss: 1.036329 \tValidation Loss: 1.044836\n",
      "Validation loss decreased (1.045252 --> 1.044836).         Saving model ...\n",
      "Epoch: 262 \tTraining Loss: 1.035912 \tValidation Loss: 1.044423\n",
      "Validation loss decreased (1.044836 --> 1.044423).         Saving model ...\n",
      "Epoch: 263 \tTraining Loss: 1.035497 \tValidation Loss: 1.044011\n",
      "Validation loss decreased (1.044423 --> 1.044011).         Saving model ...\n",
      "Epoch: 264 \tTraining Loss: 1.035084 \tValidation Loss: 1.043602\n",
      "Validation loss decreased (1.044011 --> 1.043602).         Saving model ...\n",
      "Epoch: 265 \tTraining Loss: 1.034673 \tValidation Loss: 1.043195\n",
      "Validation loss decreased (1.043602 --> 1.043195).         Saving model ...\n",
      "Epoch: 266 \tTraining Loss: 1.034264 \tValidation Loss: 1.042790\n",
      "Validation loss decreased (1.043195 --> 1.042790).         Saving model ...\n",
      "Epoch: 267 \tTraining Loss: 1.033857 \tValidation Loss: 1.042387\n",
      "Validation loss decreased (1.042790 --> 1.042387).         Saving model ...\n",
      "Epoch: 268 \tTraining Loss: 1.033453 \tValidation Loss: 1.041986\n",
      "Validation loss decreased (1.042387 --> 1.041986).         Saving model ...\n",
      "Epoch: 269 \tTraining Loss: 1.033050 \tValidation Loss: 1.041587\n",
      "Validation loss decreased (1.041986 --> 1.041587).         Saving model ...\n",
      "Epoch: 270 \tTraining Loss: 1.032650 \tValidation Loss: 1.041191\n",
      "Validation loss decreased (1.041587 --> 1.041191).         Saving model ...\n",
      "Epoch: 271 \tTraining Loss: 1.032251 \tValidation Loss: 1.040796\n",
      "Validation loss decreased (1.041191 --> 1.040796).         Saving model ...\n",
      "Epoch: 272 \tTraining Loss: 1.031854 \tValidation Loss: 1.040403\n",
      "Validation loss decreased (1.040796 --> 1.040403).         Saving model ...\n",
      "Epoch: 273 \tTraining Loss: 1.031460 \tValidation Loss: 1.040012\n",
      "Validation loss decreased (1.040403 --> 1.040012).         Saving model ...\n",
      "Epoch: 274 \tTraining Loss: 1.031067 \tValidation Loss: 1.039623\n",
      "Validation loss decreased (1.040012 --> 1.039623).         Saving model ...\n",
      "Epoch: 275 \tTraining Loss: 1.030676 \tValidation Loss: 1.039236\n",
      "Validation loss decreased (1.039623 --> 1.039236).         Saving model ...\n",
      "Epoch: 276 \tTraining Loss: 1.030288 \tValidation Loss: 1.038852\n",
      "Validation loss decreased (1.039236 --> 1.038852).         Saving model ...\n",
      "Epoch: 277 \tTraining Loss: 1.029901 \tValidation Loss: 1.038468\n",
      "Validation loss decreased (1.038852 --> 1.038468).         Saving model ...\n",
      "Epoch: 278 \tTraining Loss: 1.029516 \tValidation Loss: 1.038087\n",
      "Validation loss decreased (1.038468 --> 1.038087).         Saving model ...\n",
      "Epoch: 279 \tTraining Loss: 1.029132 \tValidation Loss: 1.037708\n",
      "Validation loss decreased (1.038087 --> 1.037708).         Saving model ...\n",
      "Epoch: 280 \tTraining Loss: 1.028751 \tValidation Loss: 1.037330\n",
      "Validation loss decreased (1.037708 --> 1.037330).         Saving model ...\n",
      "Epoch: 281 \tTraining Loss: 1.028371 \tValidation Loss: 1.036955\n",
      "Validation loss decreased (1.037330 --> 1.036955).         Saving model ...\n",
      "Epoch: 282 \tTraining Loss: 1.027994 \tValidation Loss: 1.036581\n",
      "Validation loss decreased (1.036955 --> 1.036581).         Saving model ...\n",
      "Epoch: 283 \tTraining Loss: 1.027618 \tValidation Loss: 1.036209\n",
      "Validation loss decreased (1.036581 --> 1.036209).         Saving model ...\n",
      "Epoch: 284 \tTraining Loss: 1.027244 \tValidation Loss: 1.035839\n",
      "Validation loss decreased (1.036209 --> 1.035839).         Saving model ...\n",
      "Epoch: 285 \tTraining Loss: 1.026871 \tValidation Loss: 1.035470\n",
      "Validation loss decreased (1.035839 --> 1.035470).         Saving model ...\n",
      "Epoch: 286 \tTraining Loss: 1.026501 \tValidation Loss: 1.035104\n",
      "Validation loss decreased (1.035470 --> 1.035104).         Saving model ...\n",
      "Epoch: 287 \tTraining Loss: 1.026132 \tValidation Loss: 1.034739\n",
      "Validation loss decreased (1.035104 --> 1.034739).         Saving model ...\n",
      "Epoch: 288 \tTraining Loss: 1.025765 \tValidation Loss: 1.034375\n",
      "Validation loss decreased (1.034739 --> 1.034375).         Saving model ...\n",
      "Epoch: 289 \tTraining Loss: 1.025399 \tValidation Loss: 1.034014\n",
      "Validation loss decreased (1.034375 --> 1.034014).         Saving model ...\n",
      "Epoch: 290 \tTraining Loss: 1.025035 \tValidation Loss: 1.033654\n",
      "Validation loss decreased (1.034014 --> 1.033654).         Saving model ...\n",
      "Epoch: 291 \tTraining Loss: 1.024673 \tValidation Loss: 1.033296\n",
      "Validation loss decreased (1.033654 --> 1.033296).         Saving model ...\n",
      "Epoch: 292 \tTraining Loss: 1.024313 \tValidation Loss: 1.032940\n",
      "Validation loss decreased (1.033296 --> 1.032940).         Saving model ...\n",
      "Epoch: 293 \tTraining Loss: 1.023954 \tValidation Loss: 1.032585\n",
      "Validation loss decreased (1.032940 --> 1.032585).         Saving model ...\n",
      "Epoch: 294 \tTraining Loss: 1.023597 \tValidation Loss: 1.032232\n",
      "Validation loss decreased (1.032585 --> 1.032232).         Saving model ...\n",
      "Epoch: 295 \tTraining Loss: 1.023241 \tValidation Loss: 1.031880\n",
      "Validation loss decreased (1.032232 --> 1.031880).         Saving model ...\n",
      "Epoch: 296 \tTraining Loss: 1.022887 \tValidation Loss: 1.031530\n",
      "Validation loss decreased (1.031880 --> 1.031530).         Saving model ...\n",
      "Epoch: 297 \tTraining Loss: 1.022535 \tValidation Loss: 1.031182\n",
      "Validation loss decreased (1.031530 --> 1.031182).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 298 \tTraining Loss: 1.022184 \tValidation Loss: 1.030835\n",
      "Validation loss decreased (1.031182 --> 1.030835).         Saving model ...\n",
      "Epoch: 299 \tTraining Loss: 1.021835 \tValidation Loss: 1.030490\n",
      "Validation loss decreased (1.030835 --> 1.030490).         Saving model ...\n",
      "Epoch: 300 \tTraining Loss: 1.021488 \tValidation Loss: 1.030147\n",
      "Validation loss decreased (1.030490 --> 1.030147).         Saving model ...\n",
      "Epoch: 301 \tTraining Loss: 1.021141 \tValidation Loss: 1.029805\n",
      "Validation loss decreased (1.030147 --> 1.029805).         Saving model ...\n",
      "Epoch: 302 \tTraining Loss: 1.020797 \tValidation Loss: 1.029464\n",
      "Validation loss decreased (1.029805 --> 1.029464).         Saving model ...\n",
      "Epoch: 303 \tTraining Loss: 1.020454 \tValidation Loss: 1.029125\n",
      "Validation loss decreased (1.029464 --> 1.029125).         Saving model ...\n",
      "Epoch: 304 \tTraining Loss: 1.020112 \tValidation Loss: 1.028788\n",
      "Validation loss decreased (1.029125 --> 1.028788).         Saving model ...\n",
      "Epoch: 305 \tTraining Loss: 1.019772 \tValidation Loss: 1.028452\n",
      "Validation loss decreased (1.028788 --> 1.028452).         Saving model ...\n",
      "Epoch: 306 \tTraining Loss: 1.019434 \tValidation Loss: 1.028117\n",
      "Validation loss decreased (1.028452 --> 1.028117).         Saving model ...\n",
      "Epoch: 307 \tTraining Loss: 1.019097 \tValidation Loss: 1.027785\n",
      "Validation loss decreased (1.028117 --> 1.027785).         Saving model ...\n",
      "Epoch: 308 \tTraining Loss: 1.018761 \tValidation Loss: 1.027453\n",
      "Validation loss decreased (1.027785 --> 1.027453).         Saving model ...\n",
      "Epoch: 309 \tTraining Loss: 1.018427 \tValidation Loss: 1.027123\n",
      "Validation loss decreased (1.027453 --> 1.027123).         Saving model ...\n",
      "Epoch: 310 \tTraining Loss: 1.018094 \tValidation Loss: 1.026794\n",
      "Validation loss decreased (1.027123 --> 1.026794).         Saving model ...\n",
      "Epoch: 311 \tTraining Loss: 1.017763 \tValidation Loss: 1.026467\n",
      "Validation loss decreased (1.026794 --> 1.026467).         Saving model ...\n",
      "Epoch: 312 \tTraining Loss: 1.017433 \tValidation Loss: 1.026142\n",
      "Validation loss decreased (1.026467 --> 1.026142).         Saving model ...\n",
      "Epoch: 313 \tTraining Loss: 1.017105 \tValidation Loss: 1.025817\n",
      "Validation loss decreased (1.026142 --> 1.025817).         Saving model ...\n",
      "Epoch: 314 \tTraining Loss: 1.016778 \tValidation Loss: 1.025494\n",
      "Validation loss decreased (1.025817 --> 1.025494).         Saving model ...\n",
      "Epoch: 315 \tTraining Loss: 1.016452 \tValidation Loss: 1.025173\n",
      "Validation loss decreased (1.025494 --> 1.025173).         Saving model ...\n",
      "Epoch: 316 \tTraining Loss: 1.016128 \tValidation Loss: 1.024853\n",
      "Validation loss decreased (1.025173 --> 1.024853).         Saving model ...\n",
      "Epoch: 317 \tTraining Loss: 1.015805 \tValidation Loss: 1.024534\n",
      "Validation loss decreased (1.024853 --> 1.024534).         Saving model ...\n",
      "Epoch: 318 \tTraining Loss: 1.015483 \tValidation Loss: 1.024217\n",
      "Validation loss decreased (1.024534 --> 1.024217).         Saving model ...\n",
      "Epoch: 319 \tTraining Loss: 1.015163 \tValidation Loss: 1.023901\n",
      "Validation loss decreased (1.024217 --> 1.023901).         Saving model ...\n",
      "Epoch: 320 \tTraining Loss: 1.014844 \tValidation Loss: 1.023586\n",
      "Validation loss decreased (1.023901 --> 1.023586).         Saving model ...\n",
      "Epoch: 321 \tTraining Loss: 1.014526 \tValidation Loss: 1.023272\n",
      "Validation loss decreased (1.023586 --> 1.023272).         Saving model ...\n",
      "Epoch: 322 \tTraining Loss: 1.014210 \tValidation Loss: 1.022960\n",
      "Validation loss decreased (1.023272 --> 1.022960).         Saving model ...\n",
      "Epoch: 323 \tTraining Loss: 1.013895 \tValidation Loss: 1.022650\n",
      "Validation loss decreased (1.022960 --> 1.022650).         Saving model ...\n",
      "Epoch: 324 \tTraining Loss: 1.013581 \tValidation Loss: 1.022340\n",
      "Validation loss decreased (1.022650 --> 1.022340).         Saving model ...\n",
      "Epoch: 325 \tTraining Loss: 1.013269 \tValidation Loss: 1.022032\n",
      "Validation loss decreased (1.022340 --> 1.022032).         Saving model ...\n",
      "Epoch: 326 \tTraining Loss: 1.012958 \tValidation Loss: 1.021725\n",
      "Validation loss decreased (1.022032 --> 1.021725).         Saving model ...\n",
      "Epoch: 327 \tTraining Loss: 1.012648 \tValidation Loss: 1.021419\n",
      "Validation loss decreased (1.021725 --> 1.021419).         Saving model ...\n",
      "Epoch: 328 \tTraining Loss: 1.012339 \tValidation Loss: 1.021115\n",
      "Validation loss decreased (1.021419 --> 1.021115).         Saving model ...\n",
      "Epoch: 329 \tTraining Loss: 1.012032 \tValidation Loss: 1.020812\n",
      "Validation loss decreased (1.021115 --> 1.020812).         Saving model ...\n",
      "Epoch: 330 \tTraining Loss: 1.011726 \tValidation Loss: 1.020510\n",
      "Validation loss decreased (1.020812 --> 1.020510).         Saving model ...\n",
      "Epoch: 331 \tTraining Loss: 1.011421 \tValidation Loss: 1.020209\n",
      "Validation loss decreased (1.020510 --> 1.020209).         Saving model ...\n",
      "Epoch: 332 \tTraining Loss: 1.011117 \tValidation Loss: 1.019910\n",
      "Validation loss decreased (1.020209 --> 1.019910).         Saving model ...\n",
      "Epoch: 333 \tTraining Loss: 1.010814 \tValidation Loss: 1.019612\n",
      "Validation loss decreased (1.019910 --> 1.019612).         Saving model ...\n",
      "Epoch: 334 \tTraining Loss: 1.010513 \tValidation Loss: 1.019315\n",
      "Validation loss decreased (1.019612 --> 1.019315).         Saving model ...\n",
      "Epoch: 335 \tTraining Loss: 1.010213 \tValidation Loss: 1.019019\n",
      "Validation loss decreased (1.019315 --> 1.019019).         Saving model ...\n",
      "Epoch: 336 \tTraining Loss: 1.009914 \tValidation Loss: 1.018724\n",
      "Validation loss decreased (1.019019 --> 1.018724).         Saving model ...\n",
      "Epoch: 337 \tTraining Loss: 1.009616 \tValidation Loss: 1.018431\n",
      "Validation loss decreased (1.018724 --> 1.018431).         Saving model ...\n",
      "Epoch: 338 \tTraining Loss: 1.009320 \tValidation Loss: 1.018138\n",
      "Validation loss decreased (1.018431 --> 1.018138).         Saving model ...\n",
      "Epoch: 339 \tTraining Loss: 1.009024 \tValidation Loss: 1.017847\n",
      "Validation loss decreased (1.018138 --> 1.017847).         Saving model ...\n",
      "Epoch: 340 \tTraining Loss: 1.008730 \tValidation Loss: 1.017557\n",
      "Validation loss decreased (1.017847 --> 1.017557).         Saving model ...\n",
      "Epoch: 341 \tTraining Loss: 1.008437 \tValidation Loss: 1.017268\n",
      "Validation loss decreased (1.017557 --> 1.017268).         Saving model ...\n",
      "Epoch: 342 \tTraining Loss: 1.008145 \tValidation Loss: 1.016981\n",
      "Validation loss decreased (1.017268 --> 1.016981).         Saving model ...\n",
      "Epoch: 343 \tTraining Loss: 1.007854 \tValidation Loss: 1.016694\n",
      "Validation loss decreased (1.016981 --> 1.016694).         Saving model ...\n",
      "Epoch: 344 \tTraining Loss: 1.007564 \tValidation Loss: 1.016409\n",
      "Validation loss decreased (1.016694 --> 1.016409).         Saving model ...\n",
      "Epoch: 345 \tTraining Loss: 1.007276 \tValidation Loss: 1.016125\n",
      "Validation loss decreased (1.016409 --> 1.016125).         Saving model ...\n",
      "Epoch: 346 \tTraining Loss: 1.006988 \tValidation Loss: 1.015841\n",
      "Validation loss decreased (1.016125 --> 1.015841).         Saving model ...\n",
      "Epoch: 347 \tTraining Loss: 1.006702 \tValidation Loss: 1.015559\n",
      "Validation loss decreased (1.015841 --> 1.015559).         Saving model ...\n",
      "Epoch: 348 \tTraining Loss: 1.006416 \tValidation Loss: 1.015278\n",
      "Validation loss decreased (1.015559 --> 1.015278).         Saving model ...\n",
      "Epoch: 349 \tTraining Loss: 1.006132 \tValidation Loss: 1.014998\n",
      "Validation loss decreased (1.015278 --> 1.014998).         Saving model ...\n",
      "Epoch: 350 \tTraining Loss: 1.005849 \tValidation Loss: 1.014719\n",
      "Validation loss decreased (1.014998 --> 1.014719).         Saving model ...\n",
      "Epoch: 351 \tTraining Loss: 1.005567 \tValidation Loss: 1.014442\n",
      "Validation loss decreased (1.014719 --> 1.014442).         Saving model ...\n",
      "Epoch: 352 \tTraining Loss: 1.005286 \tValidation Loss: 1.014165\n",
      "Validation loss decreased (1.014442 --> 1.014165).         Saving model ...\n",
      "Epoch: 353 \tTraining Loss: 1.005005 \tValidation Loss: 1.013889\n",
      "Validation loss decreased (1.014165 --> 1.013889).         Saving model ...\n",
      "Epoch: 354 \tTraining Loss: 1.004726 \tValidation Loss: 1.013615\n",
      "Validation loss decreased (1.013889 --> 1.013615).         Saving model ...\n",
      "Epoch: 355 \tTraining Loss: 1.004449 \tValidation Loss: 1.013341\n",
      "Validation loss decreased (1.013615 --> 1.013341).         Saving model ...\n",
      "Epoch: 356 \tTraining Loss: 1.004172 \tValidation Loss: 1.013069\n",
      "Validation loss decreased (1.013341 --> 1.013069).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 357 \tTraining Loss: 1.003896 \tValidation Loss: 1.012797\n",
      "Validation loss decreased (1.013069 --> 1.012797).         Saving model ...\n",
      "Epoch: 358 \tTraining Loss: 1.003621 \tValidation Loss: 1.012527\n",
      "Validation loss decreased (1.012797 --> 1.012527).         Saving model ...\n",
      "Epoch: 359 \tTraining Loss: 1.003347 \tValidation Loss: 1.012257\n",
      "Validation loss decreased (1.012527 --> 1.012257).         Saving model ...\n",
      "Epoch: 360 \tTraining Loss: 1.003074 \tValidation Loss: 1.011989\n",
      "Validation loss decreased (1.012257 --> 1.011989).         Saving model ...\n",
      "Epoch: 361 \tTraining Loss: 1.002802 \tValidation Loss: 1.011721\n",
      "Validation loss decreased (1.011989 --> 1.011721).         Saving model ...\n",
      "Epoch: 362 \tTraining Loss: 1.002531 \tValidation Loss: 1.011455\n",
      "Validation loss decreased (1.011721 --> 1.011455).         Saving model ...\n",
      "Epoch: 363 \tTraining Loss: 1.002261 \tValidation Loss: 1.011189\n",
      "Validation loss decreased (1.011455 --> 1.011189).         Saving model ...\n",
      "Epoch: 364 \tTraining Loss: 1.001992 \tValidation Loss: 1.010925\n",
      "Validation loss decreased (1.011189 --> 1.010925).         Saving model ...\n",
      "Epoch: 365 \tTraining Loss: 1.001724 \tValidation Loss: 1.010661\n",
      "Validation loss decreased (1.010925 --> 1.010661).         Saving model ...\n",
      "Epoch: 366 \tTraining Loss: 1.001457 \tValidation Loss: 1.010398\n",
      "Validation loss decreased (1.010661 --> 1.010398).         Saving model ...\n",
      "Epoch: 367 \tTraining Loss: 1.001191 \tValidation Loss: 1.010137\n",
      "Validation loss decreased (1.010398 --> 1.010137).         Saving model ...\n",
      "Epoch: 368 \tTraining Loss: 1.000926 \tValidation Loss: 1.009876\n",
      "Validation loss decreased (1.010137 --> 1.009876).         Saving model ...\n",
      "Epoch: 369 \tTraining Loss: 1.000662 \tValidation Loss: 1.009616\n",
      "Validation loss decreased (1.009876 --> 1.009616).         Saving model ...\n",
      "Epoch: 370 \tTraining Loss: 1.000399 \tValidation Loss: 1.009358\n",
      "Validation loss decreased (1.009616 --> 1.009358).         Saving model ...\n",
      "Epoch: 371 \tTraining Loss: 1.000137 \tValidation Loss: 1.009100\n",
      "Validation loss decreased (1.009358 --> 1.009100).         Saving model ...\n",
      "Epoch: 372 \tTraining Loss: 0.999875 \tValidation Loss: 1.008843\n",
      "Validation loss decreased (1.009100 --> 1.008843).         Saving model ...\n",
      "Epoch: 373 \tTraining Loss: 0.999615 \tValidation Loss: 1.008587\n",
      "Validation loss decreased (1.008843 --> 1.008587).         Saving model ...\n",
      "Epoch: 374 \tTraining Loss: 0.999355 \tValidation Loss: 1.008332\n",
      "Validation loss decreased (1.008587 --> 1.008332).         Saving model ...\n",
      "Epoch: 375 \tTraining Loss: 0.999097 \tValidation Loss: 1.008078\n",
      "Validation loss decreased (1.008332 --> 1.008078).         Saving model ...\n",
      "Epoch: 376 \tTraining Loss: 0.998839 \tValidation Loss: 1.007825\n",
      "Validation loss decreased (1.008078 --> 1.007825).         Saving model ...\n",
      "Epoch: 377 \tTraining Loss: 0.998582 \tValidation Loss: 1.007572\n",
      "Validation loss decreased (1.007825 --> 1.007572).         Saving model ...\n",
      "Epoch: 378 \tTraining Loss: 0.998326 \tValidation Loss: 1.007321\n",
      "Validation loss decreased (1.007572 --> 1.007321).         Saving model ...\n",
      "Epoch: 379 \tTraining Loss: 0.998071 \tValidation Loss: 1.007070\n",
      "Validation loss decreased (1.007321 --> 1.007070).         Saving model ...\n",
      "Epoch: 380 \tTraining Loss: 0.997817 \tValidation Loss: 1.006821\n",
      "Validation loss decreased (1.007070 --> 1.006821).         Saving model ...\n",
      "Epoch: 381 \tTraining Loss: 0.997564 \tValidation Loss: 1.006572\n",
      "Validation loss decreased (1.006821 --> 1.006572).         Saving model ...\n",
      "Epoch: 382 \tTraining Loss: 0.997312 \tValidation Loss: 1.006324\n",
      "Validation loss decreased (1.006572 --> 1.006324).         Saving model ...\n",
      "Epoch: 383 \tTraining Loss: 0.997060 \tValidation Loss: 1.006077\n",
      "Validation loss decreased (1.006324 --> 1.006077).         Saving model ...\n",
      "Epoch: 384 \tTraining Loss: 0.996809 \tValidation Loss: 1.005831\n",
      "Validation loss decreased (1.006077 --> 1.005831).         Saving model ...\n",
      "Epoch: 385 \tTraining Loss: 0.996560 \tValidation Loss: 1.005585\n",
      "Validation loss decreased (1.005831 --> 1.005585).         Saving model ...\n",
      "Epoch: 386 \tTraining Loss: 0.996311 \tValidation Loss: 1.005341\n",
      "Validation loss decreased (1.005585 --> 1.005341).         Saving model ...\n",
      "Epoch: 387 \tTraining Loss: 0.996063 \tValidation Loss: 1.005097\n",
      "Validation loss decreased (1.005341 --> 1.005097).         Saving model ...\n",
      "Epoch: 388 \tTraining Loss: 0.995815 \tValidation Loss: 1.004855\n",
      "Validation loss decreased (1.005097 --> 1.004855).         Saving model ...\n",
      "Epoch: 389 \tTraining Loss: 0.995569 \tValidation Loss: 1.004613\n",
      "Validation loss decreased (1.004855 --> 1.004613).         Saving model ...\n",
      "Epoch: 390 \tTraining Loss: 0.995323 \tValidation Loss: 1.004372\n",
      "Validation loss decreased (1.004613 --> 1.004372).         Saving model ...\n",
      "Epoch: 391 \tTraining Loss: 0.995078 \tValidation Loss: 1.004131\n",
      "Validation loss decreased (1.004372 --> 1.004131).         Saving model ...\n",
      "Epoch: 392 \tTraining Loss: 0.994834 \tValidation Loss: 1.003892\n",
      "Validation loss decreased (1.004131 --> 1.003892).         Saving model ...\n",
      "Epoch: 393 \tTraining Loss: 0.994591 \tValidation Loss: 1.003653\n",
      "Validation loss decreased (1.003892 --> 1.003653).         Saving model ...\n",
      "Epoch: 394 \tTraining Loss: 0.994349 \tValidation Loss: 1.003415\n",
      "Validation loss decreased (1.003653 --> 1.003415).         Saving model ...\n",
      "Epoch: 395 \tTraining Loss: 0.994107 \tValidation Loss: 1.003178\n",
      "Validation loss decreased (1.003415 --> 1.003178).         Saving model ...\n",
      "Epoch: 396 \tTraining Loss: 0.993867 \tValidation Loss: 1.002942\n",
      "Validation loss decreased (1.003178 --> 1.002942).         Saving model ...\n",
      "Epoch: 397 \tTraining Loss: 0.993627 \tValidation Loss: 1.002707\n",
      "Validation loss decreased (1.002942 --> 1.002707).         Saving model ...\n",
      "Epoch: 398 \tTraining Loss: 0.993388 \tValidation Loss: 1.002472\n",
      "Validation loss decreased (1.002707 --> 1.002472).         Saving model ...\n",
      "Epoch: 399 \tTraining Loss: 0.993149 \tValidation Loss: 1.002238\n",
      "Validation loss decreased (1.002472 --> 1.002238).         Saving model ...\n",
      "Epoch: 400 \tTraining Loss: 0.992912 \tValidation Loss: 1.002005\n",
      "Validation loss decreased (1.002238 --> 1.002005).         Saving model ...\n",
      "Epoch: 401 \tTraining Loss: 0.992675 \tValidation Loss: 1.001773\n",
      "Validation loss decreased (1.002005 --> 1.001773).         Saving model ...\n",
      "Epoch: 402 \tTraining Loss: 0.992439 \tValidation Loss: 1.001541\n",
      "Validation loss decreased (1.001773 --> 1.001541).         Saving model ...\n",
      "Epoch: 403 \tTraining Loss: 0.992204 \tValidation Loss: 1.001311\n",
      "Validation loss decreased (1.001541 --> 1.001311).         Saving model ...\n",
      "Epoch: 404 \tTraining Loss: 0.991969 \tValidation Loss: 1.001081\n",
      "Validation loss decreased (1.001311 --> 1.001081).         Saving model ...\n",
      "Epoch: 405 \tTraining Loss: 0.991735 \tValidation Loss: 1.000852\n",
      "Validation loss decreased (1.001081 --> 1.000852).         Saving model ...\n",
      "Epoch: 406 \tTraining Loss: 0.991502 \tValidation Loss: 1.000623\n",
      "Validation loss decreased (1.000852 --> 1.000623).         Saving model ...\n",
      "Epoch: 407 \tTraining Loss: 0.991270 \tValidation Loss: 1.000395\n",
      "Validation loss decreased (1.000623 --> 1.000395).         Saving model ...\n",
      "Epoch: 408 \tTraining Loss: 0.991039 \tValidation Loss: 1.000168\n",
      "Validation loss decreased (1.000395 --> 1.000168).         Saving model ...\n",
      "Epoch: 409 \tTraining Loss: 0.990808 \tValidation Loss: 0.999942\n",
      "Validation loss decreased (1.000168 --> 0.999942).         Saving model ...\n",
      "Epoch: 410 \tTraining Loss: 0.990578 \tValidation Loss: 0.999717\n",
      "Validation loss decreased (0.999942 --> 0.999717).         Saving model ...\n",
      "Epoch: 411 \tTraining Loss: 0.990349 \tValidation Loss: 0.999492\n",
      "Validation loss decreased (0.999717 --> 0.999492).         Saving model ...\n",
      "Epoch: 412 \tTraining Loss: 0.990120 \tValidation Loss: 0.999268\n",
      "Validation loss decreased (0.999492 --> 0.999268).         Saving model ...\n",
      "Epoch: 413 \tTraining Loss: 0.989892 \tValidation Loss: 0.999045\n",
      "Validation loss decreased (0.999268 --> 0.999045).         Saving model ...\n",
      "Epoch: 414 \tTraining Loss: 0.989665 \tValidation Loss: 0.998822\n",
      "Validation loss decreased (0.999045 --> 0.998822).         Saving model ...\n",
      "Epoch: 415 \tTraining Loss: 0.989439 \tValidation Loss: 0.998600\n",
      "Validation loss decreased (0.998822 --> 0.998600).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 416 \tTraining Loss: 0.989213 \tValidation Loss: 0.998379\n",
      "Validation loss decreased (0.998600 --> 0.998379).         Saving model ...\n",
      "Epoch: 417 \tTraining Loss: 0.988988 \tValidation Loss: 0.998159\n",
      "Validation loss decreased (0.998379 --> 0.998159).         Saving model ...\n",
      "Epoch: 418 \tTraining Loss: 0.988764 \tValidation Loss: 0.997939\n",
      "Validation loss decreased (0.998159 --> 0.997939).         Saving model ...\n",
      "Epoch: 419 \tTraining Loss: 0.988540 \tValidation Loss: 0.997720\n",
      "Validation loss decreased (0.997939 --> 0.997720).         Saving model ...\n",
      "Epoch: 420 \tTraining Loss: 0.988318 \tValidation Loss: 0.997502\n",
      "Validation loss decreased (0.997720 --> 0.997502).         Saving model ...\n",
      "Epoch: 421 \tTraining Loss: 0.988095 \tValidation Loss: 0.997284\n",
      "Validation loss decreased (0.997502 --> 0.997284).         Saving model ...\n",
      "Epoch: 422 \tTraining Loss: 0.987874 \tValidation Loss: 0.997068\n",
      "Validation loss decreased (0.997284 --> 0.997068).         Saving model ...\n",
      "Epoch: 423 \tTraining Loss: 0.987653 \tValidation Loss: 0.996851\n",
      "Validation loss decreased (0.997068 --> 0.996851).         Saving model ...\n",
      "Epoch: 424 \tTraining Loss: 0.987433 \tValidation Loss: 0.996636\n",
      "Validation loss decreased (0.996851 --> 0.996636).         Saving model ...\n",
      "Epoch: 425 \tTraining Loss: 0.987214 \tValidation Loss: 0.996421\n",
      "Validation loss decreased (0.996636 --> 0.996421).         Saving model ...\n",
      "Epoch: 426 \tTraining Loss: 0.986995 \tValidation Loss: 0.996207\n",
      "Validation loss decreased (0.996421 --> 0.996207).         Saving model ...\n",
      "Epoch: 427 \tTraining Loss: 0.986777 \tValidation Loss: 0.995993\n",
      "Validation loss decreased (0.996207 --> 0.995993).         Saving model ...\n",
      "Epoch: 428 \tTraining Loss: 0.986559 \tValidation Loss: 0.995780\n",
      "Validation loss decreased (0.995993 --> 0.995780).         Saving model ...\n",
      "Epoch: 429 \tTraining Loss: 0.986343 \tValidation Loss: 0.995568\n",
      "Validation loss decreased (0.995780 --> 0.995568).         Saving model ...\n",
      "Epoch: 430 \tTraining Loss: 0.986127 \tValidation Loss: 0.995357\n",
      "Validation loss decreased (0.995568 --> 0.995357).         Saving model ...\n",
      "Epoch: 431 \tTraining Loss: 0.985911 \tValidation Loss: 0.995146\n",
      "Validation loss decreased (0.995357 --> 0.995146).         Saving model ...\n",
      "Epoch: 432 \tTraining Loss: 0.985696 \tValidation Loss: 0.994936\n",
      "Validation loss decreased (0.995146 --> 0.994936).         Saving model ...\n",
      "Epoch: 433 \tTraining Loss: 0.985482 \tValidation Loss: 0.994726\n",
      "Validation loss decreased (0.994936 --> 0.994726).         Saving model ...\n",
      "Epoch: 434 \tTraining Loss: 0.985269 \tValidation Loss: 0.994517\n",
      "Validation loss decreased (0.994726 --> 0.994517).         Saving model ...\n",
      "Epoch: 435 \tTraining Loss: 0.985056 \tValidation Loss: 0.994309\n",
      "Validation loss decreased (0.994517 --> 0.994309).         Saving model ...\n",
      "Epoch: 436 \tTraining Loss: 0.984844 \tValidation Loss: 0.994102\n",
      "Validation loss decreased (0.994309 --> 0.994102).         Saving model ...\n",
      "Epoch: 437 \tTraining Loss: 0.984632 \tValidation Loss: 0.993895\n",
      "Validation loss decreased (0.994102 --> 0.993895).         Saving model ...\n",
      "Epoch: 438 \tTraining Loss: 0.984421 \tValidation Loss: 0.993688\n",
      "Validation loss decreased (0.993895 --> 0.993688).         Saving model ...\n",
      "Epoch: 439 \tTraining Loss: 0.984211 \tValidation Loss: 0.993483\n",
      "Validation loss decreased (0.993688 --> 0.993483).         Saving model ...\n",
      "Epoch: 440 \tTraining Loss: 0.984001 \tValidation Loss: 0.993277\n",
      "Validation loss decreased (0.993483 --> 0.993277).         Saving model ...\n",
      "Epoch: 441 \tTraining Loss: 0.983792 \tValidation Loss: 0.993073\n",
      "Validation loss decreased (0.993277 --> 0.993073).         Saving model ...\n",
      "Epoch: 442 \tTraining Loss: 0.983584 \tValidation Loss: 0.992869\n",
      "Validation loss decreased (0.993073 --> 0.992869).         Saving model ...\n",
      "Epoch: 443 \tTraining Loss: 0.983376 \tValidation Loss: 0.992666\n",
      "Validation loss decreased (0.992869 --> 0.992666).         Saving model ...\n",
      "Epoch: 444 \tTraining Loss: 0.983169 \tValidation Loss: 0.992463\n",
      "Validation loss decreased (0.992666 --> 0.992463).         Saving model ...\n",
      "Epoch: 445 \tTraining Loss: 0.982962 \tValidation Loss: 0.992261\n",
      "Validation loss decreased (0.992463 --> 0.992261).         Saving model ...\n",
      "Epoch: 446 \tTraining Loss: 0.982756 \tValidation Loss: 0.992060\n",
      "Validation loss decreased (0.992261 --> 0.992060).         Saving model ...\n",
      "Epoch: 447 \tTraining Loss: 0.982551 \tValidation Loss: 0.991859\n",
      "Validation loss decreased (0.992060 --> 0.991859).         Saving model ...\n",
      "Epoch: 448 \tTraining Loss: 0.982346 \tValidation Loss: 0.991659\n",
      "Validation loss decreased (0.991859 --> 0.991659).         Saving model ...\n",
      "Epoch: 449 \tTraining Loss: 0.982142 \tValidation Loss: 0.991460\n",
      "Validation loss decreased (0.991659 --> 0.991460).         Saving model ...\n",
      "Epoch: 450 \tTraining Loss: 0.981938 \tValidation Loss: 0.991261\n",
      "Validation loss decreased (0.991460 --> 0.991261).         Saving model ...\n",
      "Epoch: 451 \tTraining Loss: 0.981735 \tValidation Loss: 0.991062\n",
      "Validation loss decreased (0.991261 --> 0.991062).         Saving model ...\n",
      "Epoch: 452 \tTraining Loss: 0.981533 \tValidation Loss: 0.990864\n",
      "Validation loss decreased (0.991062 --> 0.990864).         Saving model ...\n",
      "Epoch: 453 \tTraining Loss: 0.981331 \tValidation Loss: 0.990667\n",
      "Validation loss decreased (0.990864 --> 0.990667).         Saving model ...\n",
      "Epoch: 454 \tTraining Loss: 0.981130 \tValidation Loss: 0.990471\n",
      "Validation loss decreased (0.990667 --> 0.990471).         Saving model ...\n",
      "Epoch: 455 \tTraining Loss: 0.980929 \tValidation Loss: 0.990275\n",
      "Validation loss decreased (0.990471 --> 0.990275).         Saving model ...\n",
      "Epoch: 456 \tTraining Loss: 0.980729 \tValidation Loss: 0.990079\n",
      "Validation loss decreased (0.990275 --> 0.990079).         Saving model ...\n",
      "Epoch: 457 \tTraining Loss: 0.980530 \tValidation Loss: 0.989884\n",
      "Validation loss decreased (0.990079 --> 0.989884).         Saving model ...\n",
      "Epoch: 458 \tTraining Loss: 0.980331 \tValidation Loss: 0.989690\n",
      "Validation loss decreased (0.989884 --> 0.989690).         Saving model ...\n",
      "Epoch: 459 \tTraining Loss: 0.980132 \tValidation Loss: 0.989496\n",
      "Validation loss decreased (0.989690 --> 0.989496).         Saving model ...\n",
      "Epoch: 460 \tTraining Loss: 0.979934 \tValidation Loss: 0.989303\n",
      "Validation loss decreased (0.989496 --> 0.989303).         Saving model ...\n",
      "Epoch: 461 \tTraining Loss: 0.979737 \tValidation Loss: 0.989110\n",
      "Validation loss decreased (0.989303 --> 0.989110).         Saving model ...\n",
      "Epoch: 462 \tTraining Loss: 0.979541 \tValidation Loss: 0.988918\n",
      "Validation loss decreased (0.989110 --> 0.988918).         Saving model ...\n",
      "Epoch: 463 \tTraining Loss: 0.979344 \tValidation Loss: 0.988727\n",
      "Validation loss decreased (0.988918 --> 0.988727).         Saving model ...\n",
      "Epoch: 464 \tTraining Loss: 0.979149 \tValidation Loss: 0.988536\n",
      "Validation loss decreased (0.988727 --> 0.988536).         Saving model ...\n",
      "Epoch: 465 \tTraining Loss: 0.978954 \tValidation Loss: 0.988345\n",
      "Validation loss decreased (0.988536 --> 0.988345).         Saving model ...\n",
      "Epoch: 466 \tTraining Loss: 0.978759 \tValidation Loss: 0.988156\n",
      "Validation loss decreased (0.988345 --> 0.988156).         Saving model ...\n",
      "Epoch: 467 \tTraining Loss: 0.978565 \tValidation Loss: 0.987966\n",
      "Validation loss decreased (0.988156 --> 0.987966).         Saving model ...\n",
      "Epoch: 468 \tTraining Loss: 0.978372 \tValidation Loss: 0.987778\n",
      "Validation loss decreased (0.987966 --> 0.987778).         Saving model ...\n",
      "Epoch: 469 \tTraining Loss: 0.978179 \tValidation Loss: 0.987589\n",
      "Validation loss decreased (0.987778 --> 0.987589).         Saving model ...\n",
      "Epoch: 470 \tTraining Loss: 0.977987 \tValidation Loss: 0.987402\n",
      "Validation loss decreased (0.987589 --> 0.987402).         Saving model ...\n",
      "Epoch: 471 \tTraining Loss: 0.977795 \tValidation Loss: 0.987215\n",
      "Validation loss decreased (0.987402 --> 0.987215).         Saving model ...\n",
      "Epoch: 472 \tTraining Loss: 0.977604 \tValidation Loss: 0.987028\n",
      "Validation loss decreased (0.987215 --> 0.987028).         Saving model ...\n",
      "Epoch: 473 \tTraining Loss: 0.977413 \tValidation Loss: 0.986842\n",
      "Validation loss decreased (0.987028 --> 0.986842).         Saving model ...\n",
      "Epoch: 474 \tTraining Loss: 0.977223 \tValidation Loss: 0.986656\n",
      "Validation loss decreased (0.986842 --> 0.986656).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 475 \tTraining Loss: 0.977033 \tValidation Loss: 0.986471\n",
      "Validation loss decreased (0.986656 --> 0.986471).         Saving model ...\n",
      "Epoch: 476 \tTraining Loss: 0.976844 \tValidation Loss: 0.986287\n",
      "Validation loss decreased (0.986471 --> 0.986287).         Saving model ...\n",
      "Epoch: 477 \tTraining Loss: 0.976656 \tValidation Loss: 0.986103\n",
      "Validation loss decreased (0.986287 --> 0.986103).         Saving model ...\n",
      "Epoch: 478 \tTraining Loss: 0.976468 \tValidation Loss: 0.985919\n",
      "Validation loss decreased (0.986103 --> 0.985919).         Saving model ...\n",
      "Epoch: 479 \tTraining Loss: 0.976280 \tValidation Loss: 0.985736\n",
      "Validation loss decreased (0.985919 --> 0.985736).         Saving model ...\n",
      "Epoch: 480 \tTraining Loss: 0.976093 \tValidation Loss: 0.985554\n",
      "Validation loss decreased (0.985736 --> 0.985554).         Saving model ...\n",
      "Epoch: 481 \tTraining Loss: 0.975906 \tValidation Loss: 0.985372\n",
      "Validation loss decreased (0.985554 --> 0.985372).         Saving model ...\n",
      "Epoch: 482 \tTraining Loss: 0.975720 \tValidation Loss: 0.985191\n",
      "Validation loss decreased (0.985372 --> 0.985191).         Saving model ...\n",
      "Epoch: 483 \tTraining Loss: 0.975535 \tValidation Loss: 0.985010\n",
      "Validation loss decreased (0.985191 --> 0.985010).         Saving model ...\n",
      "Epoch: 484 \tTraining Loss: 0.975350 \tValidation Loss: 0.984829\n",
      "Validation loss decreased (0.985010 --> 0.984829).         Saving model ...\n",
      "Epoch: 485 \tTraining Loss: 0.975165 \tValidation Loss: 0.984649\n",
      "Validation loss decreased (0.984829 --> 0.984649).         Saving model ...\n",
      "Epoch: 486 \tTraining Loss: 0.974981 \tValidation Loss: 0.984470\n",
      "Validation loss decreased (0.984649 --> 0.984470).         Saving model ...\n",
      "Epoch: 487 \tTraining Loss: 0.974798 \tValidation Loss: 0.984291\n",
      "Validation loss decreased (0.984470 --> 0.984291).         Saving model ...\n",
      "Epoch: 488 \tTraining Loss: 0.974615 \tValidation Loss: 0.984113\n",
      "Validation loss decreased (0.984291 --> 0.984113).         Saving model ...\n",
      "Epoch: 489 \tTraining Loss: 0.974432 \tValidation Loss: 0.983935\n",
      "Validation loss decreased (0.984113 --> 0.983935).         Saving model ...\n",
      "Epoch: 490 \tTraining Loss: 0.974250 \tValidation Loss: 0.983757\n",
      "Validation loss decreased (0.983935 --> 0.983757).         Saving model ...\n",
      "Epoch: 491 \tTraining Loss: 0.974068 \tValidation Loss: 0.983580\n",
      "Validation loss decreased (0.983757 --> 0.983580).         Saving model ...\n",
      "Epoch: 492 \tTraining Loss: 0.973887 \tValidation Loss: 0.983404\n",
      "Validation loss decreased (0.983580 --> 0.983404).         Saving model ...\n",
      "Epoch: 493 \tTraining Loss: 0.973707 \tValidation Loss: 0.983228\n",
      "Validation loss decreased (0.983404 --> 0.983228).         Saving model ...\n",
      "Epoch: 494 \tTraining Loss: 0.973526 \tValidation Loss: 0.983053\n",
      "Validation loss decreased (0.983228 --> 0.983053).         Saving model ...\n",
      "Epoch: 495 \tTraining Loss: 0.973347 \tValidation Loss: 0.982878\n",
      "Validation loss decreased (0.983053 --> 0.982878).         Saving model ...\n",
      "Epoch: 496 \tTraining Loss: 0.973168 \tValidation Loss: 0.982703\n",
      "Validation loss decreased (0.982878 --> 0.982703).         Saving model ...\n",
      "Epoch: 497 \tTraining Loss: 0.972989 \tValidation Loss: 0.982529\n",
      "Validation loss decreased (0.982703 --> 0.982529).         Saving model ...\n",
      "Epoch: 498 \tTraining Loss: 0.972811 \tValidation Loss: 0.982355\n",
      "Validation loss decreased (0.982529 --> 0.982355).         Saving model ...\n",
      "Epoch: 499 \tTraining Loss: 0.972633 \tValidation Loss: 0.982182\n",
      "Validation loss decreased (0.982355 --> 0.982182).         Saving model ...\n",
      "Epoch: 500 \tTraining Loss: 0.972456 \tValidation Loss: 0.982009\n",
      "Validation loss decreased (0.982182 --> 0.982009).         Saving model ...\n",
      "Epoch: 501 \tTraining Loss: 0.972279 \tValidation Loss: 0.981837\n",
      "Validation loss decreased (0.982009 --> 0.981837).         Saving model ...\n",
      "Epoch: 502 \tTraining Loss: 0.972102 \tValidation Loss: 0.981666\n",
      "Validation loss decreased (0.981837 --> 0.981666).         Saving model ...\n",
      "Epoch: 503 \tTraining Loss: 0.971926 \tValidation Loss: 0.981494\n",
      "Validation loss decreased (0.981666 --> 0.981494).         Saving model ...\n",
      "Epoch: 504 \tTraining Loss: 0.971751 \tValidation Loss: 0.981324\n",
      "Validation loss decreased (0.981494 --> 0.981324).         Saving model ...\n",
      "Epoch: 505 \tTraining Loss: 0.971576 \tValidation Loss: 0.981153\n",
      "Validation loss decreased (0.981324 --> 0.981153).         Saving model ...\n",
      "Epoch: 506 \tTraining Loss: 0.971402 \tValidation Loss: 0.980983\n",
      "Validation loss decreased (0.981153 --> 0.980983).         Saving model ...\n",
      "Epoch: 507 \tTraining Loss: 0.971227 \tValidation Loss: 0.980814\n",
      "Validation loss decreased (0.980983 --> 0.980814).         Saving model ...\n",
      "Epoch: 508 \tTraining Loss: 0.971054 \tValidation Loss: 0.980645\n",
      "Validation loss decreased (0.980814 --> 0.980645).         Saving model ...\n",
      "Epoch: 509 \tTraining Loss: 0.970881 \tValidation Loss: 0.980476\n",
      "Validation loss decreased (0.980645 --> 0.980476).         Saving model ...\n",
      "Epoch: 510 \tTraining Loss: 0.970708 \tValidation Loss: 0.980308\n",
      "Validation loss decreased (0.980476 --> 0.980308).         Saving model ...\n",
      "Epoch: 511 \tTraining Loss: 0.970536 \tValidation Loss: 0.980141\n",
      "Validation loss decreased (0.980308 --> 0.980141).         Saving model ...\n",
      "Epoch: 512 \tTraining Loss: 0.970364 \tValidation Loss: 0.979973\n",
      "Validation loss decreased (0.980141 --> 0.979973).         Saving model ...\n",
      "Epoch: 513 \tTraining Loss: 0.970192 \tValidation Loss: 0.979807\n",
      "Validation loss decreased (0.979973 --> 0.979807).         Saving model ...\n",
      "Epoch: 514 \tTraining Loss: 0.970021 \tValidation Loss: 0.979640\n",
      "Validation loss decreased (0.979807 --> 0.979640).         Saving model ...\n",
      "Epoch: 515 \tTraining Loss: 0.969851 \tValidation Loss: 0.979474\n",
      "Validation loss decreased (0.979640 --> 0.979474).         Saving model ...\n",
      "Epoch: 516 \tTraining Loss: 0.969681 \tValidation Loss: 0.979309\n",
      "Validation loss decreased (0.979474 --> 0.979309).         Saving model ...\n",
      "Epoch: 517 \tTraining Loss: 0.969511 \tValidation Loss: 0.979144\n",
      "Validation loss decreased (0.979309 --> 0.979144).         Saving model ...\n",
      "Epoch: 518 \tTraining Loss: 0.969342 \tValidation Loss: 0.978979\n",
      "Validation loss decreased (0.979144 --> 0.978979).         Saving model ...\n",
      "Epoch: 519 \tTraining Loss: 0.969173 \tValidation Loss: 0.978815\n",
      "Validation loss decreased (0.978979 --> 0.978815).         Saving model ...\n",
      "Epoch: 520 \tTraining Loss: 0.969005 \tValidation Loss: 0.978651\n",
      "Validation loss decreased (0.978815 --> 0.978651).         Saving model ...\n",
      "Epoch: 521 \tTraining Loss: 0.968837 \tValidation Loss: 0.978488\n",
      "Validation loss decreased (0.978651 --> 0.978488).         Saving model ...\n",
      "Epoch: 522 \tTraining Loss: 0.968669 \tValidation Loss: 0.978325\n",
      "Validation loss decreased (0.978488 --> 0.978325).         Saving model ...\n",
      "Epoch: 523 \tTraining Loss: 0.968502 \tValidation Loss: 0.978163\n",
      "Validation loss decreased (0.978325 --> 0.978163).         Saving model ...\n",
      "Epoch: 524 \tTraining Loss: 0.968335 \tValidation Loss: 0.978001\n",
      "Validation loss decreased (0.978163 --> 0.978001).         Saving model ...\n",
      "Epoch: 525 \tTraining Loss: 0.968169 \tValidation Loss: 0.977839\n",
      "Validation loss decreased (0.978001 --> 0.977839).         Saving model ...\n",
      "Epoch: 526 \tTraining Loss: 0.968003 \tValidation Loss: 0.977678\n",
      "Validation loss decreased (0.977839 --> 0.977678).         Saving model ...\n",
      "Epoch: 527 \tTraining Loss: 0.967838 \tValidation Loss: 0.977517\n",
      "Validation loss decreased (0.977678 --> 0.977517).         Saving model ...\n",
      "Epoch: 528 \tTraining Loss: 0.967673 \tValidation Loss: 0.977357\n",
      "Validation loss decreased (0.977517 --> 0.977357).         Saving model ...\n",
      "Epoch: 529 \tTraining Loss: 0.967508 \tValidation Loss: 0.977197\n",
      "Validation loss decreased (0.977357 --> 0.977197).         Saving model ...\n",
      "Epoch: 530 \tTraining Loss: 0.967344 \tValidation Loss: 0.977037\n",
      "Validation loss decreased (0.977197 --> 0.977037).         Saving model ...\n",
      "Epoch: 531 \tTraining Loss: 0.967180 \tValidation Loss: 0.976878\n",
      "Validation loss decreased (0.977037 --> 0.976878).         Saving model ...\n",
      "Epoch: 532 \tTraining Loss: 0.967017 \tValidation Loss: 0.976719\n",
      "Validation loss decreased (0.976878 --> 0.976719).         Saving model ...\n",
      "Epoch: 533 \tTraining Loss: 0.966854 \tValidation Loss: 0.976561\n",
      "Validation loss decreased (0.976719 --> 0.976561).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 534 \tTraining Loss: 0.966691 \tValidation Loss: 0.976403\n",
      "Validation loss decreased (0.976561 --> 0.976403).         Saving model ...\n",
      "Epoch: 535 \tTraining Loss: 0.966529 \tValidation Loss: 0.976245\n",
      "Validation loss decreased (0.976403 --> 0.976245).         Saving model ...\n",
      "Epoch: 536 \tTraining Loss: 0.966367 \tValidation Loss: 0.976088\n",
      "Validation loss decreased (0.976245 --> 0.976088).         Saving model ...\n",
      "Epoch: 537 \tTraining Loss: 0.966206 \tValidation Loss: 0.975931\n",
      "Validation loss decreased (0.976088 --> 0.975931).         Saving model ...\n",
      "Epoch: 538 \tTraining Loss: 0.966045 \tValidation Loss: 0.975775\n",
      "Validation loss decreased (0.975931 --> 0.975775).         Saving model ...\n",
      "Epoch: 539 \tTraining Loss: 0.965884 \tValidation Loss: 0.975619\n",
      "Validation loss decreased (0.975775 --> 0.975619).         Saving model ...\n",
      "Epoch: 540 \tTraining Loss: 0.965724 \tValidation Loss: 0.975463\n",
      "Validation loss decreased (0.975619 --> 0.975463).         Saving model ...\n",
      "Epoch: 541 \tTraining Loss: 0.965564 \tValidation Loss: 0.975308\n",
      "Validation loss decreased (0.975463 --> 0.975308).         Saving model ...\n",
      "Epoch: 542 \tTraining Loss: 0.965404 \tValidation Loss: 0.975153\n",
      "Validation loss decreased (0.975308 --> 0.975153).         Saving model ...\n",
      "Epoch: 543 \tTraining Loss: 0.965245 \tValidation Loss: 0.974999\n",
      "Validation loss decreased (0.975153 --> 0.974999).         Saving model ...\n",
      "Epoch: 544 \tTraining Loss: 0.965087 \tValidation Loss: 0.974845\n",
      "Validation loss decreased (0.974999 --> 0.974845).         Saving model ...\n",
      "Epoch: 545 \tTraining Loss: 0.964928 \tValidation Loss: 0.974691\n",
      "Validation loss decreased (0.974845 --> 0.974691).         Saving model ...\n",
      "Epoch: 546 \tTraining Loss: 0.964771 \tValidation Loss: 0.974538\n",
      "Validation loss decreased (0.974691 --> 0.974538).         Saving model ...\n",
      "Epoch: 547 \tTraining Loss: 0.964613 \tValidation Loss: 0.974385\n",
      "Validation loss decreased (0.974538 --> 0.974385).         Saving model ...\n",
      "Epoch: 548 \tTraining Loss: 0.964456 \tValidation Loss: 0.974233\n",
      "Validation loss decreased (0.974385 --> 0.974233).         Saving model ...\n",
      "Epoch: 549 \tTraining Loss: 0.964299 \tValidation Loss: 0.974081\n",
      "Validation loss decreased (0.974233 --> 0.974081).         Saving model ...\n",
      "Epoch: 550 \tTraining Loss: 0.964143 \tValidation Loss: 0.973929\n",
      "Validation loss decreased (0.974081 --> 0.973929).         Saving model ...\n",
      "Epoch: 551 \tTraining Loss: 0.963987 \tValidation Loss: 0.973777\n",
      "Validation loss decreased (0.973929 --> 0.973777).         Saving model ...\n",
      "Epoch: 552 \tTraining Loss: 0.963831 \tValidation Loss: 0.973627\n",
      "Validation loss decreased (0.973777 --> 0.973627).         Saving model ...\n",
      "Epoch: 553 \tTraining Loss: 0.963676 \tValidation Loss: 0.973476\n",
      "Validation loss decreased (0.973627 --> 0.973476).         Saving model ...\n",
      "Epoch: 554 \tTraining Loss: 0.963521 \tValidation Loss: 0.973326\n",
      "Validation loss decreased (0.973476 --> 0.973326).         Saving model ...\n",
      "Epoch: 555 \tTraining Loss: 0.963366 \tValidation Loss: 0.973176\n",
      "Validation loss decreased (0.973326 --> 0.973176).         Saving model ...\n",
      "Epoch: 556 \tTraining Loss: 0.963212 \tValidation Loss: 0.973026\n",
      "Validation loss decreased (0.973176 --> 0.973026).         Saving model ...\n",
      "Epoch: 557 \tTraining Loss: 0.963058 \tValidation Loss: 0.972877\n",
      "Validation loss decreased (0.973026 --> 0.972877).         Saving model ...\n",
      "Epoch: 558 \tTraining Loss: 0.962905 \tValidation Loss: 0.972728\n",
      "Validation loss decreased (0.972877 --> 0.972728).         Saving model ...\n",
      "Epoch: 559 \tTraining Loss: 0.962752 \tValidation Loss: 0.972580\n",
      "Validation loss decreased (0.972728 --> 0.972580).         Saving model ...\n",
      "Epoch: 560 \tTraining Loss: 0.962599 \tValidation Loss: 0.972432\n",
      "Validation loss decreased (0.972580 --> 0.972432).         Saving model ...\n",
      "Epoch: 561 \tTraining Loss: 0.962447 \tValidation Loss: 0.972284\n",
      "Validation loss decreased (0.972432 --> 0.972284).         Saving model ...\n",
      "Epoch: 562 \tTraining Loss: 0.962295 \tValidation Loss: 0.972137\n",
      "Validation loss decreased (0.972284 --> 0.972137).         Saving model ...\n",
      "Epoch: 563 \tTraining Loss: 0.962143 \tValidation Loss: 0.971990\n",
      "Validation loss decreased (0.972137 --> 0.971990).         Saving model ...\n",
      "Epoch: 564 \tTraining Loss: 0.961992 \tValidation Loss: 0.971843\n",
      "Validation loss decreased (0.971990 --> 0.971843).         Saving model ...\n",
      "Epoch: 565 \tTraining Loss: 0.961841 \tValidation Loss: 0.971697\n",
      "Validation loss decreased (0.971843 --> 0.971697).         Saving model ...\n",
      "Epoch: 566 \tTraining Loss: 0.961690 \tValidation Loss: 0.971551\n",
      "Validation loss decreased (0.971697 --> 0.971551).         Saving model ...\n",
      "Epoch: 567 \tTraining Loss: 0.961540 \tValidation Loss: 0.971405\n",
      "Validation loss decreased (0.971551 --> 0.971405).         Saving model ...\n",
      "Epoch: 568 \tTraining Loss: 0.961390 \tValidation Loss: 0.971260\n",
      "Validation loss decreased (0.971405 --> 0.971260).         Saving model ...\n",
      "Epoch: 569 \tTraining Loss: 0.961241 \tValidation Loss: 0.971115\n",
      "Validation loss decreased (0.971260 --> 0.971115).         Saving model ...\n",
      "Epoch: 570 \tTraining Loss: 0.961092 \tValidation Loss: 0.970970\n",
      "Validation loss decreased (0.971115 --> 0.970970).         Saving model ...\n",
      "Epoch: 571 \tTraining Loss: 0.960943 \tValidation Loss: 0.970826\n",
      "Validation loss decreased (0.970970 --> 0.970826).         Saving model ...\n",
      "Epoch: 572 \tTraining Loss: 0.960794 \tValidation Loss: 0.970682\n",
      "Validation loss decreased (0.970826 --> 0.970682).         Saving model ...\n",
      "Epoch: 573 \tTraining Loss: 0.960646 \tValidation Loss: 0.970539\n",
      "Validation loss decreased (0.970682 --> 0.970539).         Saving model ...\n",
      "Epoch: 574 \tTraining Loss: 0.960498 \tValidation Loss: 0.970396\n",
      "Validation loss decreased (0.970539 --> 0.970396).         Saving model ...\n",
      "Epoch: 575 \tTraining Loss: 0.960351 \tValidation Loss: 0.970253\n",
      "Validation loss decreased (0.970396 --> 0.970253).         Saving model ...\n",
      "Epoch: 576 \tTraining Loss: 0.960204 \tValidation Loss: 0.970110\n",
      "Validation loss decreased (0.970253 --> 0.970110).         Saving model ...\n",
      "Epoch: 577 \tTraining Loss: 0.960057 \tValidation Loss: 0.969968\n",
      "Validation loss decreased (0.970110 --> 0.969968).         Saving model ...\n",
      "Epoch: 578 \tTraining Loss: 0.959911 \tValidation Loss: 0.969826\n",
      "Validation loss decreased (0.969968 --> 0.969826).         Saving model ...\n",
      "Epoch: 579 \tTraining Loss: 0.959764 \tValidation Loss: 0.969685\n",
      "Validation loss decreased (0.969826 --> 0.969685).         Saving model ...\n",
      "Epoch: 580 \tTraining Loss: 0.959619 \tValidation Loss: 0.969544\n",
      "Validation loss decreased (0.969685 --> 0.969544).         Saving model ...\n",
      "Epoch: 581 \tTraining Loss: 0.959473 \tValidation Loss: 0.969403\n",
      "Validation loss decreased (0.969544 --> 0.969403).         Saving model ...\n",
      "Epoch: 582 \tTraining Loss: 0.959328 \tValidation Loss: 0.969262\n",
      "Validation loss decreased (0.969403 --> 0.969262).         Saving model ...\n",
      "Epoch: 583 \tTraining Loss: 0.959183 \tValidation Loss: 0.969122\n",
      "Validation loss decreased (0.969262 --> 0.969122).         Saving model ...\n",
      "Epoch: 584 \tTraining Loss: 0.959039 \tValidation Loss: 0.968982\n",
      "Validation loss decreased (0.969122 --> 0.968982).         Saving model ...\n",
      "Epoch: 585 \tTraining Loss: 0.958895 \tValidation Loss: 0.968843\n",
      "Validation loss decreased (0.968982 --> 0.968843).         Saving model ...\n",
      "Epoch: 586 \tTraining Loss: 0.958751 \tValidation Loss: 0.968704\n",
      "Validation loss decreased (0.968843 --> 0.968704).         Saving model ...\n",
      "Epoch: 587 \tTraining Loss: 0.958607 \tValidation Loss: 0.968565\n",
      "Validation loss decreased (0.968704 --> 0.968565).         Saving model ...\n",
      "Epoch: 588 \tTraining Loss: 0.958464 \tValidation Loss: 0.968426\n",
      "Validation loss decreased (0.968565 --> 0.968426).         Saving model ...\n",
      "Epoch: 589 \tTraining Loss: 0.958321 \tValidation Loss: 0.968288\n",
      "Validation loss decreased (0.968426 --> 0.968288).         Saving model ...\n",
      "Epoch: 590 \tTraining Loss: 0.958179 \tValidation Loss: 0.968150\n",
      "Validation loss decreased (0.968288 --> 0.968150).         Saving model ...\n",
      "Epoch: 591 \tTraining Loss: 0.958037 \tValidation Loss: 0.968012\n",
      "Validation loss decreased (0.968150 --> 0.968012).         Saving model ...\n",
      "Epoch: 592 \tTraining Loss: 0.957895 \tValidation Loss: 0.967875\n",
      "Validation loss decreased (0.968012 --> 0.967875).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 593 \tTraining Loss: 0.957753 \tValidation Loss: 0.967738\n",
      "Validation loss decreased (0.967875 --> 0.967738).         Saving model ...\n",
      "Epoch: 594 \tTraining Loss: 0.957612 \tValidation Loss: 0.967601\n",
      "Validation loss decreased (0.967738 --> 0.967601).         Saving model ...\n",
      "Epoch: 595 \tTraining Loss: 0.957471 \tValidation Loss: 0.967465\n",
      "Validation loss decreased (0.967601 --> 0.967465).         Saving model ...\n",
      "Epoch: 596 \tTraining Loss: 0.957330 \tValidation Loss: 0.967329\n",
      "Validation loss decreased (0.967465 --> 0.967329).         Saving model ...\n",
      "Epoch: 597 \tTraining Loss: 0.957190 \tValidation Loss: 0.967193\n",
      "Validation loss decreased (0.967329 --> 0.967193).         Saving model ...\n",
      "Epoch: 598 \tTraining Loss: 0.957050 \tValidation Loss: 0.967058\n",
      "Validation loss decreased (0.967193 --> 0.967058).         Saving model ...\n",
      "Epoch: 599 \tTraining Loss: 0.956910 \tValidation Loss: 0.966923\n",
      "Validation loss decreased (0.967058 --> 0.966923).         Saving model ...\n",
      "Epoch: 600 \tTraining Loss: 0.956771 \tValidation Loss: 0.966788\n",
      "Validation loss decreased (0.966923 --> 0.966788).         Saving model ...\n",
      "Epoch: 601 \tTraining Loss: 0.956632 \tValidation Loss: 0.966654\n",
      "Validation loss decreased (0.966788 --> 0.966654).         Saving model ...\n",
      "Epoch: 602 \tTraining Loss: 0.956493 \tValidation Loss: 0.966519\n",
      "Validation loss decreased (0.966654 --> 0.966519).         Saving model ...\n",
      "Epoch: 603 \tTraining Loss: 0.956355 \tValidation Loss: 0.966386\n",
      "Validation loss decreased (0.966519 --> 0.966386).         Saving model ...\n",
      "Epoch: 604 \tTraining Loss: 0.956217 \tValidation Loss: 0.966252\n",
      "Validation loss decreased (0.966386 --> 0.966252).         Saving model ...\n",
      "Epoch: 605 \tTraining Loss: 0.956079 \tValidation Loss: 0.966119\n",
      "Validation loss decreased (0.966252 --> 0.966119).         Saving model ...\n",
      "Epoch: 606 \tTraining Loss: 0.955941 \tValidation Loss: 0.965986\n",
      "Validation loss decreased (0.966119 --> 0.965986).         Saving model ...\n",
      "Epoch: 607 \tTraining Loss: 0.955804 \tValidation Loss: 0.965853\n",
      "Validation loss decreased (0.965986 --> 0.965853).         Saving model ...\n",
      "Epoch: 608 \tTraining Loss: 0.955667 \tValidation Loss: 0.965721\n",
      "Validation loss decreased (0.965853 --> 0.965721).         Saving model ...\n",
      "Epoch: 609 \tTraining Loss: 0.955530 \tValidation Loss: 0.965589\n",
      "Validation loss decreased (0.965721 --> 0.965589).         Saving model ...\n",
      "Epoch: 610 \tTraining Loss: 0.955394 \tValidation Loss: 0.965457\n",
      "Validation loss decreased (0.965589 --> 0.965457).         Saving model ...\n",
      "Epoch: 611 \tTraining Loss: 0.955258 \tValidation Loss: 0.965325\n",
      "Validation loss decreased (0.965457 --> 0.965325).         Saving model ...\n",
      "Epoch: 612 \tTraining Loss: 0.955122 \tValidation Loss: 0.965194\n",
      "Validation loss decreased (0.965325 --> 0.965194).         Saving model ...\n",
      "Epoch: 613 \tTraining Loss: 0.954987 \tValidation Loss: 0.965063\n",
      "Validation loss decreased (0.965194 --> 0.965063).         Saving model ...\n",
      "Epoch: 614 \tTraining Loss: 0.954851 \tValidation Loss: 0.964933\n",
      "Validation loss decreased (0.965063 --> 0.964933).         Saving model ...\n",
      "Epoch: 615 \tTraining Loss: 0.954716 \tValidation Loss: 0.964802\n",
      "Validation loss decreased (0.964933 --> 0.964802).         Saving model ...\n",
      "Epoch: 616 \tTraining Loss: 0.954582 \tValidation Loss: 0.964672\n",
      "Validation loss decreased (0.964802 --> 0.964672).         Saving model ...\n",
      "Epoch: 617 \tTraining Loss: 0.954448 \tValidation Loss: 0.964543\n",
      "Validation loss decreased (0.964672 --> 0.964543).         Saving model ...\n",
      "Epoch: 618 \tTraining Loss: 0.954314 \tValidation Loss: 0.964413\n",
      "Validation loss decreased (0.964543 --> 0.964413).         Saving model ...\n",
      "Epoch: 619 \tTraining Loss: 0.954180 \tValidation Loss: 0.964284\n",
      "Validation loss decreased (0.964413 --> 0.964284).         Saving model ...\n",
      "Epoch: 620 \tTraining Loss: 0.954046 \tValidation Loss: 0.964155\n",
      "Validation loss decreased (0.964284 --> 0.964155).         Saving model ...\n",
      "Epoch: 621 \tTraining Loss: 0.953913 \tValidation Loss: 0.964027\n",
      "Validation loss decreased (0.964155 --> 0.964027).         Saving model ...\n",
      "Epoch: 622 \tTraining Loss: 0.953780 \tValidation Loss: 0.963898\n",
      "Validation loss decreased (0.964027 --> 0.963898).         Saving model ...\n",
      "Epoch: 623 \tTraining Loss: 0.953648 \tValidation Loss: 0.963770\n",
      "Validation loss decreased (0.963898 --> 0.963770).         Saving model ...\n",
      "Epoch: 624 \tTraining Loss: 0.953515 \tValidation Loss: 0.963643\n",
      "Validation loss decreased (0.963770 --> 0.963643).         Saving model ...\n",
      "Epoch: 625 \tTraining Loss: 0.953383 \tValidation Loss: 0.963515\n",
      "Validation loss decreased (0.963643 --> 0.963515).         Saving model ...\n",
      "Epoch: 626 \tTraining Loss: 0.953252 \tValidation Loss: 0.963388\n",
      "Validation loss decreased (0.963515 --> 0.963388).         Saving model ...\n",
      "Epoch: 627 \tTraining Loss: 0.953120 \tValidation Loss: 0.963261\n",
      "Validation loss decreased (0.963388 --> 0.963261).         Saving model ...\n",
      "Epoch: 628 \tTraining Loss: 0.952989 \tValidation Loss: 0.963134\n",
      "Validation loss decreased (0.963261 --> 0.963134).         Saving model ...\n",
      "Epoch: 629 \tTraining Loss: 0.952858 \tValidation Loss: 0.963008\n",
      "Validation loss decreased (0.963134 --> 0.963008).         Saving model ...\n",
      "Epoch: 630 \tTraining Loss: 0.952727 \tValidation Loss: 0.962882\n",
      "Validation loss decreased (0.963008 --> 0.962882).         Saving model ...\n",
      "Epoch: 631 \tTraining Loss: 0.952597 \tValidation Loss: 0.962756\n",
      "Validation loss decreased (0.962882 --> 0.962756).         Saving model ...\n",
      "Epoch: 632 \tTraining Loss: 0.952467 \tValidation Loss: 0.962631\n",
      "Validation loss decreased (0.962756 --> 0.962631).         Saving model ...\n",
      "Epoch: 633 \tTraining Loss: 0.952337 \tValidation Loss: 0.962505\n",
      "Validation loss decreased (0.962631 --> 0.962505).         Saving model ...\n",
      "Epoch: 634 \tTraining Loss: 0.952208 \tValidation Loss: 0.962380\n",
      "Validation loss decreased (0.962505 --> 0.962380).         Saving model ...\n",
      "Epoch: 635 \tTraining Loss: 0.952078 \tValidation Loss: 0.962256\n",
      "Validation loss decreased (0.962380 --> 0.962256).         Saving model ...\n",
      "Epoch: 636 \tTraining Loss: 0.951949 \tValidation Loss: 0.962131\n",
      "Validation loss decreased (0.962256 --> 0.962131).         Saving model ...\n",
      "Epoch: 637 \tTraining Loss: 0.951821 \tValidation Loss: 0.962007\n",
      "Validation loss decreased (0.962131 --> 0.962007).         Saving model ...\n",
      "Epoch: 638 \tTraining Loss: 0.951692 \tValidation Loss: 0.961883\n",
      "Validation loss decreased (0.962007 --> 0.961883).         Saving model ...\n",
      "Epoch: 639 \tTraining Loss: 0.951564 \tValidation Loss: 0.961760\n",
      "Validation loss decreased (0.961883 --> 0.961760).         Saving model ...\n",
      "Epoch: 640 \tTraining Loss: 0.951436 \tValidation Loss: 0.961636\n",
      "Validation loss decreased (0.961760 --> 0.961636).         Saving model ...\n",
      "Epoch: 641 \tTraining Loss: 0.951308 \tValidation Loss: 0.961513\n",
      "Validation loss decreased (0.961636 --> 0.961513).         Saving model ...\n",
      "Epoch: 642 \tTraining Loss: 0.951181 \tValidation Loss: 0.961390\n",
      "Validation loss decreased (0.961513 --> 0.961390).         Saving model ...\n",
      "Epoch: 643 \tTraining Loss: 0.951054 \tValidation Loss: 0.961268\n",
      "Validation loss decreased (0.961390 --> 0.961268).         Saving model ...\n",
      "Epoch: 644 \tTraining Loss: 0.950927 \tValidation Loss: 0.961145\n",
      "Validation loss decreased (0.961268 --> 0.961145).         Saving model ...\n",
      "Epoch: 645 \tTraining Loss: 0.950800 \tValidation Loss: 0.961023\n",
      "Validation loss decreased (0.961145 --> 0.961023).         Saving model ...\n",
      "Epoch: 646 \tTraining Loss: 0.950674 \tValidation Loss: 0.960901\n",
      "Validation loss decreased (0.961023 --> 0.960901).         Saving model ...\n",
      "Epoch: 647 \tTraining Loss: 0.950548 \tValidation Loss: 0.960780\n",
      "Validation loss decreased (0.960901 --> 0.960780).         Saving model ...\n",
      "Epoch: 648 \tTraining Loss: 0.950422 \tValidation Loss: 0.960659\n",
      "Validation loss decreased (0.960780 --> 0.960659).         Saving model ...\n",
      "Epoch: 649 \tTraining Loss: 0.950297 \tValidation Loss: 0.960538\n",
      "Validation loss decreased (0.960659 --> 0.960538).         Saving model ...\n",
      "Epoch: 650 \tTraining Loss: 0.950171 \tValidation Loss: 0.960417\n",
      "Validation loss decreased (0.960538 --> 0.960417).         Saving model ...\n",
      "Epoch: 651 \tTraining Loss: 0.950046 \tValidation Loss: 0.960296\n",
      "Validation loss decreased (0.960417 --> 0.960296).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 652 \tTraining Loss: 0.949921 \tValidation Loss: 0.960176\n",
      "Validation loss decreased (0.960296 --> 0.960176).         Saving model ...\n",
      "Epoch: 653 \tTraining Loss: 0.949797 \tValidation Loss: 0.960056\n",
      "Validation loss decreased (0.960176 --> 0.960056).         Saving model ...\n",
      "Epoch: 654 \tTraining Loss: 0.949673 \tValidation Loss: 0.959936\n",
      "Validation loss decreased (0.960056 --> 0.959936).         Saving model ...\n",
      "Epoch: 655 \tTraining Loss: 0.949548 \tValidation Loss: 0.959817\n",
      "Validation loss decreased (0.959936 --> 0.959817).         Saving model ...\n",
      "Epoch: 656 \tTraining Loss: 0.949425 \tValidation Loss: 0.959697\n",
      "Validation loss decreased (0.959817 --> 0.959697).         Saving model ...\n",
      "Epoch: 657 \tTraining Loss: 0.949301 \tValidation Loss: 0.959578\n",
      "Validation loss decreased (0.959697 --> 0.959578).         Saving model ...\n",
      "Epoch: 658 \tTraining Loss: 0.949178 \tValidation Loss: 0.959460\n",
      "Validation loss decreased (0.959578 --> 0.959460).         Saving model ...\n",
      "Epoch: 659 \tTraining Loss: 0.949055 \tValidation Loss: 0.959341\n",
      "Validation loss decreased (0.959460 --> 0.959341).         Saving model ...\n",
      "Epoch: 660 \tTraining Loss: 0.948932 \tValidation Loss: 0.959223\n",
      "Validation loss decreased (0.959341 --> 0.959223).         Saving model ...\n",
      "Epoch: 661 \tTraining Loss: 0.948809 \tValidation Loss: 0.959105\n",
      "Validation loss decreased (0.959223 --> 0.959105).         Saving model ...\n",
      "Epoch: 662 \tTraining Loss: 0.948687 \tValidation Loss: 0.958987\n",
      "Validation loss decreased (0.959105 --> 0.958987).         Saving model ...\n",
      "Epoch: 663 \tTraining Loss: 0.948565 \tValidation Loss: 0.958869\n",
      "Validation loss decreased (0.958987 --> 0.958869).         Saving model ...\n",
      "Epoch: 664 \tTraining Loss: 0.948443 \tValidation Loss: 0.958752\n",
      "Validation loss decreased (0.958869 --> 0.958752).         Saving model ...\n",
      "Epoch: 665 \tTraining Loss: 0.948322 \tValidation Loss: 0.958635\n",
      "Validation loss decreased (0.958752 --> 0.958635).         Saving model ...\n",
      "Epoch: 666 \tTraining Loss: 0.948200 \tValidation Loss: 0.958518\n",
      "Validation loss decreased (0.958635 --> 0.958518).         Saving model ...\n",
      "Epoch: 667 \tTraining Loss: 0.948079 \tValidation Loss: 0.958402\n",
      "Validation loss decreased (0.958518 --> 0.958402).         Saving model ...\n",
      "Epoch: 668 \tTraining Loss: 0.947959 \tValidation Loss: 0.958285\n",
      "Validation loss decreased (0.958402 --> 0.958285).         Saving model ...\n",
      "Epoch: 669 \tTraining Loss: 0.947838 \tValidation Loss: 0.958169\n",
      "Validation loss decreased (0.958285 --> 0.958169).         Saving model ...\n",
      "Epoch: 670 \tTraining Loss: 0.947718 \tValidation Loss: 0.958053\n",
      "Validation loss decreased (0.958169 --> 0.958053).         Saving model ...\n",
      "Epoch: 671 \tTraining Loss: 0.947598 \tValidation Loss: 0.957938\n",
      "Validation loss decreased (0.958053 --> 0.957938).         Saving model ...\n",
      "Epoch: 672 \tTraining Loss: 0.947478 \tValidation Loss: 0.957822\n",
      "Validation loss decreased (0.957938 --> 0.957822).         Saving model ...\n",
      "Epoch: 673 \tTraining Loss: 0.947358 \tValidation Loss: 0.957707\n",
      "Validation loss decreased (0.957822 --> 0.957707).         Saving model ...\n",
      "Epoch: 674 \tTraining Loss: 0.947239 \tValidation Loss: 0.957592\n",
      "Validation loss decreased (0.957707 --> 0.957592).         Saving model ...\n",
      "Epoch: 675 \tTraining Loss: 0.947119 \tValidation Loss: 0.957478\n",
      "Validation loss decreased (0.957592 --> 0.957478).         Saving model ...\n",
      "Epoch: 676 \tTraining Loss: 0.947001 \tValidation Loss: 0.957363\n",
      "Validation loss decreased (0.957478 --> 0.957363).         Saving model ...\n",
      "Epoch: 677 \tTraining Loss: 0.946882 \tValidation Loss: 0.957249\n",
      "Validation loss decreased (0.957363 --> 0.957249).         Saving model ...\n",
      "Epoch: 678 \tTraining Loss: 0.946763 \tValidation Loss: 0.957135\n",
      "Validation loss decreased (0.957249 --> 0.957135).         Saving model ...\n",
      "Epoch: 679 \tTraining Loss: 0.946645 \tValidation Loss: 0.957021\n",
      "Validation loss decreased (0.957135 --> 0.957021).         Saving model ...\n",
      "Epoch: 680 \tTraining Loss: 0.946527 \tValidation Loss: 0.956908\n",
      "Validation loss decreased (0.957021 --> 0.956908).         Saving model ...\n",
      "Epoch: 681 \tTraining Loss: 0.946409 \tValidation Loss: 0.956795\n",
      "Validation loss decreased (0.956908 --> 0.956795).         Saving model ...\n",
      "Epoch: 682 \tTraining Loss: 0.946292 \tValidation Loss: 0.956681\n",
      "Validation loss decreased (0.956795 --> 0.956681).         Saving model ...\n",
      "Epoch: 683 \tTraining Loss: 0.946175 \tValidation Loss: 0.956569\n",
      "Validation loss decreased (0.956681 --> 0.956569).         Saving model ...\n",
      "Epoch: 684 \tTraining Loss: 0.946057 \tValidation Loss: 0.956456\n",
      "Validation loss decreased (0.956569 --> 0.956456).         Saving model ...\n",
      "Epoch: 685 \tTraining Loss: 0.945941 \tValidation Loss: 0.956344\n",
      "Validation loss decreased (0.956456 --> 0.956344).         Saving model ...\n",
      "Epoch: 686 \tTraining Loss: 0.945824 \tValidation Loss: 0.956232\n",
      "Validation loss decreased (0.956344 --> 0.956232).         Saving model ...\n",
      "Epoch: 687 \tTraining Loss: 0.945708 \tValidation Loss: 0.956120\n",
      "Validation loss decreased (0.956232 --> 0.956120).         Saving model ...\n",
      "Epoch: 688 \tTraining Loss: 0.945591 \tValidation Loss: 0.956008\n",
      "Validation loss decreased (0.956120 --> 0.956008).         Saving model ...\n",
      "Epoch: 689 \tTraining Loss: 0.945476 \tValidation Loss: 0.955896\n",
      "Validation loss decreased (0.956008 --> 0.955896).         Saving model ...\n",
      "Epoch: 690 \tTraining Loss: 0.945360 \tValidation Loss: 0.955785\n",
      "Validation loss decreased (0.955896 --> 0.955785).         Saving model ...\n",
      "Epoch: 691 \tTraining Loss: 0.945244 \tValidation Loss: 0.955674\n",
      "Validation loss decreased (0.955785 --> 0.955674).         Saving model ...\n",
      "Epoch: 692 \tTraining Loss: 0.945129 \tValidation Loss: 0.955563\n",
      "Validation loss decreased (0.955674 --> 0.955563).         Saving model ...\n",
      "Epoch: 693 \tTraining Loss: 0.945014 \tValidation Loss: 0.955453\n",
      "Validation loss decreased (0.955563 --> 0.955453).         Saving model ...\n",
      "Epoch: 694 \tTraining Loss: 0.944899 \tValidation Loss: 0.955342\n",
      "Validation loss decreased (0.955453 --> 0.955342).         Saving model ...\n",
      "Epoch: 695 \tTraining Loss: 0.944785 \tValidation Loss: 0.955232\n",
      "Validation loss decreased (0.955342 --> 0.955232).         Saving model ...\n",
      "Epoch: 696 \tTraining Loss: 0.944670 \tValidation Loss: 0.955122\n",
      "Validation loss decreased (0.955232 --> 0.955122).         Saving model ...\n",
      "Epoch: 697 \tTraining Loss: 0.944556 \tValidation Loss: 0.955013\n",
      "Validation loss decreased (0.955122 --> 0.955013).         Saving model ...\n",
      "Epoch: 698 \tTraining Loss: 0.944442 \tValidation Loss: 0.954903\n",
      "Validation loss decreased (0.955013 --> 0.954903).         Saving model ...\n",
      "Epoch: 699 \tTraining Loss: 0.944328 \tValidation Loss: 0.954794\n",
      "Validation loss decreased (0.954903 --> 0.954794).         Saving model ...\n",
      "Epoch: 700 \tTraining Loss: 0.944215 \tValidation Loss: 0.954685\n",
      "Validation loss decreased (0.954794 --> 0.954685).         Saving model ...\n",
      "Epoch: 701 \tTraining Loss: 0.944102 \tValidation Loss: 0.954576\n",
      "Validation loss decreased (0.954685 --> 0.954576).         Saving model ...\n",
      "Epoch: 702 \tTraining Loss: 0.943988 \tValidation Loss: 0.954467\n",
      "Validation loss decreased (0.954576 --> 0.954467).         Saving model ...\n",
      "Epoch: 703 \tTraining Loss: 0.943876 \tValidation Loss: 0.954359\n",
      "Validation loss decreased (0.954467 --> 0.954359).         Saving model ...\n",
      "Epoch: 704 \tTraining Loss: 0.943763 \tValidation Loss: 0.954251\n",
      "Validation loss decreased (0.954359 --> 0.954251).         Saving model ...\n",
      "Epoch: 705 \tTraining Loss: 0.943651 \tValidation Loss: 0.954143\n",
      "Validation loss decreased (0.954251 --> 0.954143).         Saving model ...\n",
      "Epoch: 706 \tTraining Loss: 0.943538 \tValidation Loss: 0.954035\n",
      "Validation loss decreased (0.954143 --> 0.954035).         Saving model ...\n",
      "Epoch: 707 \tTraining Loss: 0.943426 \tValidation Loss: 0.953927\n",
      "Validation loss decreased (0.954035 --> 0.953927).         Saving model ...\n",
      "Epoch: 708 \tTraining Loss: 0.943314 \tValidation Loss: 0.953820\n",
      "Validation loss decreased (0.953927 --> 0.953820).         Saving model ...\n",
      "Epoch: 709 \tTraining Loss: 0.943203 \tValidation Loss: 0.953713\n",
      "Validation loss decreased (0.953820 --> 0.953713).         Saving model ...\n",
      "Epoch: 710 \tTraining Loss: 0.943092 \tValidation Loss: 0.953606\n",
      "Validation loss decreased (0.953713 --> 0.953606).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 711 \tTraining Loss: 0.942980 \tValidation Loss: 0.953499\n",
      "Validation loss decreased (0.953606 --> 0.953499).         Saving model ...\n",
      "Epoch: 712 \tTraining Loss: 0.942869 \tValidation Loss: 0.953393\n",
      "Validation loss decreased (0.953499 --> 0.953393).         Saving model ...\n",
      "Epoch: 713 \tTraining Loss: 0.942759 \tValidation Loss: 0.953286\n",
      "Validation loss decreased (0.953393 --> 0.953286).         Saving model ...\n",
      "Epoch: 714 \tTraining Loss: 0.942648 \tValidation Loss: 0.953180\n",
      "Validation loss decreased (0.953286 --> 0.953180).         Saving model ...\n",
      "Epoch: 715 \tTraining Loss: 0.942538 \tValidation Loss: 0.953074\n",
      "Validation loss decreased (0.953180 --> 0.953074).         Saving model ...\n",
      "Epoch: 716 \tTraining Loss: 0.942428 \tValidation Loss: 0.952969\n",
      "Validation loss decreased (0.953074 --> 0.952969).         Saving model ...\n",
      "Epoch: 717 \tTraining Loss: 0.942318 \tValidation Loss: 0.952863\n",
      "Validation loss decreased (0.952969 --> 0.952863).         Saving model ...\n",
      "Epoch: 718 \tTraining Loss: 0.942208 \tValidation Loss: 0.952758\n",
      "Validation loss decreased (0.952863 --> 0.952758).         Saving model ...\n",
      "Epoch: 719 \tTraining Loss: 0.942099 \tValidation Loss: 0.952653\n",
      "Validation loss decreased (0.952758 --> 0.952653).         Saving model ...\n",
      "Epoch: 720 \tTraining Loss: 0.941989 \tValidation Loss: 0.952548\n",
      "Validation loss decreased (0.952653 --> 0.952548).         Saving model ...\n",
      "Epoch: 721 \tTraining Loss: 0.941880 \tValidation Loss: 0.952443\n",
      "Validation loss decreased (0.952548 --> 0.952443).         Saving model ...\n",
      "Epoch: 722 \tTraining Loss: 0.941771 \tValidation Loss: 0.952339\n",
      "Validation loss decreased (0.952443 --> 0.952339).         Saving model ...\n",
      "Epoch: 723 \tTraining Loss: 0.941662 \tValidation Loss: 0.952234\n",
      "Validation loss decreased (0.952339 --> 0.952234).         Saving model ...\n",
      "Epoch: 724 \tTraining Loss: 0.941554 \tValidation Loss: 0.952130\n",
      "Validation loss decreased (0.952234 --> 0.952130).         Saving model ...\n",
      "Epoch: 725 \tTraining Loss: 0.941446 \tValidation Loss: 0.952026\n",
      "Validation loss decreased (0.952130 --> 0.952026).         Saving model ...\n",
      "Epoch: 726 \tTraining Loss: 0.941338 \tValidation Loss: 0.951923\n",
      "Validation loss decreased (0.952026 --> 0.951923).         Saving model ...\n",
      "Epoch: 727 \tTraining Loss: 0.941230 \tValidation Loss: 0.951819\n",
      "Validation loss decreased (0.951923 --> 0.951819).         Saving model ...\n",
      "Epoch: 728 \tTraining Loss: 0.941122 \tValidation Loss: 0.951716\n",
      "Validation loss decreased (0.951819 --> 0.951716).         Saving model ...\n",
      "Epoch: 729 \tTraining Loss: 0.941014 \tValidation Loss: 0.951613\n",
      "Validation loss decreased (0.951716 --> 0.951613).         Saving model ...\n",
      "Epoch: 730 \tTraining Loss: 0.940907 \tValidation Loss: 0.951510\n",
      "Validation loss decreased (0.951613 --> 0.951510).         Saving model ...\n",
      "Epoch: 731 \tTraining Loss: 0.940800 \tValidation Loss: 0.951407\n",
      "Validation loss decreased (0.951510 --> 0.951407).         Saving model ...\n",
      "Epoch: 732 \tTraining Loss: 0.940693 \tValidation Loss: 0.951304\n",
      "Validation loss decreased (0.951407 --> 0.951304).         Saving model ...\n",
      "Epoch: 733 \tTraining Loss: 0.940586 \tValidation Loss: 0.951202\n",
      "Validation loss decreased (0.951304 --> 0.951202).         Saving model ...\n",
      "Epoch: 734 \tTraining Loss: 0.940480 \tValidation Loss: 0.951100\n",
      "Validation loss decreased (0.951202 --> 0.951100).         Saving model ...\n",
      "Epoch: 735 \tTraining Loss: 0.940374 \tValidation Loss: 0.950998\n",
      "Validation loss decreased (0.951100 --> 0.950998).         Saving model ...\n",
      "Epoch: 736 \tTraining Loss: 0.940267 \tValidation Loss: 0.950896\n",
      "Validation loss decreased (0.950998 --> 0.950896).         Saving model ...\n",
      "Epoch: 737 \tTraining Loss: 0.940161 \tValidation Loss: 0.950795\n",
      "Validation loss decreased (0.950896 --> 0.950795).         Saving model ...\n",
      "Epoch: 738 \tTraining Loss: 0.940056 \tValidation Loss: 0.950693\n",
      "Validation loss decreased (0.950795 --> 0.950693).         Saving model ...\n",
      "Epoch: 739 \tTraining Loss: 0.939950 \tValidation Loss: 0.950592\n",
      "Validation loss decreased (0.950693 --> 0.950592).         Saving model ...\n",
      "Epoch: 740 \tTraining Loss: 0.939845 \tValidation Loss: 0.950491\n",
      "Validation loss decreased (0.950592 --> 0.950491).         Saving model ...\n",
      "Epoch: 741 \tTraining Loss: 0.939740 \tValidation Loss: 0.950390\n",
      "Validation loss decreased (0.950491 --> 0.950390).         Saving model ...\n",
      "Epoch: 742 \tTraining Loss: 0.939635 \tValidation Loss: 0.950290\n",
      "Validation loss decreased (0.950390 --> 0.950290).         Saving model ...\n",
      "Epoch: 743 \tTraining Loss: 0.939530 \tValidation Loss: 0.950189\n",
      "Validation loss decreased (0.950290 --> 0.950189).         Saving model ...\n",
      "Epoch: 744 \tTraining Loss: 0.939425 \tValidation Loss: 0.950089\n",
      "Validation loss decreased (0.950189 --> 0.950089).         Saving model ...\n",
      "Epoch: 745 \tTraining Loss: 0.939321 \tValidation Loss: 0.949989\n",
      "Validation loss decreased (0.950089 --> 0.949989).         Saving model ...\n",
      "Epoch: 746 \tTraining Loss: 0.939217 \tValidation Loss: 0.949889\n",
      "Validation loss decreased (0.949989 --> 0.949889).         Saving model ...\n",
      "Epoch: 747 \tTraining Loss: 0.939113 \tValidation Loss: 0.949789\n",
      "Validation loss decreased (0.949889 --> 0.949789).         Saving model ...\n",
      "Epoch: 748 \tTraining Loss: 0.939009 \tValidation Loss: 0.949690\n",
      "Validation loss decreased (0.949789 --> 0.949690).         Saving model ...\n",
      "Epoch: 749 \tTraining Loss: 0.938905 \tValidation Loss: 0.949590\n",
      "Validation loss decreased (0.949690 --> 0.949590).         Saving model ...\n",
      "Epoch: 750 \tTraining Loss: 0.938802 \tValidation Loss: 0.949491\n",
      "Validation loss decreased (0.949590 --> 0.949491).         Saving model ...\n",
      "Epoch: 751 \tTraining Loss: 0.938698 \tValidation Loss: 0.949392\n",
      "Validation loss decreased (0.949491 --> 0.949392).         Saving model ...\n",
      "Epoch: 752 \tTraining Loss: 0.938595 \tValidation Loss: 0.949294\n",
      "Validation loss decreased (0.949392 --> 0.949294).         Saving model ...\n",
      "Epoch: 753 \tTraining Loss: 0.938492 \tValidation Loss: 0.949195\n",
      "Validation loss decreased (0.949294 --> 0.949195).         Saving model ...\n",
      "Epoch: 754 \tTraining Loss: 0.938389 \tValidation Loss: 0.949097\n",
      "Validation loss decreased (0.949195 --> 0.949097).         Saving model ...\n",
      "Epoch: 755 \tTraining Loss: 0.938287 \tValidation Loss: 0.948998\n",
      "Validation loss decreased (0.949097 --> 0.948998).         Saving model ...\n",
      "Epoch: 756 \tTraining Loss: 0.938184 \tValidation Loss: 0.948900\n",
      "Validation loss decreased (0.948998 --> 0.948900).         Saving model ...\n",
      "Epoch: 757 \tTraining Loss: 0.938082 \tValidation Loss: 0.948802\n",
      "Validation loss decreased (0.948900 --> 0.948802).         Saving model ...\n",
      "Epoch: 758 \tTraining Loss: 0.937980 \tValidation Loss: 0.948705\n",
      "Validation loss decreased (0.948802 --> 0.948705).         Saving model ...\n",
      "Epoch: 759 \tTraining Loss: 0.937878 \tValidation Loss: 0.948607\n",
      "Validation loss decreased (0.948705 --> 0.948607).         Saving model ...\n",
      "Epoch: 760 \tTraining Loss: 0.937777 \tValidation Loss: 0.948510\n",
      "Validation loss decreased (0.948607 --> 0.948510).         Saving model ...\n",
      "Epoch: 761 \tTraining Loss: 0.937675 \tValidation Loss: 0.948413\n",
      "Validation loss decreased (0.948510 --> 0.948413).         Saving model ...\n",
      "Epoch: 762 \tTraining Loss: 0.937574 \tValidation Loss: 0.948316\n",
      "Validation loss decreased (0.948413 --> 0.948316).         Saving model ...\n",
      "Epoch: 763 \tTraining Loss: 0.937473 \tValidation Loss: 0.948219\n",
      "Validation loss decreased (0.948316 --> 0.948219).         Saving model ...\n",
      "Epoch: 764 \tTraining Loss: 0.937372 \tValidation Loss: 0.948122\n",
      "Validation loss decreased (0.948219 --> 0.948122).         Saving model ...\n",
      "Epoch: 765 \tTraining Loss: 0.937271 \tValidation Loss: 0.948026\n",
      "Validation loss decreased (0.948122 --> 0.948026).         Saving model ...\n",
      "Epoch: 766 \tTraining Loss: 0.937171 \tValidation Loss: 0.947930\n",
      "Validation loss decreased (0.948026 --> 0.947930).         Saving model ...\n",
      "Epoch: 767 \tTraining Loss: 0.937070 \tValidation Loss: 0.947833\n",
      "Validation loss decreased (0.947930 --> 0.947833).         Saving model ...\n",
      "Epoch: 768 \tTraining Loss: 0.936970 \tValidation Loss: 0.947738\n",
      "Validation loss decreased (0.947833 --> 0.947738).         Saving model ...\n",
      "Epoch: 769 \tTraining Loss: 0.936870 \tValidation Loss: 0.947642\n",
      "Validation loss decreased (0.947738 --> 0.947642).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 770 \tTraining Loss: 0.936770 \tValidation Loss: 0.947546\n",
      "Validation loss decreased (0.947642 --> 0.947546).         Saving model ...\n",
      "Epoch: 771 \tTraining Loss: 0.936670 \tValidation Loss: 0.947451\n",
      "Validation loss decreased (0.947546 --> 0.947451).         Saving model ...\n",
      "Epoch: 772 \tTraining Loss: 0.936571 \tValidation Loss: 0.947355\n",
      "Validation loss decreased (0.947451 --> 0.947355).         Saving model ...\n",
      "Epoch: 773 \tTraining Loss: 0.936471 \tValidation Loss: 0.947260\n",
      "Validation loss decreased (0.947355 --> 0.947260).         Saving model ...\n",
      "Epoch: 774 \tTraining Loss: 0.936372 \tValidation Loss: 0.947166\n",
      "Validation loss decreased (0.947260 --> 0.947166).         Saving model ...\n",
      "Epoch: 775 \tTraining Loss: 0.936273 \tValidation Loss: 0.947071\n",
      "Validation loss decreased (0.947166 --> 0.947071).         Saving model ...\n",
      "Epoch: 776 \tTraining Loss: 0.936174 \tValidation Loss: 0.946976\n",
      "Validation loss decreased (0.947071 --> 0.946976).         Saving model ...\n",
      "Epoch: 777 \tTraining Loss: 0.936076 \tValidation Loss: 0.946882\n",
      "Validation loss decreased (0.946976 --> 0.946882).         Saving model ...\n",
      "Epoch: 778 \tTraining Loss: 0.935977 \tValidation Loss: 0.946788\n",
      "Validation loss decreased (0.946882 --> 0.946788).         Saving model ...\n",
      "Epoch: 779 \tTraining Loss: 0.935879 \tValidation Loss: 0.946693\n",
      "Validation loss decreased (0.946788 --> 0.946693).         Saving model ...\n",
      "Epoch: 780 \tTraining Loss: 0.935781 \tValidation Loss: 0.946600\n",
      "Validation loss decreased (0.946693 --> 0.946600).         Saving model ...\n",
      "Epoch: 781 \tTraining Loss: 0.935683 \tValidation Loss: 0.946506\n",
      "Validation loss decreased (0.946600 --> 0.946506).         Saving model ...\n",
      "Epoch: 782 \tTraining Loss: 0.935585 \tValidation Loss: 0.946412\n",
      "Validation loss decreased (0.946506 --> 0.946412).         Saving model ...\n",
      "Epoch: 783 \tTraining Loss: 0.935487 \tValidation Loss: 0.946319\n",
      "Validation loss decreased (0.946412 --> 0.946319).         Saving model ...\n",
      "Epoch: 784 \tTraining Loss: 0.935390 \tValidation Loss: 0.946226\n",
      "Validation loss decreased (0.946319 --> 0.946226).         Saving model ...\n",
      "Epoch: 785 \tTraining Loss: 0.935292 \tValidation Loss: 0.946133\n",
      "Validation loss decreased (0.946226 --> 0.946133).         Saving model ...\n",
      "Epoch: 786 \tTraining Loss: 0.935195 \tValidation Loss: 0.946040\n",
      "Validation loss decreased (0.946133 --> 0.946040).         Saving model ...\n",
      "Epoch: 787 \tTraining Loss: 0.935098 \tValidation Loss: 0.945947\n",
      "Validation loss decreased (0.946040 --> 0.945947).         Saving model ...\n",
      "Epoch: 788 \tTraining Loss: 0.935001 \tValidation Loss: 0.945854\n",
      "Validation loss decreased (0.945947 --> 0.945854).         Saving model ...\n",
      "Epoch: 789 \tTraining Loss: 0.934904 \tValidation Loss: 0.945762\n",
      "Validation loss decreased (0.945854 --> 0.945762).         Saving model ...\n",
      "Epoch: 790 \tTraining Loss: 0.934808 \tValidation Loss: 0.945670\n",
      "Validation loss decreased (0.945762 --> 0.945670).         Saving model ...\n",
      "Epoch: 791 \tTraining Loss: 0.934712 \tValidation Loss: 0.945578\n",
      "Validation loss decreased (0.945670 --> 0.945578).         Saving model ...\n",
      "Epoch: 792 \tTraining Loss: 0.934615 \tValidation Loss: 0.945486\n",
      "Validation loss decreased (0.945578 --> 0.945486).         Saving model ...\n",
      "Epoch: 793 \tTraining Loss: 0.934519 \tValidation Loss: 0.945394\n",
      "Validation loss decreased (0.945486 --> 0.945394).         Saving model ...\n",
      "Epoch: 794 \tTraining Loss: 0.934424 \tValidation Loss: 0.945302\n",
      "Validation loss decreased (0.945394 --> 0.945302).         Saving model ...\n",
      "Epoch: 795 \tTraining Loss: 0.934328 \tValidation Loss: 0.945211\n",
      "Validation loss decreased (0.945302 --> 0.945211).         Saving model ...\n",
      "Epoch: 796 \tTraining Loss: 0.934232 \tValidation Loss: 0.945120\n",
      "Validation loss decreased (0.945211 --> 0.945120).         Saving model ...\n",
      "Epoch: 797 \tTraining Loss: 0.934137 \tValidation Loss: 0.945029\n",
      "Validation loss decreased (0.945120 --> 0.945029).         Saving model ...\n",
      "Epoch: 798 \tTraining Loss: 0.934042 \tValidation Loss: 0.944938\n",
      "Validation loss decreased (0.945029 --> 0.944938).         Saving model ...\n",
      "Epoch: 799 \tTraining Loss: 0.933947 \tValidation Loss: 0.944847\n",
      "Validation loss decreased (0.944938 --> 0.944847).         Saving model ...\n",
      "Epoch: 800 \tTraining Loss: 0.933852 \tValidation Loss: 0.944756\n",
      "Validation loss decreased (0.944847 --> 0.944756).         Saving model ...\n",
      "Epoch: 801 \tTraining Loss: 0.933757 \tValidation Loss: 0.944666\n",
      "Validation loss decreased (0.944756 --> 0.944666).         Saving model ...\n",
      "Epoch: 802 \tTraining Loss: 0.933663 \tValidation Loss: 0.944575\n",
      "Validation loss decreased (0.944666 --> 0.944575).         Saving model ...\n",
      "Epoch: 803 \tTraining Loss: 0.933568 \tValidation Loss: 0.944485\n",
      "Validation loss decreased (0.944575 --> 0.944485).         Saving model ...\n",
      "Epoch: 804 \tTraining Loss: 0.933474 \tValidation Loss: 0.944395\n",
      "Validation loss decreased (0.944485 --> 0.944395).         Saving model ...\n",
      "Epoch: 805 \tTraining Loss: 0.933380 \tValidation Loss: 0.944305\n",
      "Validation loss decreased (0.944395 --> 0.944305).         Saving model ...\n",
      "Epoch: 806 \tTraining Loss: 0.933286 \tValidation Loss: 0.944216\n",
      "Validation loss decreased (0.944305 --> 0.944216).         Saving model ...\n",
      "Epoch: 807 \tTraining Loss: 0.933192 \tValidation Loss: 0.944126\n",
      "Validation loss decreased (0.944216 --> 0.944126).         Saving model ...\n",
      "Epoch: 808 \tTraining Loss: 0.933099 \tValidation Loss: 0.944037\n",
      "Validation loss decreased (0.944126 --> 0.944037).         Saving model ...\n",
      "Epoch: 809 \tTraining Loss: 0.933005 \tValidation Loss: 0.943947\n",
      "Validation loss decreased (0.944037 --> 0.943947).         Saving model ...\n",
      "Epoch: 810 \tTraining Loss: 0.932912 \tValidation Loss: 0.943858\n",
      "Validation loss decreased (0.943947 --> 0.943858).         Saving model ...\n",
      "Epoch: 811 \tTraining Loss: 0.932819 \tValidation Loss: 0.943769\n",
      "Validation loss decreased (0.943858 --> 0.943769).         Saving model ...\n",
      "Epoch: 812 \tTraining Loss: 0.932726 \tValidation Loss: 0.943680\n",
      "Validation loss decreased (0.943769 --> 0.943680).         Saving model ...\n",
      "Epoch: 813 \tTraining Loss: 0.932633 \tValidation Loss: 0.943592\n",
      "Validation loss decreased (0.943680 --> 0.943592).         Saving model ...\n",
      "Epoch: 814 \tTraining Loss: 0.932540 \tValidation Loss: 0.943503\n",
      "Validation loss decreased (0.943592 --> 0.943503).         Saving model ...\n",
      "Epoch: 815 \tTraining Loss: 0.932448 \tValidation Loss: 0.943415\n",
      "Validation loss decreased (0.943503 --> 0.943415).         Saving model ...\n",
      "Epoch: 816 \tTraining Loss: 0.932355 \tValidation Loss: 0.943327\n",
      "Validation loss decreased (0.943415 --> 0.943327).         Saving model ...\n",
      "Epoch: 817 \tTraining Loss: 0.932263 \tValidation Loss: 0.943239\n",
      "Validation loss decreased (0.943327 --> 0.943239).         Saving model ...\n",
      "Epoch: 818 \tTraining Loss: 0.932171 \tValidation Loss: 0.943151\n",
      "Validation loss decreased (0.943239 --> 0.943151).         Saving model ...\n",
      "Epoch: 819 \tTraining Loss: 0.932079 \tValidation Loss: 0.943063\n",
      "Validation loss decreased (0.943151 --> 0.943063).         Saving model ...\n",
      "Epoch: 820 \tTraining Loss: 0.931987 \tValidation Loss: 0.942976\n",
      "Validation loss decreased (0.943063 --> 0.942976).         Saving model ...\n",
      "Epoch: 821 \tTraining Loss: 0.931895 \tValidation Loss: 0.942888\n",
      "Validation loss decreased (0.942976 --> 0.942888).         Saving model ...\n",
      "Epoch: 822 \tTraining Loss: 0.931804 \tValidation Loss: 0.942801\n",
      "Validation loss decreased (0.942888 --> 0.942801).         Saving model ...\n",
      "Epoch: 823 \tTraining Loss: 0.931713 \tValidation Loss: 0.942714\n",
      "Validation loss decreased (0.942801 --> 0.942714).         Saving model ...\n",
      "Epoch: 824 \tTraining Loss: 0.931621 \tValidation Loss: 0.942627\n",
      "Validation loss decreased (0.942714 --> 0.942627).         Saving model ...\n",
      "Epoch: 825 \tTraining Loss: 0.931530 \tValidation Loss: 0.942540\n",
      "Validation loss decreased (0.942627 --> 0.942540).         Saving model ...\n",
      "Epoch: 826 \tTraining Loss: 0.931440 \tValidation Loss: 0.942453\n",
      "Validation loss decreased (0.942540 --> 0.942453).         Saving model ...\n",
      "Epoch: 827 \tTraining Loss: 0.931349 \tValidation Loss: 0.942366\n",
      "Validation loss decreased (0.942453 --> 0.942366).         Saving model ...\n",
      "Epoch: 828 \tTraining Loss: 0.931258 \tValidation Loss: 0.942280\n",
      "Validation loss decreased (0.942366 --> 0.942280).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 829 \tTraining Loss: 0.931168 \tValidation Loss: 0.942194\n",
      "Validation loss decreased (0.942280 --> 0.942194).         Saving model ...\n",
      "Epoch: 830 \tTraining Loss: 0.931078 \tValidation Loss: 0.942108\n",
      "Validation loss decreased (0.942194 --> 0.942108).         Saving model ...\n",
      "Epoch: 831 \tTraining Loss: 0.930987 \tValidation Loss: 0.942022\n",
      "Validation loss decreased (0.942108 --> 0.942022).         Saving model ...\n",
      "Epoch: 832 \tTraining Loss: 0.930897 \tValidation Loss: 0.941936\n",
      "Validation loss decreased (0.942022 --> 0.941936).         Saving model ...\n",
      "Epoch: 833 \tTraining Loss: 0.930808 \tValidation Loss: 0.941850\n",
      "Validation loss decreased (0.941936 --> 0.941850).         Saving model ...\n",
      "Epoch: 834 \tTraining Loss: 0.930718 \tValidation Loss: 0.941765\n",
      "Validation loss decreased (0.941850 --> 0.941765).         Saving model ...\n",
      "Epoch: 835 \tTraining Loss: 0.930628 \tValidation Loss: 0.941679\n",
      "Validation loss decreased (0.941765 --> 0.941679).         Saving model ...\n",
      "Epoch: 836 \tTraining Loss: 0.930539 \tValidation Loss: 0.941594\n",
      "Validation loss decreased (0.941679 --> 0.941594).         Saving model ...\n",
      "Epoch: 837 \tTraining Loss: 0.930450 \tValidation Loss: 0.941509\n",
      "Validation loss decreased (0.941594 --> 0.941509).         Saving model ...\n",
      "Epoch: 838 \tTraining Loss: 0.930360 \tValidation Loss: 0.941424\n",
      "Validation loss decreased (0.941509 --> 0.941424).         Saving model ...\n",
      "Epoch: 839 \tTraining Loss: 0.930271 \tValidation Loss: 0.941339\n",
      "Validation loss decreased (0.941424 --> 0.941339).         Saving model ...\n",
      "Epoch: 840 \tTraining Loss: 0.930183 \tValidation Loss: 0.941254\n",
      "Validation loss decreased (0.941339 --> 0.941254).         Saving model ...\n",
      "Epoch: 841 \tTraining Loss: 0.930094 \tValidation Loss: 0.941170\n",
      "Validation loss decreased (0.941254 --> 0.941170).         Saving model ...\n",
      "Epoch: 842 \tTraining Loss: 0.930005 \tValidation Loss: 0.941085\n",
      "Validation loss decreased (0.941170 --> 0.941085).         Saving model ...\n",
      "Epoch: 843 \tTraining Loss: 0.929917 \tValidation Loss: 0.941001\n",
      "Validation loss decreased (0.941085 --> 0.941001).         Saving model ...\n",
      "Epoch: 844 \tTraining Loss: 0.929829 \tValidation Loss: 0.940917\n",
      "Validation loss decreased (0.941001 --> 0.940917).         Saving model ...\n",
      "Epoch: 845 \tTraining Loss: 0.929741 \tValidation Loss: 0.940833\n",
      "Validation loss decreased (0.940917 --> 0.940833).         Saving model ...\n",
      "Epoch: 846 \tTraining Loss: 0.929653 \tValidation Loss: 0.940749\n",
      "Validation loss decreased (0.940833 --> 0.940749).         Saving model ...\n",
      "Epoch: 847 \tTraining Loss: 0.929565 \tValidation Loss: 0.940665\n",
      "Validation loss decreased (0.940749 --> 0.940665).         Saving model ...\n",
      "Epoch: 848 \tTraining Loss: 0.929477 \tValidation Loss: 0.940581\n",
      "Validation loss decreased (0.940665 --> 0.940581).         Saving model ...\n",
      "Epoch: 849 \tTraining Loss: 0.929389 \tValidation Loss: 0.940498\n",
      "Validation loss decreased (0.940581 --> 0.940498).         Saving model ...\n",
      "Epoch: 850 \tTraining Loss: 0.929302 \tValidation Loss: 0.940415\n",
      "Validation loss decreased (0.940498 --> 0.940415).         Saving model ...\n",
      "Epoch: 851 \tTraining Loss: 0.929215 \tValidation Loss: 0.940331\n",
      "Validation loss decreased (0.940415 --> 0.940331).         Saving model ...\n",
      "Epoch: 852 \tTraining Loss: 0.929128 \tValidation Loss: 0.940248\n",
      "Validation loss decreased (0.940331 --> 0.940248).         Saving model ...\n",
      "Epoch: 853 \tTraining Loss: 0.929040 \tValidation Loss: 0.940166\n",
      "Validation loss decreased (0.940248 --> 0.940166).         Saving model ...\n",
      "Epoch: 854 \tTraining Loss: 0.928954 \tValidation Loss: 0.940083\n",
      "Validation loss decreased (0.940166 --> 0.940083).         Saving model ...\n",
      "Epoch: 855 \tTraining Loss: 0.928867 \tValidation Loss: 0.940000\n",
      "Validation loss decreased (0.940083 --> 0.940000).         Saving model ...\n",
      "Epoch: 856 \tTraining Loss: 0.928780 \tValidation Loss: 0.939918\n",
      "Validation loss decreased (0.940000 --> 0.939918).         Saving model ...\n",
      "Epoch: 857 \tTraining Loss: 0.928694 \tValidation Loss: 0.939835\n",
      "Validation loss decreased (0.939918 --> 0.939835).         Saving model ...\n",
      "Epoch: 858 \tTraining Loss: 0.928607 \tValidation Loss: 0.939753\n",
      "Validation loss decreased (0.939835 --> 0.939753).         Saving model ...\n",
      "Epoch: 859 \tTraining Loss: 0.928521 \tValidation Loss: 0.939671\n",
      "Validation loss decreased (0.939753 --> 0.939671).         Saving model ...\n",
      "Epoch: 860 \tTraining Loss: 0.928435 \tValidation Loss: 0.939589\n",
      "Validation loss decreased (0.939671 --> 0.939589).         Saving model ...\n",
      "Epoch: 861 \tTraining Loss: 0.928349 \tValidation Loss: 0.939507\n",
      "Validation loss decreased (0.939589 --> 0.939507).         Saving model ...\n",
      "Epoch: 862 \tTraining Loss: 0.928263 \tValidation Loss: 0.939425\n",
      "Validation loss decreased (0.939507 --> 0.939425).         Saving model ...\n",
      "Epoch: 863 \tTraining Loss: 0.928178 \tValidation Loss: 0.939344\n",
      "Validation loss decreased (0.939425 --> 0.939344).         Saving model ...\n",
      "Epoch: 864 \tTraining Loss: 0.928092 \tValidation Loss: 0.939262\n",
      "Validation loss decreased (0.939344 --> 0.939262).         Saving model ...\n",
      "Epoch: 865 \tTraining Loss: 0.928007 \tValidation Loss: 0.939181\n",
      "Validation loss decreased (0.939262 --> 0.939181).         Saving model ...\n",
      "Epoch: 866 \tTraining Loss: 0.927922 \tValidation Loss: 0.939100\n",
      "Validation loss decreased (0.939181 --> 0.939100).         Saving model ...\n",
      "Epoch: 867 \tTraining Loss: 0.927837 \tValidation Loss: 0.939019\n",
      "Validation loss decreased (0.939100 --> 0.939019).         Saving model ...\n",
      "Epoch: 868 \tTraining Loss: 0.927752 \tValidation Loss: 0.938938\n",
      "Validation loss decreased (0.939019 --> 0.938938).         Saving model ...\n",
      "Epoch: 869 \tTraining Loss: 0.927667 \tValidation Loss: 0.938857\n",
      "Validation loss decreased (0.938938 --> 0.938857).         Saving model ...\n",
      "Epoch: 870 \tTraining Loss: 0.927582 \tValidation Loss: 0.938776\n",
      "Validation loss decreased (0.938857 --> 0.938776).         Saving model ...\n",
      "Epoch: 871 \tTraining Loss: 0.927497 \tValidation Loss: 0.938696\n",
      "Validation loss decreased (0.938776 --> 0.938696).         Saving model ...\n",
      "Epoch: 872 \tTraining Loss: 0.927413 \tValidation Loss: 0.938615\n",
      "Validation loss decreased (0.938696 --> 0.938615).         Saving model ...\n",
      "Epoch: 873 \tTraining Loss: 0.927329 \tValidation Loss: 0.938535\n",
      "Validation loss decreased (0.938615 --> 0.938535).         Saving model ...\n",
      "Epoch: 874 \tTraining Loss: 0.927244 \tValidation Loss: 0.938455\n",
      "Validation loss decreased (0.938535 --> 0.938455).         Saving model ...\n",
      "Epoch: 875 \tTraining Loss: 0.927160 \tValidation Loss: 0.938375\n",
      "Validation loss decreased (0.938455 --> 0.938375).         Saving model ...\n",
      "Epoch: 876 \tTraining Loss: 0.927076 \tValidation Loss: 0.938295\n",
      "Validation loss decreased (0.938375 --> 0.938295).         Saving model ...\n",
      "Epoch: 877 \tTraining Loss: 0.926992 \tValidation Loss: 0.938215\n",
      "Validation loss decreased (0.938295 --> 0.938215).         Saving model ...\n",
      "Epoch: 878 \tTraining Loss: 0.926909 \tValidation Loss: 0.938135\n",
      "Validation loss decreased (0.938215 --> 0.938135).         Saving model ...\n",
      "Epoch: 879 \tTraining Loss: 0.926825 \tValidation Loss: 0.938056\n",
      "Validation loss decreased (0.938135 --> 0.938056).         Saving model ...\n",
      "Epoch: 880 \tTraining Loss: 0.926742 \tValidation Loss: 0.937976\n",
      "Validation loss decreased (0.938056 --> 0.937976).         Saving model ...\n",
      "Epoch: 881 \tTraining Loss: 0.926659 \tValidation Loss: 0.937897\n",
      "Validation loss decreased (0.937976 --> 0.937897).         Saving model ...\n",
      "Epoch: 882 \tTraining Loss: 0.926575 \tValidation Loss: 0.937818\n",
      "Validation loss decreased (0.937897 --> 0.937818).         Saving model ...\n",
      "Epoch: 883 \tTraining Loss: 0.926492 \tValidation Loss: 0.937739\n",
      "Validation loss decreased (0.937818 --> 0.937739).         Saving model ...\n",
      "Epoch: 884 \tTraining Loss: 0.926409 \tValidation Loss: 0.937660\n",
      "Validation loss decreased (0.937739 --> 0.937660).         Saving model ...\n",
      "Epoch: 885 \tTraining Loss: 0.926327 \tValidation Loss: 0.937581\n",
      "Validation loss decreased (0.937660 --> 0.937581).         Saving model ...\n",
      "Epoch: 886 \tTraining Loss: 0.926244 \tValidation Loss: 0.937503\n",
      "Validation loss decreased (0.937581 --> 0.937503).         Saving model ...\n",
      "Epoch: 887 \tTraining Loss: 0.926161 \tValidation Loss: 0.937424\n",
      "Validation loss decreased (0.937503 --> 0.937424).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 888 \tTraining Loss: 0.926079 \tValidation Loss: 0.937346\n",
      "Validation loss decreased (0.937424 --> 0.937346).         Saving model ...\n",
      "Epoch: 889 \tTraining Loss: 0.925997 \tValidation Loss: 0.937267\n",
      "Validation loss decreased (0.937346 --> 0.937267).         Saving model ...\n",
      "Epoch: 890 \tTraining Loss: 0.925915 \tValidation Loss: 0.937189\n",
      "Validation loss decreased (0.937267 --> 0.937189).         Saving model ...\n",
      "Epoch: 891 \tTraining Loss: 0.925832 \tValidation Loss: 0.937111\n",
      "Validation loss decreased (0.937189 --> 0.937111).         Saving model ...\n",
      "Epoch: 892 \tTraining Loss: 0.925751 \tValidation Loss: 0.937033\n",
      "Validation loss decreased (0.937111 --> 0.937033).         Saving model ...\n",
      "Epoch: 893 \tTraining Loss: 0.925669 \tValidation Loss: 0.936956\n",
      "Validation loss decreased (0.937033 --> 0.936956).         Saving model ...\n",
      "Epoch: 894 \tTraining Loss: 0.925587 \tValidation Loss: 0.936878\n",
      "Validation loss decreased (0.936956 --> 0.936878).         Saving model ...\n",
      "Epoch: 895 \tTraining Loss: 0.925506 \tValidation Loss: 0.936800\n",
      "Validation loss decreased (0.936878 --> 0.936800).         Saving model ...\n",
      "Epoch: 896 \tTraining Loss: 0.925424 \tValidation Loss: 0.936723\n",
      "Validation loss decreased (0.936800 --> 0.936723).         Saving model ...\n",
      "Epoch: 897 \tTraining Loss: 0.925343 \tValidation Loss: 0.936646\n",
      "Validation loss decreased (0.936723 --> 0.936646).         Saving model ...\n",
      "Epoch: 898 \tTraining Loss: 0.925262 \tValidation Loss: 0.936568\n",
      "Validation loss decreased (0.936646 --> 0.936568).         Saving model ...\n",
      "Epoch: 899 \tTraining Loss: 0.925181 \tValidation Loss: 0.936491\n",
      "Validation loss decreased (0.936568 --> 0.936491).         Saving model ...\n",
      "Epoch: 900 \tTraining Loss: 0.925100 \tValidation Loss: 0.936414\n",
      "Validation loss decreased (0.936491 --> 0.936414).         Saving model ...\n",
      "Epoch: 901 \tTraining Loss: 0.925019 \tValidation Loss: 0.936338\n",
      "Validation loss decreased (0.936414 --> 0.936338).         Saving model ...\n",
      "Epoch: 902 \tTraining Loss: 0.924938 \tValidation Loss: 0.936261\n",
      "Validation loss decreased (0.936338 --> 0.936261).         Saving model ...\n",
      "Epoch: 903 \tTraining Loss: 0.924858 \tValidation Loss: 0.936184\n",
      "Validation loss decreased (0.936261 --> 0.936184).         Saving model ...\n",
      "Epoch: 904 \tTraining Loss: 0.924777 \tValidation Loss: 0.936108\n",
      "Validation loss decreased (0.936184 --> 0.936108).         Saving model ...\n",
      "Epoch: 905 \tTraining Loss: 0.924697 \tValidation Loss: 0.936032\n",
      "Validation loss decreased (0.936108 --> 0.936032).         Saving model ...\n",
      "Epoch: 906 \tTraining Loss: 0.924617 \tValidation Loss: 0.935955\n",
      "Validation loss decreased (0.936032 --> 0.935955).         Saving model ...\n",
      "Epoch: 907 \tTraining Loss: 0.924537 \tValidation Loss: 0.935879\n",
      "Validation loss decreased (0.935955 --> 0.935879).         Saving model ...\n",
      "Epoch: 908 \tTraining Loss: 0.924457 \tValidation Loss: 0.935803\n",
      "Validation loss decreased (0.935879 --> 0.935803).         Saving model ...\n",
      "Epoch: 909 \tTraining Loss: 0.924377 \tValidation Loss: 0.935727\n",
      "Validation loss decreased (0.935803 --> 0.935727).         Saving model ...\n",
      "Epoch: 910 \tTraining Loss: 0.924297 \tValidation Loss: 0.935651\n",
      "Validation loss decreased (0.935727 --> 0.935651).         Saving model ...\n",
      "Epoch: 911 \tTraining Loss: 0.924217 \tValidation Loss: 0.935576\n",
      "Validation loss decreased (0.935651 --> 0.935576).         Saving model ...\n",
      "Epoch: 912 \tTraining Loss: 0.924138 \tValidation Loss: 0.935500\n",
      "Validation loss decreased (0.935576 --> 0.935500).         Saving model ...\n",
      "Epoch: 913 \tTraining Loss: 0.924059 \tValidation Loss: 0.935425\n",
      "Validation loss decreased (0.935500 --> 0.935425).         Saving model ...\n",
      "Epoch: 914 \tTraining Loss: 0.923979 \tValidation Loss: 0.935350\n",
      "Validation loss decreased (0.935425 --> 0.935350).         Saving model ...\n",
      "Epoch: 915 \tTraining Loss: 0.923900 \tValidation Loss: 0.935274\n",
      "Validation loss decreased (0.935350 --> 0.935274).         Saving model ...\n",
      "Epoch: 916 \tTraining Loss: 0.923821 \tValidation Loss: 0.935199\n",
      "Validation loss decreased (0.935274 --> 0.935199).         Saving model ...\n",
      "Epoch: 917 \tTraining Loss: 0.923742 \tValidation Loss: 0.935124\n",
      "Validation loss decreased (0.935199 --> 0.935124).         Saving model ...\n",
      "Epoch: 918 \tTraining Loss: 0.923663 \tValidation Loss: 0.935049\n",
      "Validation loss decreased (0.935124 --> 0.935049).         Saving model ...\n",
      "Epoch: 919 \tTraining Loss: 0.923585 \tValidation Loss: 0.934975\n",
      "Validation loss decreased (0.935049 --> 0.934975).         Saving model ...\n",
      "Epoch: 920 \tTraining Loss: 0.923506 \tValidation Loss: 0.934900\n",
      "Validation loss decreased (0.934975 --> 0.934900).         Saving model ...\n",
      "Epoch: 921 \tTraining Loss: 0.923428 \tValidation Loss: 0.934826\n",
      "Validation loss decreased (0.934900 --> 0.934826).         Saving model ...\n",
      "Epoch: 922 \tTraining Loss: 0.923350 \tValidation Loss: 0.934751\n",
      "Validation loss decreased (0.934826 --> 0.934751).         Saving model ...\n",
      "Epoch: 923 \tTraining Loss: 0.923271 \tValidation Loss: 0.934677\n",
      "Validation loss decreased (0.934751 --> 0.934677).         Saving model ...\n",
      "Epoch: 924 \tTraining Loss: 0.923193 \tValidation Loss: 0.934603\n",
      "Validation loss decreased (0.934677 --> 0.934603).         Saving model ...\n",
      "Epoch: 925 \tTraining Loss: 0.923115 \tValidation Loss: 0.934529\n",
      "Validation loss decreased (0.934603 --> 0.934529).         Saving model ...\n",
      "Epoch: 926 \tTraining Loss: 0.923037 \tValidation Loss: 0.934455\n",
      "Validation loss decreased (0.934529 --> 0.934455).         Saving model ...\n",
      "Epoch: 927 \tTraining Loss: 0.922960 \tValidation Loss: 0.934381\n",
      "Validation loss decreased (0.934455 --> 0.934381).         Saving model ...\n",
      "Epoch: 928 \tTraining Loss: 0.922882 \tValidation Loss: 0.934307\n",
      "Validation loss decreased (0.934381 --> 0.934307).         Saving model ...\n",
      "Epoch: 929 \tTraining Loss: 0.922804 \tValidation Loss: 0.934233\n",
      "Validation loss decreased (0.934307 --> 0.934233).         Saving model ...\n",
      "Epoch: 930 \tTraining Loss: 0.922727 \tValidation Loss: 0.934160\n",
      "Validation loss decreased (0.934233 --> 0.934160).         Saving model ...\n",
      "Epoch: 931 \tTraining Loss: 0.922650 \tValidation Loss: 0.934087\n",
      "Validation loss decreased (0.934160 --> 0.934087).         Saving model ...\n",
      "Epoch: 932 \tTraining Loss: 0.922573 \tValidation Loss: 0.934013\n",
      "Validation loss decreased (0.934087 --> 0.934013).         Saving model ...\n",
      "Epoch: 933 \tTraining Loss: 0.922496 \tValidation Loss: 0.933940\n",
      "Validation loss decreased (0.934013 --> 0.933940).         Saving model ...\n",
      "Epoch: 934 \tTraining Loss: 0.922419 \tValidation Loss: 0.933867\n",
      "Validation loss decreased (0.933940 --> 0.933867).         Saving model ...\n",
      "Epoch: 935 \tTraining Loss: 0.922342 \tValidation Loss: 0.933794\n",
      "Validation loss decreased (0.933867 --> 0.933794).         Saving model ...\n",
      "Epoch: 936 \tTraining Loss: 0.922265 \tValidation Loss: 0.933721\n",
      "Validation loss decreased (0.933794 --> 0.933721).         Saving model ...\n",
      "Epoch: 937 \tTraining Loss: 0.922188 \tValidation Loss: 0.933648\n",
      "Validation loss decreased (0.933721 --> 0.933648).         Saving model ...\n",
      "Epoch: 938 \tTraining Loss: 0.922112 \tValidation Loss: 0.933576\n",
      "Validation loss decreased (0.933648 --> 0.933576).         Saving model ...\n",
      "Epoch: 939 \tTraining Loss: 0.922035 \tValidation Loss: 0.933503\n",
      "Validation loss decreased (0.933576 --> 0.933503).         Saving model ...\n",
      "Epoch: 940 \tTraining Loss: 0.921959 \tValidation Loss: 0.933431\n",
      "Validation loss decreased (0.933503 --> 0.933431).         Saving model ...\n",
      "Epoch: 941 \tTraining Loss: 0.921883 \tValidation Loss: 0.933359\n",
      "Validation loss decreased (0.933431 --> 0.933359).         Saving model ...\n",
      "Epoch: 942 \tTraining Loss: 0.921807 \tValidation Loss: 0.933286\n",
      "Validation loss decreased (0.933359 --> 0.933286).         Saving model ...\n",
      "Epoch: 943 \tTraining Loss: 0.921731 \tValidation Loss: 0.933214\n",
      "Validation loss decreased (0.933286 --> 0.933214).         Saving model ...\n",
      "Epoch: 944 \tTraining Loss: 0.921655 \tValidation Loss: 0.933142\n",
      "Validation loss decreased (0.933214 --> 0.933142).         Saving model ...\n",
      "Epoch: 945 \tTraining Loss: 0.921579 \tValidation Loss: 0.933070\n",
      "Validation loss decreased (0.933142 --> 0.933070).         Saving model ...\n",
      "Epoch: 946 \tTraining Loss: 0.921504 \tValidation Loss: 0.932999\n",
      "Validation loss decreased (0.933070 --> 0.932999).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 947 \tTraining Loss: 0.921428 \tValidation Loss: 0.932927\n",
      "Validation loss decreased (0.932999 --> 0.932927).         Saving model ...\n",
      "Epoch: 948 \tTraining Loss: 0.921353 \tValidation Loss: 0.932855\n",
      "Validation loss decreased (0.932927 --> 0.932855).         Saving model ...\n",
      "Epoch: 949 \tTraining Loss: 0.921277 \tValidation Loss: 0.932784\n",
      "Validation loss decreased (0.932855 --> 0.932784).         Saving model ...\n",
      "Epoch: 950 \tTraining Loss: 0.921202 \tValidation Loss: 0.932713\n",
      "Validation loss decreased (0.932784 --> 0.932713).         Saving model ...\n",
      "Epoch: 951 \tTraining Loss: 0.921127 \tValidation Loss: 0.932641\n",
      "Validation loss decreased (0.932713 --> 0.932641).         Saving model ...\n",
      "Epoch: 952 \tTraining Loss: 0.921052 \tValidation Loss: 0.932570\n",
      "Validation loss decreased (0.932641 --> 0.932570).         Saving model ...\n",
      "Epoch: 953 \tTraining Loss: 0.920977 \tValidation Loss: 0.932499\n",
      "Validation loss decreased (0.932570 --> 0.932499).         Saving model ...\n",
      "Epoch: 954 \tTraining Loss: 0.920902 \tValidation Loss: 0.932428\n",
      "Validation loss decreased (0.932499 --> 0.932428).         Saving model ...\n",
      "Epoch: 955 \tTraining Loss: 0.920828 \tValidation Loss: 0.932357\n",
      "Validation loss decreased (0.932428 --> 0.932357).         Saving model ...\n",
      "Epoch: 956 \tTraining Loss: 0.920753 \tValidation Loss: 0.932287\n",
      "Validation loss decreased (0.932357 --> 0.932287).         Saving model ...\n",
      "Epoch: 957 \tTraining Loss: 0.920679 \tValidation Loss: 0.932216\n",
      "Validation loss decreased (0.932287 --> 0.932216).         Saving model ...\n",
      "Epoch: 958 \tTraining Loss: 0.920604 \tValidation Loss: 0.932145\n",
      "Validation loss decreased (0.932216 --> 0.932145).         Saving model ...\n",
      "Epoch: 959 \tTraining Loss: 0.920530 \tValidation Loss: 0.932075\n",
      "Validation loss decreased (0.932145 --> 0.932075).         Saving model ...\n",
      "Epoch: 960 \tTraining Loss: 0.920456 \tValidation Loss: 0.932005\n",
      "Validation loss decreased (0.932075 --> 0.932005).         Saving model ...\n",
      "Epoch: 961 \tTraining Loss: 0.920382 \tValidation Loss: 0.931934\n",
      "Validation loss decreased (0.932005 --> 0.931934).         Saving model ...\n",
      "Epoch: 962 \tTraining Loss: 0.920308 \tValidation Loss: 0.931864\n",
      "Validation loss decreased (0.931934 --> 0.931864).         Saving model ...\n",
      "Epoch: 963 \tTraining Loss: 0.920234 \tValidation Loss: 0.931794\n",
      "Validation loss decreased (0.931864 --> 0.931794).         Saving model ...\n",
      "Epoch: 964 \tTraining Loss: 0.920160 \tValidation Loss: 0.931724\n",
      "Validation loss decreased (0.931794 --> 0.931724).         Saving model ...\n",
      "Epoch: 965 \tTraining Loss: 0.920087 \tValidation Loss: 0.931655\n",
      "Validation loss decreased (0.931724 --> 0.931655).         Saving model ...\n",
      "Epoch: 966 \tTraining Loss: 0.920013 \tValidation Loss: 0.931585\n",
      "Validation loss decreased (0.931655 --> 0.931585).         Saving model ...\n",
      "Epoch: 967 \tTraining Loss: 0.919940 \tValidation Loss: 0.931515\n",
      "Validation loss decreased (0.931585 --> 0.931515).         Saving model ...\n",
      "Epoch: 968 \tTraining Loss: 0.919867 \tValidation Loss: 0.931446\n",
      "Validation loss decreased (0.931515 --> 0.931446).         Saving model ...\n",
      "Epoch: 969 \tTraining Loss: 0.919793 \tValidation Loss: 0.931376\n",
      "Validation loss decreased (0.931446 --> 0.931376).         Saving model ...\n",
      "Epoch: 970 \tTraining Loss: 0.919720 \tValidation Loss: 0.931307\n",
      "Validation loss decreased (0.931376 --> 0.931307).         Saving model ...\n",
      "Epoch: 971 \tTraining Loss: 0.919647 \tValidation Loss: 0.931238\n",
      "Validation loss decreased (0.931307 --> 0.931238).         Saving model ...\n",
      "Epoch: 972 \tTraining Loss: 0.919574 \tValidation Loss: 0.931169\n",
      "Validation loss decreased (0.931238 --> 0.931169).         Saving model ...\n",
      "Epoch: 973 \tTraining Loss: 0.919502 \tValidation Loss: 0.931100\n",
      "Validation loss decreased (0.931169 --> 0.931100).         Saving model ...\n",
      "Epoch: 974 \tTraining Loss: 0.919429 \tValidation Loss: 0.931031\n",
      "Validation loss decreased (0.931100 --> 0.931031).         Saving model ...\n",
      "Epoch: 975 \tTraining Loss: 0.919356 \tValidation Loss: 0.930962\n",
      "Validation loss decreased (0.931031 --> 0.930962).         Saving model ...\n",
      "Epoch: 976 \tTraining Loss: 0.919284 \tValidation Loss: 0.930893\n",
      "Validation loss decreased (0.930962 --> 0.930893).         Saving model ...\n",
      "Epoch: 977 \tTraining Loss: 0.919211 \tValidation Loss: 0.930825\n",
      "Validation loss decreased (0.930893 --> 0.930825).         Saving model ...\n",
      "Epoch: 978 \tTraining Loss: 0.919139 \tValidation Loss: 0.930756\n",
      "Validation loss decreased (0.930825 --> 0.930756).         Saving model ...\n",
      "Epoch: 979 \tTraining Loss: 0.919067 \tValidation Loss: 0.930688\n",
      "Validation loss decreased (0.930756 --> 0.930688).         Saving model ...\n",
      "Epoch: 980 \tTraining Loss: 0.918995 \tValidation Loss: 0.930619\n",
      "Validation loss decreased (0.930688 --> 0.930619).         Saving model ...\n",
      "Epoch: 981 \tTraining Loss: 0.918923 \tValidation Loss: 0.930551\n",
      "Validation loss decreased (0.930619 --> 0.930551).         Saving model ...\n",
      "Epoch: 982 \tTraining Loss: 0.918851 \tValidation Loss: 0.930483\n",
      "Validation loss decreased (0.930551 --> 0.930483).         Saving model ...\n",
      "Epoch: 983 \tTraining Loss: 0.918779 \tValidation Loss: 0.930415\n",
      "Validation loss decreased (0.930483 --> 0.930415).         Saving model ...\n",
      "Epoch: 984 \tTraining Loss: 0.918707 \tValidation Loss: 0.930347\n",
      "Validation loss decreased (0.930415 --> 0.930347).         Saving model ...\n",
      "Epoch: 985 \tTraining Loss: 0.918636 \tValidation Loss: 0.930279\n",
      "Validation loss decreased (0.930347 --> 0.930279).         Saving model ...\n",
      "Epoch: 986 \tTraining Loss: 0.918564 \tValidation Loss: 0.930211\n",
      "Validation loss decreased (0.930279 --> 0.930211).         Saving model ...\n",
      "Epoch: 987 \tTraining Loss: 0.918493 \tValidation Loss: 0.930144\n",
      "Validation loss decreased (0.930211 --> 0.930144).         Saving model ...\n",
      "Epoch: 988 \tTraining Loss: 0.918422 \tValidation Loss: 0.930076\n",
      "Validation loss decreased (0.930144 --> 0.930076).         Saving model ...\n",
      "Epoch: 989 \tTraining Loss: 0.918350 \tValidation Loss: 0.930009\n",
      "Validation loss decreased (0.930076 --> 0.930009).         Saving model ...\n",
      "Epoch: 990 \tTraining Loss: 0.918279 \tValidation Loss: 0.929941\n",
      "Validation loss decreased (0.930009 --> 0.929941).         Saving model ...\n",
      "Epoch: 991 \tTraining Loss: 0.918208 \tValidation Loss: 0.929874\n",
      "Validation loss decreased (0.929941 --> 0.929874).         Saving model ...\n",
      "Epoch: 992 \tTraining Loss: 0.918137 \tValidation Loss: 0.929807\n",
      "Validation loss decreased (0.929874 --> 0.929807).         Saving model ...\n",
      "Epoch: 993 \tTraining Loss: 0.918067 \tValidation Loss: 0.929740\n",
      "Validation loss decreased (0.929807 --> 0.929740).         Saving model ...\n",
      "Epoch: 994 \tTraining Loss: 0.917996 \tValidation Loss: 0.929673\n",
      "Validation loss decreased (0.929740 --> 0.929673).         Saving model ...\n",
      "Epoch: 995 \tTraining Loss: 0.917925 \tValidation Loss: 0.929606\n",
      "Validation loss decreased (0.929673 --> 0.929606).         Saving model ...\n",
      "Epoch: 996 \tTraining Loss: 0.917855 \tValidation Loss: 0.929539\n",
      "Validation loss decreased (0.929606 --> 0.929539).         Saving model ...\n",
      "Epoch: 997 \tTraining Loss: 0.917784 \tValidation Loss: 0.929472\n",
      "Validation loss decreased (0.929539 --> 0.929472).         Saving model ...\n",
      "Epoch: 998 \tTraining Loss: 0.917714 \tValidation Loss: 0.929406\n",
      "Validation loss decreased (0.929472 --> 0.929406).         Saving model ...\n",
      "Epoch: 999 \tTraining Loss: 0.917644 \tValidation Loss: 0.929339\n",
      "Validation loss decreased (0.929406 --> 0.929339).         Saving model ...\n",
      "Epoch: 1000 \tTraining Loss: 0.917574 \tValidation Loss: 0.929273\n",
      "Validation loss decreased (0.929339 --> 0.929273).         Saving model ...\n",
      "Epoch: 1001 \tTraining Loss: 0.917504 \tValidation Loss: 0.929207\n",
      "Validation loss decreased (0.929273 --> 0.929207).         Saving model ...\n",
      "Epoch: 1002 \tTraining Loss: 0.917434 \tValidation Loss: 0.929140\n",
      "Validation loss decreased (0.929207 --> 0.929140).         Saving model ...\n",
      "Epoch: 1003 \tTraining Loss: 0.917364 \tValidation Loss: 0.929074\n",
      "Validation loss decreased (0.929140 --> 0.929074).         Saving model ...\n",
      "Epoch: 1004 \tTraining Loss: 0.917294 \tValidation Loss: 0.929008\n",
      "Validation loss decreased (0.929074 --> 0.929008).         Saving model ...\n",
      "Epoch: 1005 \tTraining Loss: 0.917224 \tValidation Loss: 0.928942\n",
      "Validation loss decreased (0.929008 --> 0.928942).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1006 \tTraining Loss: 0.917155 \tValidation Loss: 0.928876\n",
      "Validation loss decreased (0.928942 --> 0.928876).         Saving model ...\n",
      "Epoch: 1007 \tTraining Loss: 0.917085 \tValidation Loss: 0.928810\n",
      "Validation loss decreased (0.928876 --> 0.928810).         Saving model ...\n",
      "Epoch: 1008 \tTraining Loss: 0.917016 \tValidation Loss: 0.928745\n",
      "Validation loss decreased (0.928810 --> 0.928745).         Saving model ...\n",
      "Epoch: 1009 \tTraining Loss: 0.916947 \tValidation Loss: 0.928679\n",
      "Validation loss decreased (0.928745 --> 0.928679).         Saving model ...\n",
      "Epoch: 1010 \tTraining Loss: 0.916878 \tValidation Loss: 0.928614\n",
      "Validation loss decreased (0.928679 --> 0.928614).         Saving model ...\n",
      "Epoch: 1011 \tTraining Loss: 0.916808 \tValidation Loss: 0.928548\n",
      "Validation loss decreased (0.928614 --> 0.928548).         Saving model ...\n",
      "Epoch: 1012 \tTraining Loss: 0.916739 \tValidation Loss: 0.928483\n",
      "Validation loss decreased (0.928548 --> 0.928483).         Saving model ...\n",
      "Epoch: 1013 \tTraining Loss: 0.916671 \tValidation Loss: 0.928418\n",
      "Validation loss decreased (0.928483 --> 0.928418).         Saving model ...\n",
      "Epoch: 1014 \tTraining Loss: 0.916602 \tValidation Loss: 0.928353\n",
      "Validation loss decreased (0.928418 --> 0.928353).         Saving model ...\n",
      "Epoch: 1015 \tTraining Loss: 0.916533 \tValidation Loss: 0.928287\n",
      "Validation loss decreased (0.928353 --> 0.928287).         Saving model ...\n",
      "Epoch: 1016 \tTraining Loss: 0.916464 \tValidation Loss: 0.928222\n",
      "Validation loss decreased (0.928287 --> 0.928222).         Saving model ...\n",
      "Epoch: 1017 \tTraining Loss: 0.916396 \tValidation Loss: 0.928158\n",
      "Validation loss decreased (0.928222 --> 0.928158).         Saving model ...\n",
      "Epoch: 1018 \tTraining Loss: 0.916327 \tValidation Loss: 0.928093\n",
      "Validation loss decreased (0.928158 --> 0.928093).         Saving model ...\n",
      "Epoch: 1019 \tTraining Loss: 0.916259 \tValidation Loss: 0.928028\n",
      "Validation loss decreased (0.928093 --> 0.928028).         Saving model ...\n",
      "Epoch: 1020 \tTraining Loss: 0.916191 \tValidation Loss: 0.927963\n",
      "Validation loss decreased (0.928028 --> 0.927963).         Saving model ...\n",
      "Epoch: 1021 \tTraining Loss: 0.916123 \tValidation Loss: 0.927899\n",
      "Validation loss decreased (0.927963 --> 0.927899).         Saving model ...\n",
      "Epoch: 1022 \tTraining Loss: 0.916054 \tValidation Loss: 0.927835\n",
      "Validation loss decreased (0.927899 --> 0.927835).         Saving model ...\n",
      "Epoch: 1023 \tTraining Loss: 0.915986 \tValidation Loss: 0.927770\n",
      "Validation loss decreased (0.927835 --> 0.927770).         Saving model ...\n",
      "Epoch: 1024 \tTraining Loss: 0.915919 \tValidation Loss: 0.927706\n",
      "Validation loss decreased (0.927770 --> 0.927706).         Saving model ...\n",
      "Epoch: 1025 \tTraining Loss: 0.915851 \tValidation Loss: 0.927642\n",
      "Validation loss decreased (0.927706 --> 0.927642).         Saving model ...\n",
      "Epoch: 1026 \tTraining Loss: 0.915783 \tValidation Loss: 0.927578\n",
      "Validation loss decreased (0.927642 --> 0.927578).         Saving model ...\n",
      "Epoch: 1027 \tTraining Loss: 0.915715 \tValidation Loss: 0.927514\n",
      "Validation loss decreased (0.927578 --> 0.927514).         Saving model ...\n",
      "Epoch: 1028 \tTraining Loss: 0.915648 \tValidation Loss: 0.927450\n",
      "Validation loss decreased (0.927514 --> 0.927450).         Saving model ...\n",
      "Epoch: 1029 \tTraining Loss: 0.915580 \tValidation Loss: 0.927386\n",
      "Validation loss decreased (0.927450 --> 0.927386).         Saving model ...\n",
      "Epoch: 1030 \tTraining Loss: 0.915513 \tValidation Loss: 0.927322\n",
      "Validation loss decreased (0.927386 --> 0.927322).         Saving model ...\n",
      "Epoch: 1031 \tTraining Loss: 0.915446 \tValidation Loss: 0.927259\n",
      "Validation loss decreased (0.927322 --> 0.927259).         Saving model ...\n",
      "Epoch: 1032 \tTraining Loss: 0.915379 \tValidation Loss: 0.927195\n",
      "Validation loss decreased (0.927259 --> 0.927195).         Saving model ...\n",
      "Epoch: 1033 \tTraining Loss: 0.915311 \tValidation Loss: 0.927132\n",
      "Validation loss decreased (0.927195 --> 0.927132).         Saving model ...\n",
      "Epoch: 1034 \tTraining Loss: 0.915244 \tValidation Loss: 0.927068\n",
      "Validation loss decreased (0.927132 --> 0.927068).         Saving model ...\n",
      "Epoch: 1035 \tTraining Loss: 0.915178 \tValidation Loss: 0.927005\n",
      "Validation loss decreased (0.927068 --> 0.927005).         Saving model ...\n",
      "Epoch: 1036 \tTraining Loss: 0.915111 \tValidation Loss: 0.926942\n",
      "Validation loss decreased (0.927005 --> 0.926942).         Saving model ...\n",
      "Epoch: 1037 \tTraining Loss: 0.915044 \tValidation Loss: 0.926879\n",
      "Validation loss decreased (0.926942 --> 0.926879).         Saving model ...\n",
      "Epoch: 1038 \tTraining Loss: 0.914977 \tValidation Loss: 0.926815\n",
      "Validation loss decreased (0.926879 --> 0.926815).         Saving model ...\n",
      "Epoch: 1039 \tTraining Loss: 0.914911 \tValidation Loss: 0.926753\n",
      "Validation loss decreased (0.926815 --> 0.926753).         Saving model ...\n",
      "Epoch: 1040 \tTraining Loss: 0.914844 \tValidation Loss: 0.926690\n",
      "Validation loss decreased (0.926753 --> 0.926690).         Saving model ...\n",
      "Epoch: 1041 \tTraining Loss: 0.914778 \tValidation Loss: 0.926627\n",
      "Validation loss decreased (0.926690 --> 0.926627).         Saving model ...\n",
      "Epoch: 1042 \tTraining Loss: 0.914712 \tValidation Loss: 0.926564\n",
      "Validation loss decreased (0.926627 --> 0.926564).         Saving model ...\n",
      "Epoch: 1043 \tTraining Loss: 0.914645 \tValidation Loss: 0.926502\n",
      "Validation loss decreased (0.926564 --> 0.926502).         Saving model ...\n",
      "Epoch: 1044 \tTraining Loss: 0.914579 \tValidation Loss: 0.926439\n",
      "Validation loss decreased (0.926502 --> 0.926439).         Saving model ...\n",
      "Epoch: 1045 \tTraining Loss: 0.914513 \tValidation Loss: 0.926377\n",
      "Validation loss decreased (0.926439 --> 0.926377).         Saving model ...\n",
      "Epoch: 1046 \tTraining Loss: 0.914447 \tValidation Loss: 0.926314\n",
      "Validation loss decreased (0.926377 --> 0.926314).         Saving model ...\n",
      "Epoch: 1047 \tTraining Loss: 0.914381 \tValidation Loss: 0.926252\n",
      "Validation loss decreased (0.926314 --> 0.926252).         Saving model ...\n",
      "Epoch: 1048 \tTraining Loss: 0.914316 \tValidation Loss: 0.926190\n",
      "Validation loss decreased (0.926252 --> 0.926190).         Saving model ...\n",
      "Epoch: 1049 \tTraining Loss: 0.914250 \tValidation Loss: 0.926128\n",
      "Validation loss decreased (0.926190 --> 0.926128).         Saving model ...\n",
      "Epoch: 1050 \tTraining Loss: 0.914184 \tValidation Loss: 0.926066\n",
      "Validation loss decreased (0.926128 --> 0.926066).         Saving model ...\n",
      "Epoch: 1051 \tTraining Loss: 0.914119 \tValidation Loss: 0.926004\n",
      "Validation loss decreased (0.926066 --> 0.926004).         Saving model ...\n",
      "Epoch: 1052 \tTraining Loss: 0.914053 \tValidation Loss: 0.925942\n",
      "Validation loss decreased (0.926004 --> 0.925942).         Saving model ...\n",
      "Epoch: 1053 \tTraining Loss: 0.913988 \tValidation Loss: 0.925880\n",
      "Validation loss decreased (0.925942 --> 0.925880).         Saving model ...\n",
      "Epoch: 1054 \tTraining Loss: 0.913923 \tValidation Loss: 0.925818\n",
      "Validation loss decreased (0.925880 --> 0.925818).         Saving model ...\n",
      "Epoch: 1055 \tTraining Loss: 0.913857 \tValidation Loss: 0.925757\n",
      "Validation loss decreased (0.925818 --> 0.925757).         Saving model ...\n",
      "Epoch: 1056 \tTraining Loss: 0.913792 \tValidation Loss: 0.925695\n",
      "Validation loss decreased (0.925757 --> 0.925695).         Saving model ...\n",
      "Epoch: 1057 \tTraining Loss: 0.913727 \tValidation Loss: 0.925634\n",
      "Validation loss decreased (0.925695 --> 0.925634).         Saving model ...\n",
      "Epoch: 1058 \tTraining Loss: 0.913662 \tValidation Loss: 0.925572\n",
      "Validation loss decreased (0.925634 --> 0.925572).         Saving model ...\n",
      "Epoch: 1059 \tTraining Loss: 0.913597 \tValidation Loss: 0.925511\n",
      "Validation loss decreased (0.925572 --> 0.925511).         Saving model ...\n",
      "Epoch: 1060 \tTraining Loss: 0.913533 \tValidation Loss: 0.925450\n",
      "Validation loss decreased (0.925511 --> 0.925450).         Saving model ...\n",
      "Epoch: 1061 \tTraining Loss: 0.913468 \tValidation Loss: 0.925388\n",
      "Validation loss decreased (0.925450 --> 0.925388).         Saving model ...\n",
      "Epoch: 1062 \tTraining Loss: 0.913403 \tValidation Loss: 0.925327\n",
      "Validation loss decreased (0.925388 --> 0.925327).         Saving model ...\n",
      "Epoch: 1063 \tTraining Loss: 0.913339 \tValidation Loss: 0.925267\n",
      "Validation loss decreased (0.925327 --> 0.925267).         Saving model ...\n",
      "Epoch: 1064 \tTraining Loss: 0.913274 \tValidation Loss: 0.925206\n",
      "Validation loss decreased (0.925267 --> 0.925206).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1065 \tTraining Loss: 0.913210 \tValidation Loss: 0.925145\n",
      "Validation loss decreased (0.925206 --> 0.925145).         Saving model ...\n",
      "Epoch: 1066 \tTraining Loss: 0.913146 \tValidation Loss: 0.925084\n",
      "Validation loss decreased (0.925145 --> 0.925084).         Saving model ...\n",
      "Epoch: 1067 \tTraining Loss: 0.913082 \tValidation Loss: 0.925023\n",
      "Validation loss decreased (0.925084 --> 0.925023).         Saving model ...\n",
      "Epoch: 1068 \tTraining Loss: 0.913017 \tValidation Loss: 0.924963\n",
      "Validation loss decreased (0.925023 --> 0.924963).         Saving model ...\n",
      "Epoch: 1069 \tTraining Loss: 0.912953 \tValidation Loss: 0.924902\n",
      "Validation loss decreased (0.924963 --> 0.924902).         Saving model ...\n",
      "Epoch: 1070 \tTraining Loss: 0.912889 \tValidation Loss: 0.924842\n",
      "Validation loss decreased (0.924902 --> 0.924842).         Saving model ...\n",
      "Epoch: 1071 \tTraining Loss: 0.912826 \tValidation Loss: 0.924782\n",
      "Validation loss decreased (0.924842 --> 0.924782).         Saving model ...\n",
      "Epoch: 1072 \tTraining Loss: 0.912762 \tValidation Loss: 0.924721\n",
      "Validation loss decreased (0.924782 --> 0.924721).         Saving model ...\n",
      "Epoch: 1073 \tTraining Loss: 0.912698 \tValidation Loss: 0.924661\n",
      "Validation loss decreased (0.924721 --> 0.924661).         Saving model ...\n",
      "Epoch: 1074 \tTraining Loss: 0.912634 \tValidation Loss: 0.924601\n",
      "Validation loss decreased (0.924661 --> 0.924601).         Saving model ...\n",
      "Epoch: 1075 \tTraining Loss: 0.912571 \tValidation Loss: 0.924541\n",
      "Validation loss decreased (0.924601 --> 0.924541).         Saving model ...\n",
      "Epoch: 1076 \tTraining Loss: 0.912507 \tValidation Loss: 0.924481\n",
      "Validation loss decreased (0.924541 --> 0.924481).         Saving model ...\n",
      "Epoch: 1077 \tTraining Loss: 0.912444 \tValidation Loss: 0.924421\n",
      "Validation loss decreased (0.924481 --> 0.924421).         Saving model ...\n",
      "Epoch: 1078 \tTraining Loss: 0.912381 \tValidation Loss: 0.924361\n",
      "Validation loss decreased (0.924421 --> 0.924361).         Saving model ...\n",
      "Epoch: 1079 \tTraining Loss: 0.912318 \tValidation Loss: 0.924302\n",
      "Validation loss decreased (0.924361 --> 0.924302).         Saving model ...\n",
      "Epoch: 1080 \tTraining Loss: 0.912254 \tValidation Loss: 0.924242\n",
      "Validation loss decreased (0.924302 --> 0.924242).         Saving model ...\n",
      "Epoch: 1081 \tTraining Loss: 0.912191 \tValidation Loss: 0.924182\n",
      "Validation loss decreased (0.924242 --> 0.924182).         Saving model ...\n",
      "Epoch: 1082 \tTraining Loss: 0.912128 \tValidation Loss: 0.924123\n",
      "Validation loss decreased (0.924182 --> 0.924123).         Saving model ...\n",
      "Epoch: 1083 \tTraining Loss: 0.912066 \tValidation Loss: 0.924064\n",
      "Validation loss decreased (0.924123 --> 0.924064).         Saving model ...\n",
      "Epoch: 1084 \tTraining Loss: 0.912003 \tValidation Loss: 0.924004\n",
      "Validation loss decreased (0.924064 --> 0.924004).         Saving model ...\n",
      "Epoch: 1085 \tTraining Loss: 0.911940 \tValidation Loss: 0.923945\n",
      "Validation loss decreased (0.924004 --> 0.923945).         Saving model ...\n",
      "Epoch: 1086 \tTraining Loss: 0.911877 \tValidation Loss: 0.923886\n",
      "Validation loss decreased (0.923945 --> 0.923886).         Saving model ...\n",
      "Epoch: 1087 \tTraining Loss: 0.911815 \tValidation Loss: 0.923827\n",
      "Validation loss decreased (0.923886 --> 0.923827).         Saving model ...\n",
      "Epoch: 1088 \tTraining Loss: 0.911752 \tValidation Loss: 0.923768\n",
      "Validation loss decreased (0.923827 --> 0.923768).         Saving model ...\n",
      "Epoch: 1089 \tTraining Loss: 0.911690 \tValidation Loss: 0.923709\n",
      "Validation loss decreased (0.923768 --> 0.923709).         Saving model ...\n",
      "Epoch: 1090 \tTraining Loss: 0.911627 \tValidation Loss: 0.923650\n",
      "Validation loss decreased (0.923709 --> 0.923650).         Saving model ...\n",
      "Epoch: 1091 \tTraining Loss: 0.911565 \tValidation Loss: 0.923591\n",
      "Validation loss decreased (0.923650 --> 0.923591).         Saving model ...\n",
      "Epoch: 1092 \tTraining Loss: 0.911503 \tValidation Loss: 0.923532\n",
      "Validation loss decreased (0.923591 --> 0.923532).         Saving model ...\n",
      "Epoch: 1093 \tTraining Loss: 0.911441 \tValidation Loss: 0.923474\n",
      "Validation loss decreased (0.923532 --> 0.923474).         Saving model ...\n",
      "Epoch: 1094 \tTraining Loss: 0.911379 \tValidation Loss: 0.923415\n",
      "Validation loss decreased (0.923474 --> 0.923415).         Saving model ...\n",
      "Epoch: 1095 \tTraining Loss: 0.911317 \tValidation Loss: 0.923357\n",
      "Validation loss decreased (0.923415 --> 0.923357).         Saving model ...\n",
      "Epoch: 1096 \tTraining Loss: 0.911255 \tValidation Loss: 0.923298\n",
      "Validation loss decreased (0.923357 --> 0.923298).         Saving model ...\n",
      "Epoch: 1097 \tTraining Loss: 0.911193 \tValidation Loss: 0.923240\n",
      "Validation loss decreased (0.923298 --> 0.923240).         Saving model ...\n",
      "Epoch: 1098 \tTraining Loss: 0.911131 \tValidation Loss: 0.923182\n",
      "Validation loss decreased (0.923240 --> 0.923182).         Saving model ...\n",
      "Epoch: 1099 \tTraining Loss: 0.911070 \tValidation Loss: 0.923123\n",
      "Validation loss decreased (0.923182 --> 0.923123).         Saving model ...\n",
      "Epoch: 1100 \tTraining Loss: 0.911008 \tValidation Loss: 0.923065\n",
      "Validation loss decreased (0.923123 --> 0.923065).         Saving model ...\n",
      "Epoch: 1101 \tTraining Loss: 0.910947 \tValidation Loss: 0.923007\n",
      "Validation loss decreased (0.923065 --> 0.923007).         Saving model ...\n",
      "Epoch: 1102 \tTraining Loss: 0.910885 \tValidation Loss: 0.922949\n",
      "Validation loss decreased (0.923007 --> 0.922949).         Saving model ...\n",
      "Epoch: 1103 \tTraining Loss: 0.910824 \tValidation Loss: 0.922891\n",
      "Validation loss decreased (0.922949 --> 0.922891).         Saving model ...\n",
      "Epoch: 1104 \tTraining Loss: 0.910763 \tValidation Loss: 0.922834\n",
      "Validation loss decreased (0.922891 --> 0.922834).         Saving model ...\n",
      "Epoch: 1105 \tTraining Loss: 0.910701 \tValidation Loss: 0.922776\n",
      "Validation loss decreased (0.922834 --> 0.922776).         Saving model ...\n",
      "Epoch: 1106 \tTraining Loss: 0.910640 \tValidation Loss: 0.922718\n",
      "Validation loss decreased (0.922776 --> 0.922718).         Saving model ...\n",
      "Epoch: 1107 \tTraining Loss: 0.910579 \tValidation Loss: 0.922660\n",
      "Validation loss decreased (0.922718 --> 0.922660).         Saving model ...\n",
      "Epoch: 1108 \tTraining Loss: 0.910518 \tValidation Loss: 0.922603\n",
      "Validation loss decreased (0.922660 --> 0.922603).         Saving model ...\n",
      "Epoch: 1109 \tTraining Loss: 0.910457 \tValidation Loss: 0.922545\n",
      "Validation loss decreased (0.922603 --> 0.922545).         Saving model ...\n",
      "Epoch: 1110 \tTraining Loss: 0.910397 \tValidation Loss: 0.922488\n",
      "Validation loss decreased (0.922545 --> 0.922488).         Saving model ...\n",
      "Epoch: 1111 \tTraining Loss: 0.910336 \tValidation Loss: 0.922431\n",
      "Validation loss decreased (0.922488 --> 0.922431).         Saving model ...\n",
      "Epoch: 1112 \tTraining Loss: 0.910275 \tValidation Loss: 0.922373\n",
      "Validation loss decreased (0.922431 --> 0.922373).         Saving model ...\n",
      "Epoch: 1113 \tTraining Loss: 0.910215 \tValidation Loss: 0.922316\n",
      "Validation loss decreased (0.922373 --> 0.922316).         Saving model ...\n",
      "Epoch: 1114 \tTraining Loss: 0.910154 \tValidation Loss: 0.922259\n",
      "Validation loss decreased (0.922316 --> 0.922259).         Saving model ...\n",
      "Epoch: 1115 \tTraining Loss: 0.910094 \tValidation Loss: 0.922202\n",
      "Validation loss decreased (0.922259 --> 0.922202).         Saving model ...\n",
      "Epoch: 1116 \tTraining Loss: 0.910033 \tValidation Loss: 0.922145\n",
      "Validation loss decreased (0.922202 --> 0.922145).         Saving model ...\n",
      "Epoch: 1117 \tTraining Loss: 0.909973 \tValidation Loss: 0.922088\n",
      "Validation loss decreased (0.922145 --> 0.922088).         Saving model ...\n",
      "Epoch: 1118 \tTraining Loss: 0.909913 \tValidation Loss: 0.922032\n",
      "Validation loss decreased (0.922088 --> 0.922032).         Saving model ...\n",
      "Epoch: 1119 \tTraining Loss: 0.909853 \tValidation Loss: 0.921975\n",
      "Validation loss decreased (0.922032 --> 0.921975).         Saving model ...\n",
      "Epoch: 1120 \tTraining Loss: 0.909793 \tValidation Loss: 0.921918\n",
      "Validation loss decreased (0.921975 --> 0.921918).         Saving model ...\n",
      "Epoch: 1121 \tTraining Loss: 0.909733 \tValidation Loss: 0.921861\n",
      "Validation loss decreased (0.921918 --> 0.921861).         Saving model ...\n",
      "Epoch: 1122 \tTraining Loss: 0.909673 \tValidation Loss: 0.921805\n",
      "Validation loss decreased (0.921861 --> 0.921805).         Saving model ...\n",
      "Epoch: 1123 \tTraining Loss: 0.909613 \tValidation Loss: 0.921749\n",
      "Validation loss decreased (0.921805 --> 0.921749).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1124 \tTraining Loss: 0.909553 \tValidation Loss: 0.921692\n",
      "Validation loss decreased (0.921749 --> 0.921692).         Saving model ...\n",
      "Epoch: 1125 \tTraining Loss: 0.909493 \tValidation Loss: 0.921636\n",
      "Validation loss decreased (0.921692 --> 0.921636).         Saving model ...\n",
      "Epoch: 1126 \tTraining Loss: 0.909434 \tValidation Loss: 0.921580\n",
      "Validation loss decreased (0.921636 --> 0.921580).         Saving model ...\n",
      "Epoch: 1127 \tTraining Loss: 0.909374 \tValidation Loss: 0.921523\n",
      "Validation loss decreased (0.921580 --> 0.921523).         Saving model ...\n",
      "Epoch: 1128 \tTraining Loss: 0.909315 \tValidation Loss: 0.921467\n",
      "Validation loss decreased (0.921523 --> 0.921467).         Saving model ...\n",
      "Epoch: 1129 \tTraining Loss: 0.909255 \tValidation Loss: 0.921411\n",
      "Validation loss decreased (0.921467 --> 0.921411).         Saving model ...\n",
      "Epoch: 1130 \tTraining Loss: 0.909196 \tValidation Loss: 0.921355\n",
      "Validation loss decreased (0.921411 --> 0.921355).         Saving model ...\n",
      "Epoch: 1131 \tTraining Loss: 0.909137 \tValidation Loss: 0.921299\n",
      "Validation loss decreased (0.921355 --> 0.921299).         Saving model ...\n",
      "Epoch: 1132 \tTraining Loss: 0.909077 \tValidation Loss: 0.921244\n",
      "Validation loss decreased (0.921299 --> 0.921244).         Saving model ...\n",
      "Epoch: 1133 \tTraining Loss: 0.909018 \tValidation Loss: 0.921188\n",
      "Validation loss decreased (0.921244 --> 0.921188).         Saving model ...\n",
      "Epoch: 1134 \tTraining Loss: 0.908959 \tValidation Loss: 0.921132\n",
      "Validation loss decreased (0.921188 --> 0.921132).         Saving model ...\n",
      "Epoch: 1135 \tTraining Loss: 0.908900 \tValidation Loss: 0.921077\n",
      "Validation loss decreased (0.921132 --> 0.921077).         Saving model ...\n",
      "Epoch: 1136 \tTraining Loss: 0.908841 \tValidation Loss: 0.921021\n",
      "Validation loss decreased (0.921077 --> 0.921021).         Saving model ...\n",
      "Epoch: 1137 \tTraining Loss: 0.908783 \tValidation Loss: 0.920965\n",
      "Validation loss decreased (0.921021 --> 0.920965).         Saving model ...\n",
      "Epoch: 1138 \tTraining Loss: 0.908724 \tValidation Loss: 0.920910\n",
      "Validation loss decreased (0.920965 --> 0.920910).         Saving model ...\n",
      "Epoch: 1139 \tTraining Loss: 0.908665 \tValidation Loss: 0.920855\n",
      "Validation loss decreased (0.920910 --> 0.920855).         Saving model ...\n",
      "Epoch: 1140 \tTraining Loss: 0.908606 \tValidation Loss: 0.920799\n",
      "Validation loss decreased (0.920855 --> 0.920799).         Saving model ...\n",
      "Epoch: 1141 \tTraining Loss: 0.908548 \tValidation Loss: 0.920744\n",
      "Validation loss decreased (0.920799 --> 0.920744).         Saving model ...\n",
      "Epoch: 1142 \tTraining Loss: 0.908489 \tValidation Loss: 0.920689\n",
      "Validation loss decreased (0.920744 --> 0.920689).         Saving model ...\n",
      "Epoch: 1143 \tTraining Loss: 0.908431 \tValidation Loss: 0.920634\n",
      "Validation loss decreased (0.920689 --> 0.920634).         Saving model ...\n",
      "Epoch: 1144 \tTraining Loss: 0.908373 \tValidation Loss: 0.920579\n",
      "Validation loss decreased (0.920634 --> 0.920579).         Saving model ...\n",
      "Epoch: 1145 \tTraining Loss: 0.908314 \tValidation Loss: 0.920524\n",
      "Validation loss decreased (0.920579 --> 0.920524).         Saving model ...\n",
      "Epoch: 1146 \tTraining Loss: 0.908256 \tValidation Loss: 0.920469\n",
      "Validation loss decreased (0.920524 --> 0.920469).         Saving model ...\n",
      "Epoch: 1147 \tTraining Loss: 0.908198 \tValidation Loss: 0.920414\n",
      "Validation loss decreased (0.920469 --> 0.920414).         Saving model ...\n",
      "Epoch: 1148 \tTraining Loss: 0.908140 \tValidation Loss: 0.920360\n",
      "Validation loss decreased (0.920414 --> 0.920360).         Saving model ...\n",
      "Epoch: 1149 \tTraining Loss: 0.908082 \tValidation Loss: 0.920305\n",
      "Validation loss decreased (0.920360 --> 0.920305).         Saving model ...\n",
      "Epoch: 1150 \tTraining Loss: 0.908024 \tValidation Loss: 0.920250\n",
      "Validation loss decreased (0.920305 --> 0.920250).         Saving model ...\n",
      "Epoch: 1151 \tTraining Loss: 0.907966 \tValidation Loss: 0.920196\n",
      "Validation loss decreased (0.920250 --> 0.920196).         Saving model ...\n",
      "Epoch: 1152 \tTraining Loss: 0.907908 \tValidation Loss: 0.920141\n",
      "Validation loss decreased (0.920196 --> 0.920141).         Saving model ...\n",
      "Epoch: 1153 \tTraining Loss: 0.907851 \tValidation Loss: 0.920087\n",
      "Validation loss decreased (0.920141 --> 0.920087).         Saving model ...\n",
      "Epoch: 1154 \tTraining Loss: 0.907793 \tValidation Loss: 0.920033\n",
      "Validation loss decreased (0.920087 --> 0.920033).         Saving model ...\n",
      "Epoch: 1155 \tTraining Loss: 0.907735 \tValidation Loss: 0.919978\n",
      "Validation loss decreased (0.920033 --> 0.919978).         Saving model ...\n",
      "Epoch: 1156 \tTraining Loss: 0.907678 \tValidation Loss: 0.919924\n",
      "Validation loss decreased (0.919978 --> 0.919924).         Saving model ...\n",
      "Epoch: 1157 \tTraining Loss: 0.907620 \tValidation Loss: 0.919870\n",
      "Validation loss decreased (0.919924 --> 0.919870).         Saving model ...\n",
      "Epoch: 1158 \tTraining Loss: 0.907563 \tValidation Loss: 0.919816\n",
      "Validation loss decreased (0.919870 --> 0.919816).         Saving model ...\n",
      "Epoch: 1159 \tTraining Loss: 0.907506 \tValidation Loss: 0.919762\n",
      "Validation loss decreased (0.919816 --> 0.919762).         Saving model ...\n",
      "Epoch: 1160 \tTraining Loss: 0.907448 \tValidation Loss: 0.919708\n",
      "Validation loss decreased (0.919762 --> 0.919708).         Saving model ...\n",
      "Epoch: 1161 \tTraining Loss: 0.907391 \tValidation Loss: 0.919654\n",
      "Validation loss decreased (0.919708 --> 0.919654).         Saving model ...\n",
      "Epoch: 1162 \tTraining Loss: 0.907334 \tValidation Loss: 0.919600\n",
      "Validation loss decreased (0.919654 --> 0.919600).         Saving model ...\n",
      "Epoch: 1163 \tTraining Loss: 0.907277 \tValidation Loss: 0.919547\n",
      "Validation loss decreased (0.919600 --> 0.919547).         Saving model ...\n",
      "Epoch: 1164 \tTraining Loss: 0.907220 \tValidation Loss: 0.919493\n",
      "Validation loss decreased (0.919547 --> 0.919493).         Saving model ...\n",
      "Epoch: 1165 \tTraining Loss: 0.907163 \tValidation Loss: 0.919439\n",
      "Validation loss decreased (0.919493 --> 0.919439).         Saving model ...\n",
      "Epoch: 1166 \tTraining Loss: 0.907106 \tValidation Loss: 0.919386\n",
      "Validation loss decreased (0.919439 --> 0.919386).         Saving model ...\n",
      "Epoch: 1167 \tTraining Loss: 0.907050 \tValidation Loss: 0.919332\n",
      "Validation loss decreased (0.919386 --> 0.919332).         Saving model ...\n",
      "Epoch: 1168 \tTraining Loss: 0.906993 \tValidation Loss: 0.919279\n",
      "Validation loss decreased (0.919332 --> 0.919279).         Saving model ...\n",
      "Epoch: 1169 \tTraining Loss: 0.906936 \tValidation Loss: 0.919225\n",
      "Validation loss decreased (0.919279 --> 0.919225).         Saving model ...\n",
      "Epoch: 1170 \tTraining Loss: 0.906880 \tValidation Loss: 0.919172\n",
      "Validation loss decreased (0.919225 --> 0.919172).         Saving model ...\n",
      "Epoch: 1171 \tTraining Loss: 0.906823 \tValidation Loss: 0.919119\n",
      "Validation loss decreased (0.919172 --> 0.919119).         Saving model ...\n",
      "Epoch: 1172 \tTraining Loss: 0.906767 \tValidation Loss: 0.919066\n",
      "Validation loss decreased (0.919119 --> 0.919066).         Saving model ...\n",
      "Epoch: 1173 \tTraining Loss: 0.906710 \tValidation Loss: 0.919013\n",
      "Validation loss decreased (0.919066 --> 0.919013).         Saving model ...\n",
      "Epoch: 1174 \tTraining Loss: 0.906654 \tValidation Loss: 0.918960\n",
      "Validation loss decreased (0.919013 --> 0.918960).         Saving model ...\n",
      "Epoch: 1175 \tTraining Loss: 0.906598 \tValidation Loss: 0.918907\n",
      "Validation loss decreased (0.918960 --> 0.918907).         Saving model ...\n",
      "Epoch: 1176 \tTraining Loss: 0.906542 \tValidation Loss: 0.918854\n",
      "Validation loss decreased (0.918907 --> 0.918854).         Saving model ...\n",
      "Epoch: 1177 \tTraining Loss: 0.906485 \tValidation Loss: 0.918801\n",
      "Validation loss decreased (0.918854 --> 0.918801).         Saving model ...\n",
      "Epoch: 1178 \tTraining Loss: 0.906429 \tValidation Loss: 0.918748\n",
      "Validation loss decreased (0.918801 --> 0.918748).         Saving model ...\n",
      "Epoch: 1179 \tTraining Loss: 0.906373 \tValidation Loss: 0.918695\n",
      "Validation loss decreased (0.918748 --> 0.918695).         Saving model ...\n",
      "Epoch: 1180 \tTraining Loss: 0.906318 \tValidation Loss: 0.918643\n",
      "Validation loss decreased (0.918695 --> 0.918643).         Saving model ...\n",
      "Epoch: 1181 \tTraining Loss: 0.906262 \tValidation Loss: 0.918590\n",
      "Validation loss decreased (0.918643 --> 0.918590).         Saving model ...\n",
      "Epoch: 1182 \tTraining Loss: 0.906206 \tValidation Loss: 0.918538\n",
      "Validation loss decreased (0.918590 --> 0.918538).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1183 \tTraining Loss: 0.906150 \tValidation Loss: 0.918485\n",
      "Validation loss decreased (0.918538 --> 0.918485).         Saving model ...\n",
      "Epoch: 1184 \tTraining Loss: 0.906095 \tValidation Loss: 0.918433\n",
      "Validation loss decreased (0.918485 --> 0.918433).         Saving model ...\n",
      "Epoch: 1185 \tTraining Loss: 0.906039 \tValidation Loss: 0.918380\n",
      "Validation loss decreased (0.918433 --> 0.918380).         Saving model ...\n",
      "Epoch: 1186 \tTraining Loss: 0.905983 \tValidation Loss: 0.918328\n",
      "Validation loss decreased (0.918380 --> 0.918328).         Saving model ...\n",
      "Epoch: 1187 \tTraining Loss: 0.905928 \tValidation Loss: 0.918276\n",
      "Validation loss decreased (0.918328 --> 0.918276).         Saving model ...\n",
      "Epoch: 1188 \tTraining Loss: 0.905873 \tValidation Loss: 0.918224\n",
      "Validation loss decreased (0.918276 --> 0.918224).         Saving model ...\n",
      "Epoch: 1189 \tTraining Loss: 0.905817 \tValidation Loss: 0.918172\n",
      "Validation loss decreased (0.918224 --> 0.918172).         Saving model ...\n",
      "Epoch: 1190 \tTraining Loss: 0.905762 \tValidation Loss: 0.918120\n",
      "Validation loss decreased (0.918172 --> 0.918120).         Saving model ...\n",
      "Epoch: 1191 \tTraining Loss: 0.905707 \tValidation Loss: 0.918068\n",
      "Validation loss decreased (0.918120 --> 0.918068).         Saving model ...\n",
      "Epoch: 1192 \tTraining Loss: 0.905652 \tValidation Loss: 0.918016\n",
      "Validation loss decreased (0.918068 --> 0.918016).         Saving model ...\n",
      "Epoch: 1193 \tTraining Loss: 0.905597 \tValidation Loss: 0.917964\n",
      "Validation loss decreased (0.918016 --> 0.917964).         Saving model ...\n",
      "Epoch: 1194 \tTraining Loss: 0.905541 \tValidation Loss: 0.917912\n",
      "Validation loss decreased (0.917964 --> 0.917912).         Saving model ...\n",
      "Epoch: 1195 \tTraining Loss: 0.905487 \tValidation Loss: 0.917860\n",
      "Validation loss decreased (0.917912 --> 0.917860).         Saving model ...\n",
      "Epoch: 1196 \tTraining Loss: 0.905432 \tValidation Loss: 0.917809\n",
      "Validation loss decreased (0.917860 --> 0.917809).         Saving model ...\n",
      "Epoch: 1197 \tTraining Loss: 0.905377 \tValidation Loss: 0.917757\n",
      "Validation loss decreased (0.917809 --> 0.917757).         Saving model ...\n",
      "Epoch: 1198 \tTraining Loss: 0.905322 \tValidation Loss: 0.917705\n",
      "Validation loss decreased (0.917757 --> 0.917705).         Saving model ...\n",
      "Epoch: 1199 \tTraining Loss: 0.905267 \tValidation Loss: 0.917654\n",
      "Validation loss decreased (0.917705 --> 0.917654).         Saving model ...\n",
      "Epoch: 1200 \tTraining Loss: 0.905213 \tValidation Loss: 0.917602\n",
      "Validation loss decreased (0.917654 --> 0.917602).         Saving model ...\n",
      "Epoch: 1201 \tTraining Loss: 0.905158 \tValidation Loss: 0.917551\n",
      "Validation loss decreased (0.917602 --> 0.917551).         Saving model ...\n",
      "Epoch: 1202 \tTraining Loss: 0.905104 \tValidation Loss: 0.917500\n",
      "Validation loss decreased (0.917551 --> 0.917500).         Saving model ...\n",
      "Epoch: 1203 \tTraining Loss: 0.905049 \tValidation Loss: 0.917449\n",
      "Validation loss decreased (0.917500 --> 0.917449).         Saving model ...\n",
      "Epoch: 1204 \tTraining Loss: 0.904995 \tValidation Loss: 0.917397\n",
      "Validation loss decreased (0.917449 --> 0.917397).         Saving model ...\n",
      "Epoch: 1205 \tTraining Loss: 0.904940 \tValidation Loss: 0.917346\n",
      "Validation loss decreased (0.917397 --> 0.917346).         Saving model ...\n",
      "Epoch: 1206 \tTraining Loss: 0.904886 \tValidation Loss: 0.917295\n",
      "Validation loss decreased (0.917346 --> 0.917295).         Saving model ...\n",
      "Epoch: 1207 \tTraining Loss: 0.904832 \tValidation Loss: 0.917244\n",
      "Validation loss decreased (0.917295 --> 0.917244).         Saving model ...\n",
      "Epoch: 1208 \tTraining Loss: 0.904778 \tValidation Loss: 0.917193\n",
      "Validation loss decreased (0.917244 --> 0.917193).         Saving model ...\n",
      "Epoch: 1209 \tTraining Loss: 0.904724 \tValidation Loss: 0.917142\n",
      "Validation loss decreased (0.917193 --> 0.917142).         Saving model ...\n",
      "Epoch: 1210 \tTraining Loss: 0.904670 \tValidation Loss: 0.917091\n",
      "Validation loss decreased (0.917142 --> 0.917091).         Saving model ...\n",
      "Epoch: 1211 \tTraining Loss: 0.904616 \tValidation Loss: 0.917041\n",
      "Validation loss decreased (0.917091 --> 0.917041).         Saving model ...\n",
      "Epoch: 1212 \tTraining Loss: 0.904562 \tValidation Loss: 0.916990\n",
      "Validation loss decreased (0.917041 --> 0.916990).         Saving model ...\n",
      "Epoch: 1213 \tTraining Loss: 0.904508 \tValidation Loss: 0.916939\n",
      "Validation loss decreased (0.916990 --> 0.916939).         Saving model ...\n",
      "Epoch: 1214 \tTraining Loss: 0.904454 \tValidation Loss: 0.916889\n",
      "Validation loss decreased (0.916939 --> 0.916889).         Saving model ...\n",
      "Epoch: 1215 \tTraining Loss: 0.904401 \tValidation Loss: 0.916838\n",
      "Validation loss decreased (0.916889 --> 0.916838).         Saving model ...\n",
      "Epoch: 1216 \tTraining Loss: 0.904347 \tValidation Loss: 0.916788\n",
      "Validation loss decreased (0.916838 --> 0.916788).         Saving model ...\n",
      "Epoch: 1217 \tTraining Loss: 0.904293 \tValidation Loss: 0.916737\n",
      "Validation loss decreased (0.916788 --> 0.916737).         Saving model ...\n",
      "Epoch: 1218 \tTraining Loss: 0.904240 \tValidation Loss: 0.916687\n",
      "Validation loss decreased (0.916737 --> 0.916687).         Saving model ...\n",
      "Epoch: 1219 \tTraining Loss: 0.904186 \tValidation Loss: 0.916637\n",
      "Validation loss decreased (0.916687 --> 0.916637).         Saving model ...\n",
      "Epoch: 1220 \tTraining Loss: 0.904133 \tValidation Loss: 0.916586\n",
      "Validation loss decreased (0.916637 --> 0.916586).         Saving model ...\n",
      "Epoch: 1221 \tTraining Loss: 0.904080 \tValidation Loss: 0.916536\n",
      "Validation loss decreased (0.916586 --> 0.916536).         Saving model ...\n",
      "Epoch: 1222 \tTraining Loss: 0.904026 \tValidation Loss: 0.916486\n",
      "Validation loss decreased (0.916536 --> 0.916486).         Saving model ...\n",
      "Epoch: 1223 \tTraining Loss: 0.903973 \tValidation Loss: 0.916436\n",
      "Validation loss decreased (0.916486 --> 0.916436).         Saving model ...\n",
      "Epoch: 1224 \tTraining Loss: 0.903920 \tValidation Loss: 0.916386\n",
      "Validation loss decreased (0.916436 --> 0.916386).         Saving model ...\n",
      "Epoch: 1225 \tTraining Loss: 0.903867 \tValidation Loss: 0.916336\n",
      "Validation loss decreased (0.916386 --> 0.916336).         Saving model ...\n",
      "Epoch: 1226 \tTraining Loss: 0.903814 \tValidation Loss: 0.916286\n",
      "Validation loss decreased (0.916336 --> 0.916286).         Saving model ...\n",
      "Epoch: 1227 \tTraining Loss: 0.903761 \tValidation Loss: 0.916236\n",
      "Validation loss decreased (0.916286 --> 0.916236).         Saving model ...\n",
      "Epoch: 1228 \tTraining Loss: 0.903708 \tValidation Loss: 0.916187\n",
      "Validation loss decreased (0.916236 --> 0.916187).         Saving model ...\n",
      "Epoch: 1229 \tTraining Loss: 0.903655 \tValidation Loss: 0.916137\n",
      "Validation loss decreased (0.916187 --> 0.916137).         Saving model ...\n",
      "Epoch: 1230 \tTraining Loss: 0.903602 \tValidation Loss: 0.916087\n",
      "Validation loss decreased (0.916137 --> 0.916087).         Saving model ...\n",
      "Epoch: 1231 \tTraining Loss: 0.903549 \tValidation Loss: 0.916038\n",
      "Validation loss decreased (0.916087 --> 0.916038).         Saving model ...\n",
      "Epoch: 1232 \tTraining Loss: 0.903497 \tValidation Loss: 0.915988\n",
      "Validation loss decreased (0.916038 --> 0.915988).         Saving model ...\n",
      "Epoch: 1233 \tTraining Loss: 0.903444 \tValidation Loss: 0.915938\n",
      "Validation loss decreased (0.915988 --> 0.915938).         Saving model ...\n",
      "Epoch: 1234 \tTraining Loss: 0.903391 \tValidation Loss: 0.915889\n",
      "Validation loss decreased (0.915938 --> 0.915889).         Saving model ...\n",
      "Epoch: 1235 \tTraining Loss: 0.903339 \tValidation Loss: 0.915840\n",
      "Validation loss decreased (0.915889 --> 0.915840).         Saving model ...\n",
      "Epoch: 1236 \tTraining Loss: 0.903287 \tValidation Loss: 0.915790\n",
      "Validation loss decreased (0.915840 --> 0.915790).         Saving model ...\n",
      "Epoch: 1237 \tTraining Loss: 0.903234 \tValidation Loss: 0.915741\n",
      "Validation loss decreased (0.915790 --> 0.915741).         Saving model ...\n",
      "Epoch: 1238 \tTraining Loss: 0.903182 \tValidation Loss: 0.915692\n",
      "Validation loss decreased (0.915741 --> 0.915692).         Saving model ...\n",
      "Epoch: 1239 \tTraining Loss: 0.903129 \tValidation Loss: 0.915643\n",
      "Validation loss decreased (0.915692 --> 0.915643).         Saving model ...\n",
      "Epoch: 1240 \tTraining Loss: 0.903077 \tValidation Loss: 0.915594\n",
      "Validation loss decreased (0.915643 --> 0.915594).         Saving model ...\n",
      "Epoch: 1241 \tTraining Loss: 0.903025 \tValidation Loss: 0.915544\n",
      "Validation loss decreased (0.915594 --> 0.915544).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1242 \tTraining Loss: 0.902973 \tValidation Loss: 0.915495\n",
      "Validation loss decreased (0.915544 --> 0.915495).         Saving model ...\n",
      "Epoch: 1243 \tTraining Loss: 0.902921 \tValidation Loss: 0.915446\n",
      "Validation loss decreased (0.915495 --> 0.915446).         Saving model ...\n",
      "Epoch: 1244 \tTraining Loss: 0.902869 \tValidation Loss: 0.915398\n",
      "Validation loss decreased (0.915446 --> 0.915398).         Saving model ...\n",
      "Epoch: 1245 \tTraining Loss: 0.902817 \tValidation Loss: 0.915349\n",
      "Validation loss decreased (0.915398 --> 0.915349).         Saving model ...\n",
      "Epoch: 1246 \tTraining Loss: 0.902765 \tValidation Loss: 0.915300\n",
      "Validation loss decreased (0.915349 --> 0.915300).         Saving model ...\n",
      "Epoch: 1247 \tTraining Loss: 0.902713 \tValidation Loss: 0.915251\n",
      "Validation loss decreased (0.915300 --> 0.915251).         Saving model ...\n",
      "Epoch: 1248 \tTraining Loss: 0.902662 \tValidation Loss: 0.915203\n",
      "Validation loss decreased (0.915251 --> 0.915203).         Saving model ...\n",
      "Epoch: 1249 \tTraining Loss: 0.902610 \tValidation Loss: 0.915154\n",
      "Validation loss decreased (0.915203 --> 0.915154).         Saving model ...\n",
      "Epoch: 1250 \tTraining Loss: 0.902558 \tValidation Loss: 0.915105\n",
      "Validation loss decreased (0.915154 --> 0.915105).         Saving model ...\n",
      "Epoch: 1251 \tTraining Loss: 0.902507 \tValidation Loss: 0.915057\n",
      "Validation loss decreased (0.915105 --> 0.915057).         Saving model ...\n",
      "Epoch: 1252 \tTraining Loss: 0.902455 \tValidation Loss: 0.915009\n",
      "Validation loss decreased (0.915057 --> 0.915009).         Saving model ...\n",
      "Epoch: 1253 \tTraining Loss: 0.902404 \tValidation Loss: 0.914960\n",
      "Validation loss decreased (0.915009 --> 0.914960).         Saving model ...\n",
      "Epoch: 1254 \tTraining Loss: 0.902352 \tValidation Loss: 0.914912\n",
      "Validation loss decreased (0.914960 --> 0.914912).         Saving model ...\n",
      "Epoch: 1255 \tTraining Loss: 0.902301 \tValidation Loss: 0.914863\n",
      "Validation loss decreased (0.914912 --> 0.914863).         Saving model ...\n",
      "Epoch: 1256 \tTraining Loss: 0.902249 \tValidation Loss: 0.914815\n",
      "Validation loss decreased (0.914863 --> 0.914815).         Saving model ...\n",
      "Epoch: 1257 \tTraining Loss: 0.902198 \tValidation Loss: 0.914767\n",
      "Validation loss decreased (0.914815 --> 0.914767).         Saving model ...\n",
      "Epoch: 1258 \tTraining Loss: 0.902147 \tValidation Loss: 0.914719\n",
      "Validation loss decreased (0.914767 --> 0.914719).         Saving model ...\n",
      "Epoch: 1259 \tTraining Loss: 0.902096 \tValidation Loss: 0.914671\n",
      "Validation loss decreased (0.914719 --> 0.914671).         Saving model ...\n",
      "Epoch: 1260 \tTraining Loss: 0.902045 \tValidation Loss: 0.914623\n",
      "Validation loss decreased (0.914671 --> 0.914623).         Saving model ...\n",
      "Epoch: 1261 \tTraining Loss: 0.901994 \tValidation Loss: 0.914575\n",
      "Validation loss decreased (0.914623 --> 0.914575).         Saving model ...\n",
      "Epoch: 1262 \tTraining Loss: 0.901943 \tValidation Loss: 0.914527\n",
      "Validation loss decreased (0.914575 --> 0.914527).         Saving model ...\n",
      "Epoch: 1263 \tTraining Loss: 0.901892 \tValidation Loss: 0.914479\n",
      "Validation loss decreased (0.914527 --> 0.914479).         Saving model ...\n",
      "Epoch: 1264 \tTraining Loss: 0.901841 \tValidation Loss: 0.914431\n",
      "Validation loss decreased (0.914479 --> 0.914431).         Saving model ...\n",
      "Epoch: 1265 \tTraining Loss: 0.901790 \tValidation Loss: 0.914384\n",
      "Validation loss decreased (0.914431 --> 0.914384).         Saving model ...\n",
      "Epoch: 1266 \tTraining Loss: 0.901739 \tValidation Loss: 0.914336\n",
      "Validation loss decreased (0.914384 --> 0.914336).         Saving model ...\n",
      "Epoch: 1267 \tTraining Loss: 0.901689 \tValidation Loss: 0.914288\n",
      "Validation loss decreased (0.914336 --> 0.914288).         Saving model ...\n",
      "Epoch: 1268 \tTraining Loss: 0.901638 \tValidation Loss: 0.914241\n",
      "Validation loss decreased (0.914288 --> 0.914241).         Saving model ...\n",
      "Epoch: 1269 \tTraining Loss: 0.901588 \tValidation Loss: 0.914193\n",
      "Validation loss decreased (0.914241 --> 0.914193).         Saving model ...\n",
      "Epoch: 1270 \tTraining Loss: 0.901537 \tValidation Loss: 0.914146\n",
      "Validation loss decreased (0.914193 --> 0.914146).         Saving model ...\n",
      "Epoch: 1271 \tTraining Loss: 0.901487 \tValidation Loss: 0.914098\n",
      "Validation loss decreased (0.914146 --> 0.914098).         Saving model ...\n",
      "Epoch: 1272 \tTraining Loss: 0.901436 \tValidation Loss: 0.914051\n",
      "Validation loss decreased (0.914098 --> 0.914051).         Saving model ...\n",
      "Epoch: 1273 \tTraining Loss: 0.901386 \tValidation Loss: 0.914004\n",
      "Validation loss decreased (0.914051 --> 0.914004).         Saving model ...\n",
      "Epoch: 1274 \tTraining Loss: 0.901335 \tValidation Loss: 0.913956\n",
      "Validation loss decreased (0.914004 --> 0.913956).         Saving model ...\n",
      "Epoch: 1275 \tTraining Loss: 0.901285 \tValidation Loss: 0.913909\n",
      "Validation loss decreased (0.913956 --> 0.913909).         Saving model ...\n",
      "Epoch: 1276 \tTraining Loss: 0.901235 \tValidation Loss: 0.913862\n",
      "Validation loss decreased (0.913909 --> 0.913862).         Saving model ...\n",
      "Epoch: 1277 \tTraining Loss: 0.901185 \tValidation Loss: 0.913815\n",
      "Validation loss decreased (0.913862 --> 0.913815).         Saving model ...\n",
      "Epoch: 1278 \tTraining Loss: 0.901135 \tValidation Loss: 0.913768\n",
      "Validation loss decreased (0.913815 --> 0.913768).         Saving model ...\n",
      "Epoch: 1279 \tTraining Loss: 0.901085 \tValidation Loss: 0.913721\n",
      "Validation loss decreased (0.913768 --> 0.913721).         Saving model ...\n",
      "Epoch: 1280 \tTraining Loss: 0.901035 \tValidation Loss: 0.913674\n",
      "Validation loss decreased (0.913721 --> 0.913674).         Saving model ...\n",
      "Epoch: 1281 \tTraining Loss: 0.900985 \tValidation Loss: 0.913627\n",
      "Validation loss decreased (0.913674 --> 0.913627).         Saving model ...\n",
      "Epoch: 1282 \tTraining Loss: 0.900935 \tValidation Loss: 0.913580\n",
      "Validation loss decreased (0.913627 --> 0.913580).         Saving model ...\n",
      "Epoch: 1283 \tTraining Loss: 0.900885 \tValidation Loss: 0.913533\n",
      "Validation loss decreased (0.913580 --> 0.913533).         Saving model ...\n",
      "Epoch: 1284 \tTraining Loss: 0.900835 \tValidation Loss: 0.913487\n",
      "Validation loss decreased (0.913533 --> 0.913487).         Saving model ...\n",
      "Epoch: 1285 \tTraining Loss: 0.900786 \tValidation Loss: 0.913440\n",
      "Validation loss decreased (0.913487 --> 0.913440).         Saving model ...\n",
      "Epoch: 1286 \tTraining Loss: 0.900736 \tValidation Loss: 0.913393\n",
      "Validation loss decreased (0.913440 --> 0.913393).         Saving model ...\n",
      "Epoch: 1287 \tTraining Loss: 0.900686 \tValidation Loss: 0.913347\n",
      "Validation loss decreased (0.913393 --> 0.913347).         Saving model ...\n",
      "Epoch: 1288 \tTraining Loss: 0.900637 \tValidation Loss: 0.913300\n",
      "Validation loss decreased (0.913347 --> 0.913300).         Saving model ...\n",
      "Epoch: 1289 \tTraining Loss: 0.900587 \tValidation Loss: 0.913254\n",
      "Validation loss decreased (0.913300 --> 0.913254).         Saving model ...\n",
      "Epoch: 1290 \tTraining Loss: 0.900538 \tValidation Loss: 0.913207\n",
      "Validation loss decreased (0.913254 --> 0.913207).         Saving model ...\n",
      "Epoch: 1291 \tTraining Loss: 0.900488 \tValidation Loss: 0.913161\n",
      "Validation loss decreased (0.913207 --> 0.913161).         Saving model ...\n",
      "Epoch: 1292 \tTraining Loss: 0.900439 \tValidation Loss: 0.913114\n",
      "Validation loss decreased (0.913161 --> 0.913114).         Saving model ...\n",
      "Epoch: 1293 \tTraining Loss: 0.900390 \tValidation Loss: 0.913068\n",
      "Validation loss decreased (0.913114 --> 0.913068).         Saving model ...\n",
      "Epoch: 1294 \tTraining Loss: 0.900340 \tValidation Loss: 0.913022\n",
      "Validation loss decreased (0.913068 --> 0.913022).         Saving model ...\n",
      "Epoch: 1295 \tTraining Loss: 0.900291 \tValidation Loss: 0.912976\n",
      "Validation loss decreased (0.913022 --> 0.912976).         Saving model ...\n",
      "Epoch: 1296 \tTraining Loss: 0.900242 \tValidation Loss: 0.912930\n",
      "Validation loss decreased (0.912976 --> 0.912930).         Saving model ...\n",
      "Epoch: 1297 \tTraining Loss: 0.900193 \tValidation Loss: 0.912883\n",
      "Validation loss decreased (0.912930 --> 0.912883).         Saving model ...\n",
      "Epoch: 1298 \tTraining Loss: 0.900144 \tValidation Loss: 0.912837\n",
      "Validation loss decreased (0.912883 --> 0.912837).         Saving model ...\n",
      "Epoch: 1299 \tTraining Loss: 0.900095 \tValidation Loss: 0.912791\n",
      "Validation loss decreased (0.912837 --> 0.912791).         Saving model ...\n",
      "Epoch: 1300 \tTraining Loss: 0.900046 \tValidation Loss: 0.912746\n",
      "Validation loss decreased (0.912791 --> 0.912746).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1301 \tTraining Loss: 0.899997 \tValidation Loss: 0.912700\n",
      "Validation loss decreased (0.912746 --> 0.912700).         Saving model ...\n",
      "Epoch: 1302 \tTraining Loss: 0.899948 \tValidation Loss: 0.912654\n",
      "Validation loss decreased (0.912700 --> 0.912654).         Saving model ...\n",
      "Epoch: 1303 \tTraining Loss: 0.899900 \tValidation Loss: 0.912608\n",
      "Validation loss decreased (0.912654 --> 0.912608).         Saving model ...\n",
      "Epoch: 1304 \tTraining Loss: 0.899851 \tValidation Loss: 0.912562\n",
      "Validation loss decreased (0.912608 --> 0.912562).         Saving model ...\n",
      "Epoch: 1305 \tTraining Loss: 0.899802 \tValidation Loss: 0.912517\n",
      "Validation loss decreased (0.912562 --> 0.912517).         Saving model ...\n",
      "Epoch: 1306 \tTraining Loss: 0.899754 \tValidation Loss: 0.912471\n",
      "Validation loss decreased (0.912517 --> 0.912471).         Saving model ...\n",
      "Epoch: 1307 \tTraining Loss: 0.899705 \tValidation Loss: 0.912425\n",
      "Validation loss decreased (0.912471 --> 0.912425).         Saving model ...\n",
      "Epoch: 1308 \tTraining Loss: 0.899657 \tValidation Loss: 0.912380\n",
      "Validation loss decreased (0.912425 --> 0.912380).         Saving model ...\n",
      "Epoch: 1309 \tTraining Loss: 0.899608 \tValidation Loss: 0.912334\n",
      "Validation loss decreased (0.912380 --> 0.912334).         Saving model ...\n",
      "Epoch: 1310 \tTraining Loss: 0.899560 \tValidation Loss: 0.912289\n",
      "Validation loss decreased (0.912334 --> 0.912289).         Saving model ...\n",
      "Epoch: 1311 \tTraining Loss: 0.899511 \tValidation Loss: 0.912244\n",
      "Validation loss decreased (0.912289 --> 0.912244).         Saving model ...\n",
      "Epoch: 1312 \tTraining Loss: 0.899463 \tValidation Loss: 0.912198\n",
      "Validation loss decreased (0.912244 --> 0.912198).         Saving model ...\n",
      "Epoch: 1313 \tTraining Loss: 0.899415 \tValidation Loss: 0.912153\n",
      "Validation loss decreased (0.912198 --> 0.912153).         Saving model ...\n",
      "Epoch: 1314 \tTraining Loss: 0.899366 \tValidation Loss: 0.912108\n",
      "Validation loss decreased (0.912153 --> 0.912108).         Saving model ...\n",
      "Epoch: 1315 \tTraining Loss: 0.899318 \tValidation Loss: 0.912062\n",
      "Validation loss decreased (0.912108 --> 0.912062).         Saving model ...\n",
      "Epoch: 1316 \tTraining Loss: 0.899270 \tValidation Loss: 0.912017\n",
      "Validation loss decreased (0.912062 --> 0.912017).         Saving model ...\n",
      "Epoch: 1317 \tTraining Loss: 0.899222 \tValidation Loss: 0.911972\n",
      "Validation loss decreased (0.912017 --> 0.911972).         Saving model ...\n",
      "Epoch: 1318 \tTraining Loss: 0.899174 \tValidation Loss: 0.911927\n",
      "Validation loss decreased (0.911972 --> 0.911927).         Saving model ...\n",
      "Epoch: 1319 \tTraining Loss: 0.899126 \tValidation Loss: 0.911882\n",
      "Validation loss decreased (0.911927 --> 0.911882).         Saving model ...\n",
      "Epoch: 1320 \tTraining Loss: 0.899078 \tValidation Loss: 0.911837\n",
      "Validation loss decreased (0.911882 --> 0.911837).         Saving model ...\n",
      "Epoch: 1321 \tTraining Loss: 0.899030 \tValidation Loss: 0.911792\n",
      "Validation loss decreased (0.911837 --> 0.911792).         Saving model ...\n",
      "Epoch: 1322 \tTraining Loss: 0.898983 \tValidation Loss: 0.911747\n",
      "Validation loss decreased (0.911792 --> 0.911747).         Saving model ...\n",
      "Epoch: 1323 \tTraining Loss: 0.898935 \tValidation Loss: 0.911703\n",
      "Validation loss decreased (0.911747 --> 0.911703).         Saving model ...\n",
      "Epoch: 1324 \tTraining Loss: 0.898887 \tValidation Loss: 0.911658\n",
      "Validation loss decreased (0.911703 --> 0.911658).         Saving model ...\n",
      "Epoch: 1325 \tTraining Loss: 0.898839 \tValidation Loss: 0.911613\n",
      "Validation loss decreased (0.911658 --> 0.911613).         Saving model ...\n",
      "Epoch: 1326 \tTraining Loss: 0.898792 \tValidation Loss: 0.911568\n",
      "Validation loss decreased (0.911613 --> 0.911568).         Saving model ...\n",
      "Epoch: 1327 \tTraining Loss: 0.898744 \tValidation Loss: 0.911524\n",
      "Validation loss decreased (0.911568 --> 0.911524).         Saving model ...\n",
      "Epoch: 1328 \tTraining Loss: 0.898697 \tValidation Loss: 0.911479\n",
      "Validation loss decreased (0.911524 --> 0.911479).         Saving model ...\n",
      "Epoch: 1329 \tTraining Loss: 0.898649 \tValidation Loss: 0.911435\n",
      "Validation loss decreased (0.911479 --> 0.911435).         Saving model ...\n",
      "Epoch: 1330 \tTraining Loss: 0.898602 \tValidation Loss: 0.911390\n",
      "Validation loss decreased (0.911435 --> 0.911390).         Saving model ...\n",
      "Epoch: 1331 \tTraining Loss: 0.898555 \tValidation Loss: 0.911346\n",
      "Validation loss decreased (0.911390 --> 0.911346).         Saving model ...\n",
      "Epoch: 1332 \tTraining Loss: 0.898507 \tValidation Loss: 0.911301\n",
      "Validation loss decreased (0.911346 --> 0.911301).         Saving model ...\n",
      "Epoch: 1333 \tTraining Loss: 0.898460 \tValidation Loss: 0.911257\n",
      "Validation loss decreased (0.911301 --> 0.911257).         Saving model ...\n",
      "Epoch: 1334 \tTraining Loss: 0.898413 \tValidation Loss: 0.911213\n",
      "Validation loss decreased (0.911257 --> 0.911213).         Saving model ...\n",
      "Epoch: 1335 \tTraining Loss: 0.898366 \tValidation Loss: 0.911169\n",
      "Validation loss decreased (0.911213 --> 0.911169).         Saving model ...\n",
      "Epoch: 1336 \tTraining Loss: 0.898319 \tValidation Loss: 0.911124\n",
      "Validation loss decreased (0.911169 --> 0.911124).         Saving model ...\n",
      "Epoch: 1337 \tTraining Loss: 0.898271 \tValidation Loss: 0.911080\n",
      "Validation loss decreased (0.911124 --> 0.911080).         Saving model ...\n",
      "Epoch: 1338 \tTraining Loss: 0.898224 \tValidation Loss: 0.911036\n",
      "Validation loss decreased (0.911080 --> 0.911036).         Saving model ...\n",
      "Epoch: 1339 \tTraining Loss: 0.898177 \tValidation Loss: 0.910992\n",
      "Validation loss decreased (0.911036 --> 0.910992).         Saving model ...\n",
      "Epoch: 1340 \tTraining Loss: 0.898131 \tValidation Loss: 0.910948\n",
      "Validation loss decreased (0.910992 --> 0.910948).         Saving model ...\n",
      "Epoch: 1341 \tTraining Loss: 0.898084 \tValidation Loss: 0.910904\n",
      "Validation loss decreased (0.910948 --> 0.910904).         Saving model ...\n",
      "Epoch: 1342 \tTraining Loss: 0.898037 \tValidation Loss: 0.910860\n",
      "Validation loss decreased (0.910904 --> 0.910860).         Saving model ...\n",
      "Epoch: 1343 \tTraining Loss: 0.897990 \tValidation Loss: 0.910816\n",
      "Validation loss decreased (0.910860 --> 0.910816).         Saving model ...\n",
      "Epoch: 1344 \tTraining Loss: 0.897943 \tValidation Loss: 0.910772\n",
      "Validation loss decreased (0.910816 --> 0.910772).         Saving model ...\n",
      "Epoch: 1345 \tTraining Loss: 0.897897 \tValidation Loss: 0.910729\n",
      "Validation loss decreased (0.910772 --> 0.910729).         Saving model ...\n",
      "Epoch: 1346 \tTraining Loss: 0.897850 \tValidation Loss: 0.910685\n",
      "Validation loss decreased (0.910729 --> 0.910685).         Saving model ...\n",
      "Epoch: 1347 \tTraining Loss: 0.897803 \tValidation Loss: 0.910641\n",
      "Validation loss decreased (0.910685 --> 0.910641).         Saving model ...\n",
      "Epoch: 1348 \tTraining Loss: 0.897757 \tValidation Loss: 0.910598\n",
      "Validation loss decreased (0.910641 --> 0.910598).         Saving model ...\n",
      "Epoch: 1349 \tTraining Loss: 0.897710 \tValidation Loss: 0.910554\n",
      "Validation loss decreased (0.910598 --> 0.910554).         Saving model ...\n",
      "Epoch: 1350 \tTraining Loss: 0.897664 \tValidation Loss: 0.910511\n",
      "Validation loss decreased (0.910554 --> 0.910511).         Saving model ...\n",
      "Epoch: 1351 \tTraining Loss: 0.897618 \tValidation Loss: 0.910467\n",
      "Validation loss decreased (0.910511 --> 0.910467).         Saving model ...\n",
      "Epoch: 1352 \tTraining Loss: 0.897571 \tValidation Loss: 0.910424\n",
      "Validation loss decreased (0.910467 --> 0.910424).         Saving model ...\n",
      "Epoch: 1353 \tTraining Loss: 0.897525 \tValidation Loss: 0.910380\n",
      "Validation loss decreased (0.910424 --> 0.910380).         Saving model ...\n",
      "Epoch: 1354 \tTraining Loss: 0.897479 \tValidation Loss: 0.910337\n",
      "Validation loss decreased (0.910380 --> 0.910337).         Saving model ...\n",
      "Epoch: 1355 \tTraining Loss: 0.897432 \tValidation Loss: 0.910293\n",
      "Validation loss decreased (0.910337 --> 0.910293).         Saving model ...\n",
      "Epoch: 1356 \tTraining Loss: 0.897386 \tValidation Loss: 0.910250\n",
      "Validation loss decreased (0.910293 --> 0.910250).         Saving model ...\n",
      "Epoch: 1357 \tTraining Loss: 0.897340 \tValidation Loss: 0.910207\n",
      "Validation loss decreased (0.910250 --> 0.910207).         Saving model ...\n",
      "Epoch: 1358 \tTraining Loss: 0.897294 \tValidation Loss: 0.910164\n",
      "Validation loss decreased (0.910207 --> 0.910164).         Saving model ...\n",
      "Epoch: 1359 \tTraining Loss: 0.897248 \tValidation Loss: 0.910121\n",
      "Validation loss decreased (0.910164 --> 0.910121).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1360 \tTraining Loss: 0.897202 \tValidation Loss: 0.910078\n",
      "Validation loss decreased (0.910121 --> 0.910078).         Saving model ...\n",
      "Epoch: 1361 \tTraining Loss: 0.897156 \tValidation Loss: 0.910034\n",
      "Validation loss decreased (0.910078 --> 0.910034).         Saving model ...\n",
      "Epoch: 1362 \tTraining Loss: 0.897110 \tValidation Loss: 0.909991\n",
      "Validation loss decreased (0.910034 --> 0.909991).         Saving model ...\n",
      "Epoch: 1363 \tTraining Loss: 0.897065 \tValidation Loss: 0.909948\n",
      "Validation loss decreased (0.909991 --> 0.909948).         Saving model ...\n",
      "Epoch: 1364 \tTraining Loss: 0.897019 \tValidation Loss: 0.909906\n",
      "Validation loss decreased (0.909948 --> 0.909906).         Saving model ...\n",
      "Epoch: 1365 \tTraining Loss: 0.896973 \tValidation Loss: 0.909863\n",
      "Validation loss decreased (0.909906 --> 0.909863).         Saving model ...\n",
      "Epoch: 1366 \tTraining Loss: 0.896927 \tValidation Loss: 0.909820\n",
      "Validation loss decreased (0.909863 --> 0.909820).         Saving model ...\n",
      "Epoch: 1367 \tTraining Loss: 0.896882 \tValidation Loss: 0.909777\n",
      "Validation loss decreased (0.909820 --> 0.909777).         Saving model ...\n",
      "Epoch: 1368 \tTraining Loss: 0.896836 \tValidation Loss: 0.909734\n",
      "Validation loss decreased (0.909777 --> 0.909734).         Saving model ...\n",
      "Epoch: 1369 \tTraining Loss: 0.896791 \tValidation Loss: 0.909692\n",
      "Validation loss decreased (0.909734 --> 0.909692).         Saving model ...\n",
      "Epoch: 1370 \tTraining Loss: 0.896745 \tValidation Loss: 0.909649\n",
      "Validation loss decreased (0.909692 --> 0.909649).         Saving model ...\n",
      "Epoch: 1371 \tTraining Loss: 0.896700 \tValidation Loss: 0.909606\n",
      "Validation loss decreased (0.909649 --> 0.909606).         Saving model ...\n",
      "Epoch: 1372 \tTraining Loss: 0.896654 \tValidation Loss: 0.909564\n",
      "Validation loss decreased (0.909606 --> 0.909564).         Saving model ...\n",
      "Epoch: 1373 \tTraining Loss: 0.896609 \tValidation Loss: 0.909521\n",
      "Validation loss decreased (0.909564 --> 0.909521).         Saving model ...\n",
      "Epoch: 1374 \tTraining Loss: 0.896563 \tValidation Loss: 0.909479\n",
      "Validation loss decreased (0.909521 --> 0.909479).         Saving model ...\n",
      "Epoch: 1375 \tTraining Loss: 0.896518 \tValidation Loss: 0.909436\n",
      "Validation loss decreased (0.909479 --> 0.909436).         Saving model ...\n",
      "Epoch: 1376 \tTraining Loss: 0.896473 \tValidation Loss: 0.909394\n",
      "Validation loss decreased (0.909436 --> 0.909394).         Saving model ...\n",
      "Epoch: 1377 \tTraining Loss: 0.896428 \tValidation Loss: 0.909352\n",
      "Validation loss decreased (0.909394 --> 0.909352).         Saving model ...\n",
      "Epoch: 1378 \tTraining Loss: 0.896383 \tValidation Loss: 0.909309\n",
      "Validation loss decreased (0.909352 --> 0.909309).         Saving model ...\n",
      "Epoch: 1379 \tTraining Loss: 0.896338 \tValidation Loss: 0.909267\n",
      "Validation loss decreased (0.909309 --> 0.909267).         Saving model ...\n",
      "Epoch: 1380 \tTraining Loss: 0.896292 \tValidation Loss: 0.909225\n",
      "Validation loss decreased (0.909267 --> 0.909225).         Saving model ...\n",
      "Epoch: 1381 \tTraining Loss: 0.896247 \tValidation Loss: 0.909183\n",
      "Validation loss decreased (0.909225 --> 0.909183).         Saving model ...\n",
      "Epoch: 1382 \tTraining Loss: 0.896202 \tValidation Loss: 0.909141\n",
      "Validation loss decreased (0.909183 --> 0.909141).         Saving model ...\n",
      "Epoch: 1383 \tTraining Loss: 0.896158 \tValidation Loss: 0.909099\n",
      "Validation loss decreased (0.909141 --> 0.909099).         Saving model ...\n",
      "Epoch: 1384 \tTraining Loss: 0.896113 \tValidation Loss: 0.909056\n",
      "Validation loss decreased (0.909099 --> 0.909056).         Saving model ...\n",
      "Epoch: 1385 \tTraining Loss: 0.896068 \tValidation Loss: 0.909014\n",
      "Validation loss decreased (0.909056 --> 0.909014).         Saving model ...\n",
      "Epoch: 1386 \tTraining Loss: 0.896023 \tValidation Loss: 0.908973\n",
      "Validation loss decreased (0.909014 --> 0.908973).         Saving model ...\n",
      "Epoch: 1387 \tTraining Loss: 0.895978 \tValidation Loss: 0.908931\n",
      "Validation loss decreased (0.908973 --> 0.908931).         Saving model ...\n",
      "Epoch: 1388 \tTraining Loss: 0.895934 \tValidation Loss: 0.908889\n",
      "Validation loss decreased (0.908931 --> 0.908889).         Saving model ...\n",
      "Epoch: 1389 \tTraining Loss: 0.895889 \tValidation Loss: 0.908847\n",
      "Validation loss decreased (0.908889 --> 0.908847).         Saving model ...\n",
      "Epoch: 1390 \tTraining Loss: 0.895844 \tValidation Loss: 0.908805\n",
      "Validation loss decreased (0.908847 --> 0.908805).         Saving model ...\n",
      "Epoch: 1391 \tTraining Loss: 0.895800 \tValidation Loss: 0.908763\n",
      "Validation loss decreased (0.908805 --> 0.908763).         Saving model ...\n",
      "Epoch: 1392 \tTraining Loss: 0.895755 \tValidation Loss: 0.908722\n",
      "Validation loss decreased (0.908763 --> 0.908722).         Saving model ...\n",
      "Epoch: 1393 \tTraining Loss: 0.895711 \tValidation Loss: 0.908680\n",
      "Validation loss decreased (0.908722 --> 0.908680).         Saving model ...\n",
      "Epoch: 1394 \tTraining Loss: 0.895666 \tValidation Loss: 0.908639\n",
      "Validation loss decreased (0.908680 --> 0.908639).         Saving model ...\n",
      "Epoch: 1395 \tTraining Loss: 0.895622 \tValidation Loss: 0.908597\n",
      "Validation loss decreased (0.908639 --> 0.908597).         Saving model ...\n",
      "Epoch: 1396 \tTraining Loss: 0.895578 \tValidation Loss: 0.908555\n",
      "Validation loss decreased (0.908597 --> 0.908555).         Saving model ...\n",
      "Epoch: 1397 \tTraining Loss: 0.895533 \tValidation Loss: 0.908514\n",
      "Validation loss decreased (0.908555 --> 0.908514).         Saving model ...\n",
      "Epoch: 1398 \tTraining Loss: 0.895489 \tValidation Loss: 0.908473\n",
      "Validation loss decreased (0.908514 --> 0.908473).         Saving model ...\n",
      "Epoch: 1399 \tTraining Loss: 0.895445 \tValidation Loss: 0.908431\n",
      "Validation loss decreased (0.908473 --> 0.908431).         Saving model ...\n",
      "Epoch: 1400 \tTraining Loss: 0.895401 \tValidation Loss: 0.908390\n",
      "Validation loss decreased (0.908431 --> 0.908390).         Saving model ...\n",
      "Epoch: 1401 \tTraining Loss: 0.895357 \tValidation Loss: 0.908348\n",
      "Validation loss decreased (0.908390 --> 0.908348).         Saving model ...\n",
      "Epoch: 1402 \tTraining Loss: 0.895313 \tValidation Loss: 0.908307\n",
      "Validation loss decreased (0.908348 --> 0.908307).         Saving model ...\n",
      "Epoch: 1403 \tTraining Loss: 0.895269 \tValidation Loss: 0.908266\n",
      "Validation loss decreased (0.908307 --> 0.908266).         Saving model ...\n",
      "Epoch: 1404 \tTraining Loss: 0.895225 \tValidation Loss: 0.908225\n",
      "Validation loss decreased (0.908266 --> 0.908225).         Saving model ...\n",
      "Epoch: 1405 \tTraining Loss: 0.895181 \tValidation Loss: 0.908184\n",
      "Validation loss decreased (0.908225 --> 0.908184).         Saving model ...\n",
      "Epoch: 1406 \tTraining Loss: 0.895137 \tValidation Loss: 0.908143\n",
      "Validation loss decreased (0.908184 --> 0.908143).         Saving model ...\n",
      "Epoch: 1407 \tTraining Loss: 0.895093 \tValidation Loss: 0.908101\n",
      "Validation loss decreased (0.908143 --> 0.908101).         Saving model ...\n",
      "Epoch: 1408 \tTraining Loss: 0.895049 \tValidation Loss: 0.908060\n",
      "Validation loss decreased (0.908101 --> 0.908060).         Saving model ...\n",
      "Epoch: 1409 \tTraining Loss: 0.895005 \tValidation Loss: 0.908019\n",
      "Validation loss decreased (0.908060 --> 0.908019).         Saving model ...\n",
      "Epoch: 1410 \tTraining Loss: 0.894962 \tValidation Loss: 0.907979\n",
      "Validation loss decreased (0.908019 --> 0.907979).         Saving model ...\n",
      "Epoch: 1411 \tTraining Loss: 0.894918 \tValidation Loss: 0.907938\n",
      "Validation loss decreased (0.907979 --> 0.907938).         Saving model ...\n",
      "Epoch: 1412 \tTraining Loss: 0.894874 \tValidation Loss: 0.907897\n",
      "Validation loss decreased (0.907938 --> 0.907897).         Saving model ...\n",
      "Epoch: 1413 \tTraining Loss: 0.894831 \tValidation Loss: 0.907856\n",
      "Validation loss decreased (0.907897 --> 0.907856).         Saving model ...\n",
      "Epoch: 1414 \tTraining Loss: 0.894787 \tValidation Loss: 0.907815\n",
      "Validation loss decreased (0.907856 --> 0.907815).         Saving model ...\n",
      "Epoch: 1415 \tTraining Loss: 0.894744 \tValidation Loss: 0.907774\n",
      "Validation loss decreased (0.907815 --> 0.907774).         Saving model ...\n",
      "Epoch: 1416 \tTraining Loss: 0.894700 \tValidation Loss: 0.907734\n",
      "Validation loss decreased (0.907774 --> 0.907734).         Saving model ...\n",
      "Epoch: 1417 \tTraining Loss: 0.894657 \tValidation Loss: 0.907693\n",
      "Validation loss decreased (0.907734 --> 0.907693).         Saving model ...\n",
      "Epoch: 1418 \tTraining Loss: 0.894613 \tValidation Loss: 0.907652\n",
      "Validation loss decreased (0.907693 --> 0.907652).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1419 \tTraining Loss: 0.894570 \tValidation Loss: 0.907612\n",
      "Validation loss decreased (0.907652 --> 0.907612).         Saving model ...\n",
      "Epoch: 1420 \tTraining Loss: 0.894527 \tValidation Loss: 0.907571\n",
      "Validation loss decreased (0.907612 --> 0.907571).         Saving model ...\n",
      "Epoch: 1421 \tTraining Loss: 0.894483 \tValidation Loss: 0.907531\n",
      "Validation loss decreased (0.907571 --> 0.907531).         Saving model ...\n",
      "Epoch: 1422 \tTraining Loss: 0.894440 \tValidation Loss: 0.907490\n",
      "Validation loss decreased (0.907531 --> 0.907490).         Saving model ...\n",
      "Epoch: 1423 \tTraining Loss: 0.894397 \tValidation Loss: 0.907450\n",
      "Validation loss decreased (0.907490 --> 0.907450).         Saving model ...\n",
      "Epoch: 1424 \tTraining Loss: 0.894354 \tValidation Loss: 0.907410\n",
      "Validation loss decreased (0.907450 --> 0.907410).         Saving model ...\n",
      "Epoch: 1425 \tTraining Loss: 0.894311 \tValidation Loss: 0.907369\n",
      "Validation loss decreased (0.907410 --> 0.907369).         Saving model ...\n",
      "Epoch: 1426 \tTraining Loss: 0.894268 \tValidation Loss: 0.907329\n",
      "Validation loss decreased (0.907369 --> 0.907329).         Saving model ...\n",
      "Epoch: 1427 \tTraining Loss: 0.894225 \tValidation Loss: 0.907289\n",
      "Validation loss decreased (0.907329 --> 0.907289).         Saving model ...\n",
      "Epoch: 1428 \tTraining Loss: 0.894182 \tValidation Loss: 0.907249\n",
      "Validation loss decreased (0.907289 --> 0.907249).         Saving model ...\n",
      "Epoch: 1429 \tTraining Loss: 0.894139 \tValidation Loss: 0.907208\n",
      "Validation loss decreased (0.907249 --> 0.907208).         Saving model ...\n",
      "Epoch: 1430 \tTraining Loss: 0.894096 \tValidation Loss: 0.907168\n",
      "Validation loss decreased (0.907208 --> 0.907168).         Saving model ...\n",
      "Epoch: 1431 \tTraining Loss: 0.894053 \tValidation Loss: 0.907128\n",
      "Validation loss decreased (0.907168 --> 0.907128).         Saving model ...\n",
      "Epoch: 1432 \tTraining Loss: 0.894010 \tValidation Loss: 0.907088\n",
      "Validation loss decreased (0.907128 --> 0.907088).         Saving model ...\n",
      "Epoch: 1433 \tTraining Loss: 0.893968 \tValidation Loss: 0.907048\n",
      "Validation loss decreased (0.907088 --> 0.907048).         Saving model ...\n",
      "Epoch: 1434 \tTraining Loss: 0.893925 \tValidation Loss: 0.907008\n",
      "Validation loss decreased (0.907048 --> 0.907008).         Saving model ...\n",
      "Epoch: 1435 \tTraining Loss: 0.893882 \tValidation Loss: 0.906968\n",
      "Validation loss decreased (0.907008 --> 0.906968).         Saving model ...\n",
      "Epoch: 1436 \tTraining Loss: 0.893839 \tValidation Loss: 0.906928\n",
      "Validation loss decreased (0.906968 --> 0.906928).         Saving model ...\n",
      "Epoch: 1437 \tTraining Loss: 0.893797 \tValidation Loss: 0.906888\n",
      "Validation loss decreased (0.906928 --> 0.906888).         Saving model ...\n",
      "Epoch: 1438 \tTraining Loss: 0.893754 \tValidation Loss: 0.906849\n",
      "Validation loss decreased (0.906888 --> 0.906849).         Saving model ...\n",
      "Epoch: 1439 \tTraining Loss: 0.893712 \tValidation Loss: 0.906809\n",
      "Validation loss decreased (0.906849 --> 0.906809).         Saving model ...\n",
      "Epoch: 1440 \tTraining Loss: 0.893669 \tValidation Loss: 0.906769\n",
      "Validation loss decreased (0.906809 --> 0.906769).         Saving model ...\n",
      "Epoch: 1441 \tTraining Loss: 0.893627 \tValidation Loss: 0.906729\n",
      "Validation loss decreased (0.906769 --> 0.906729).         Saving model ...\n",
      "Epoch: 1442 \tTraining Loss: 0.893585 \tValidation Loss: 0.906690\n",
      "Validation loss decreased (0.906729 --> 0.906690).         Saving model ...\n",
      "Epoch: 1443 \tTraining Loss: 0.893542 \tValidation Loss: 0.906650\n",
      "Validation loss decreased (0.906690 --> 0.906650).         Saving model ...\n",
      "Epoch: 1444 \tTraining Loss: 0.893500 \tValidation Loss: 0.906611\n",
      "Validation loss decreased (0.906650 --> 0.906611).         Saving model ...\n",
      "Epoch: 1445 \tTraining Loss: 0.893458 \tValidation Loss: 0.906571\n",
      "Validation loss decreased (0.906611 --> 0.906571).         Saving model ...\n",
      "Epoch: 1446 \tTraining Loss: 0.893415 \tValidation Loss: 0.906532\n",
      "Validation loss decreased (0.906571 --> 0.906532).         Saving model ...\n",
      "Epoch: 1447 \tTraining Loss: 0.893373 \tValidation Loss: 0.906492\n",
      "Validation loss decreased (0.906532 --> 0.906492).         Saving model ...\n",
      "Epoch: 1448 \tTraining Loss: 0.893331 \tValidation Loss: 0.906453\n",
      "Validation loss decreased (0.906492 --> 0.906453).         Saving model ...\n",
      "Epoch: 1449 \tTraining Loss: 0.893289 \tValidation Loss: 0.906413\n",
      "Validation loss decreased (0.906453 --> 0.906413).         Saving model ...\n",
      "Epoch: 1450 \tTraining Loss: 0.893247 \tValidation Loss: 0.906374\n",
      "Validation loss decreased (0.906413 --> 0.906374).         Saving model ...\n",
      "Epoch: 1451 \tTraining Loss: 0.893205 \tValidation Loss: 0.906335\n",
      "Validation loss decreased (0.906374 --> 0.906335).         Saving model ...\n",
      "Epoch: 1452 \tTraining Loss: 0.893163 \tValidation Loss: 0.906295\n",
      "Validation loss decreased (0.906335 --> 0.906295).         Saving model ...\n",
      "Epoch: 1453 \tTraining Loss: 0.893121 \tValidation Loss: 0.906256\n",
      "Validation loss decreased (0.906295 --> 0.906256).         Saving model ...\n",
      "Epoch: 1454 \tTraining Loss: 0.893079 \tValidation Loss: 0.906217\n",
      "Validation loss decreased (0.906256 --> 0.906217).         Saving model ...\n",
      "Epoch: 1455 \tTraining Loss: 0.893037 \tValidation Loss: 0.906178\n",
      "Validation loss decreased (0.906217 --> 0.906178).         Saving model ...\n",
      "Epoch: 1456 \tTraining Loss: 0.892995 \tValidation Loss: 0.906139\n",
      "Validation loss decreased (0.906178 --> 0.906139).         Saving model ...\n",
      "Epoch: 1457 \tTraining Loss: 0.892954 \tValidation Loss: 0.906100\n",
      "Validation loss decreased (0.906139 --> 0.906100).         Saving model ...\n",
      "Epoch: 1458 \tTraining Loss: 0.892912 \tValidation Loss: 0.906061\n",
      "Validation loss decreased (0.906100 --> 0.906061).         Saving model ...\n",
      "Epoch: 1459 \tTraining Loss: 0.892870 \tValidation Loss: 0.906022\n",
      "Validation loss decreased (0.906061 --> 0.906022).         Saving model ...\n",
      "Epoch: 1460 \tTraining Loss: 0.892828 \tValidation Loss: 0.905983\n",
      "Validation loss decreased (0.906022 --> 0.905983).         Saving model ...\n",
      "Epoch: 1461 \tTraining Loss: 0.892787 \tValidation Loss: 0.905944\n",
      "Validation loss decreased (0.905983 --> 0.905944).         Saving model ...\n",
      "Epoch: 1462 \tTraining Loss: 0.892745 \tValidation Loss: 0.905905\n",
      "Validation loss decreased (0.905944 --> 0.905905).         Saving model ...\n",
      "Epoch: 1463 \tTraining Loss: 0.892704 \tValidation Loss: 0.905866\n",
      "Validation loss decreased (0.905905 --> 0.905866).         Saving model ...\n",
      "Epoch: 1464 \tTraining Loss: 0.892662 \tValidation Loss: 0.905827\n",
      "Validation loss decreased (0.905866 --> 0.905827).         Saving model ...\n",
      "Epoch: 1465 \tTraining Loss: 0.892621 \tValidation Loss: 0.905788\n",
      "Validation loss decreased (0.905827 --> 0.905788).         Saving model ...\n",
      "Epoch: 1466 \tTraining Loss: 0.892579 \tValidation Loss: 0.905750\n",
      "Validation loss decreased (0.905788 --> 0.905750).         Saving model ...\n",
      "Epoch: 1467 \tTraining Loss: 0.892538 \tValidation Loss: 0.905711\n",
      "Validation loss decreased (0.905750 --> 0.905711).         Saving model ...\n",
      "Epoch: 1468 \tTraining Loss: 0.892496 \tValidation Loss: 0.905672\n",
      "Validation loss decreased (0.905711 --> 0.905672).         Saving model ...\n",
      "Epoch: 1469 \tTraining Loss: 0.892455 \tValidation Loss: 0.905634\n",
      "Validation loss decreased (0.905672 --> 0.905634).         Saving model ...\n",
      "Epoch: 1470 \tTraining Loss: 0.892414 \tValidation Loss: 0.905595\n",
      "Validation loss decreased (0.905634 --> 0.905595).         Saving model ...\n",
      "Epoch: 1471 \tTraining Loss: 0.892373 \tValidation Loss: 0.905557\n",
      "Validation loss decreased (0.905595 --> 0.905557).         Saving model ...\n",
      "Epoch: 1472 \tTraining Loss: 0.892331 \tValidation Loss: 0.905518\n",
      "Validation loss decreased (0.905557 --> 0.905518).         Saving model ...\n",
      "Epoch: 1473 \tTraining Loss: 0.892290 \tValidation Loss: 0.905480\n",
      "Validation loss decreased (0.905518 --> 0.905480).         Saving model ...\n",
      "Epoch: 1474 \tTraining Loss: 0.892249 \tValidation Loss: 0.905441\n",
      "Validation loss decreased (0.905480 --> 0.905441).         Saving model ...\n",
      "Epoch: 1475 \tTraining Loss: 0.892208 \tValidation Loss: 0.905403\n",
      "Validation loss decreased (0.905441 --> 0.905403).         Saving model ...\n",
      "Epoch: 1476 \tTraining Loss: 0.892167 \tValidation Loss: 0.905364\n",
      "Validation loss decreased (0.905403 --> 0.905364).         Saving model ...\n",
      "Epoch: 1477 \tTraining Loss: 0.892126 \tValidation Loss: 0.905326\n",
      "Validation loss decreased (0.905364 --> 0.905326).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1478 \tTraining Loss: 0.892085 \tValidation Loss: 0.905288\n",
      "Validation loss decreased (0.905326 --> 0.905288).         Saving model ...\n",
      "Epoch: 1479 \tTraining Loss: 0.892044 \tValidation Loss: 0.905250\n",
      "Validation loss decreased (0.905288 --> 0.905250).         Saving model ...\n",
      "Epoch: 1480 \tTraining Loss: 0.892003 \tValidation Loss: 0.905211\n",
      "Validation loss decreased (0.905250 --> 0.905211).         Saving model ...\n",
      "Epoch: 1481 \tTraining Loss: 0.891962 \tValidation Loss: 0.905173\n",
      "Validation loss decreased (0.905211 --> 0.905173).         Saving model ...\n",
      "Epoch: 1482 \tTraining Loss: 0.891922 \tValidation Loss: 0.905135\n",
      "Validation loss decreased (0.905173 --> 0.905135).         Saving model ...\n",
      "Epoch: 1483 \tTraining Loss: 0.891881 \tValidation Loss: 0.905097\n",
      "Validation loss decreased (0.905135 --> 0.905097).         Saving model ...\n",
      "Epoch: 1484 \tTraining Loss: 0.891840 \tValidation Loss: 0.905059\n",
      "Validation loss decreased (0.905097 --> 0.905059).         Saving model ...\n",
      "Epoch: 1485 \tTraining Loss: 0.891799 \tValidation Loss: 0.905021\n",
      "Validation loss decreased (0.905059 --> 0.905021).         Saving model ...\n",
      "Epoch: 1486 \tTraining Loss: 0.891759 \tValidation Loss: 0.904983\n",
      "Validation loss decreased (0.905021 --> 0.904983).         Saving model ...\n",
      "Epoch: 1487 \tTraining Loss: 0.891718 \tValidation Loss: 0.904945\n",
      "Validation loss decreased (0.904983 --> 0.904945).         Saving model ...\n",
      "Epoch: 1488 \tTraining Loss: 0.891677 \tValidation Loss: 0.904907\n",
      "Validation loss decreased (0.904945 --> 0.904907).         Saving model ...\n",
      "Epoch: 1489 \tTraining Loss: 0.891637 \tValidation Loss: 0.904869\n",
      "Validation loss decreased (0.904907 --> 0.904869).         Saving model ...\n",
      "Epoch: 1490 \tTraining Loss: 0.891596 \tValidation Loss: 0.904831\n",
      "Validation loss decreased (0.904869 --> 0.904831).         Saving model ...\n",
      "Epoch: 1491 \tTraining Loss: 0.891556 \tValidation Loss: 0.904793\n",
      "Validation loss decreased (0.904831 --> 0.904793).         Saving model ...\n",
      "Epoch: 1492 \tTraining Loss: 0.891516 \tValidation Loss: 0.904756\n",
      "Validation loss decreased (0.904793 --> 0.904756).         Saving model ...\n",
      "Epoch: 1493 \tTraining Loss: 0.891475 \tValidation Loss: 0.904718\n",
      "Validation loss decreased (0.904756 --> 0.904718).         Saving model ...\n",
      "Epoch: 1494 \tTraining Loss: 0.891435 \tValidation Loss: 0.904680\n",
      "Validation loss decreased (0.904718 --> 0.904680).         Saving model ...\n",
      "Epoch: 1495 \tTraining Loss: 0.891394 \tValidation Loss: 0.904643\n",
      "Validation loss decreased (0.904680 --> 0.904643).         Saving model ...\n",
      "Epoch: 1496 \tTraining Loss: 0.891354 \tValidation Loss: 0.904605\n",
      "Validation loss decreased (0.904643 --> 0.904605).         Saving model ...\n",
      "Epoch: 1497 \tTraining Loss: 0.891314 \tValidation Loss: 0.904567\n",
      "Validation loss decreased (0.904605 --> 0.904567).         Saving model ...\n",
      "Epoch: 1498 \tTraining Loss: 0.891274 \tValidation Loss: 0.904530\n",
      "Validation loss decreased (0.904567 --> 0.904530).         Saving model ...\n",
      "Epoch: 1499 \tTraining Loss: 0.891234 \tValidation Loss: 0.904492\n",
      "Validation loss decreased (0.904530 --> 0.904492).         Saving model ...\n",
      "Epoch: 1500 \tTraining Loss: 0.891193 \tValidation Loss: 0.904455\n",
      "Validation loss decreased (0.904492 --> 0.904455).         Saving model ...\n",
      "Epoch: 1501 \tTraining Loss: 0.891153 \tValidation Loss: 0.904417\n",
      "Validation loss decreased (0.904455 --> 0.904417).         Saving model ...\n",
      "Epoch: 1502 \tTraining Loss: 0.891113 \tValidation Loss: 0.904380\n",
      "Validation loss decreased (0.904417 --> 0.904380).         Saving model ...\n",
      "Epoch: 1503 \tTraining Loss: 0.891073 \tValidation Loss: 0.904343\n",
      "Validation loss decreased (0.904380 --> 0.904343).         Saving model ...\n",
      "Epoch: 1504 \tTraining Loss: 0.891033 \tValidation Loss: 0.904305\n",
      "Validation loss decreased (0.904343 --> 0.904305).         Saving model ...\n",
      "Epoch: 1505 \tTraining Loss: 0.890993 \tValidation Loss: 0.904268\n",
      "Validation loss decreased (0.904305 --> 0.904268).         Saving model ...\n",
      "Epoch: 1506 \tTraining Loss: 0.890953 \tValidation Loss: 0.904231\n",
      "Validation loss decreased (0.904268 --> 0.904231).         Saving model ...\n",
      "Epoch: 1507 \tTraining Loss: 0.890914 \tValidation Loss: 0.904193\n",
      "Validation loss decreased (0.904231 --> 0.904193).         Saving model ...\n",
      "Epoch: 1508 \tTraining Loss: 0.890874 \tValidation Loss: 0.904156\n",
      "Validation loss decreased (0.904193 --> 0.904156).         Saving model ...\n",
      "Epoch: 1509 \tTraining Loss: 0.890834 \tValidation Loss: 0.904119\n",
      "Validation loss decreased (0.904156 --> 0.904119).         Saving model ...\n",
      "Epoch: 1510 \tTraining Loss: 0.890794 \tValidation Loss: 0.904082\n",
      "Validation loss decreased (0.904119 --> 0.904082).         Saving model ...\n",
      "Epoch: 1511 \tTraining Loss: 0.890754 \tValidation Loss: 0.904045\n",
      "Validation loss decreased (0.904082 --> 0.904045).         Saving model ...\n",
      "Epoch: 1512 \tTraining Loss: 0.890715 \tValidation Loss: 0.904008\n",
      "Validation loss decreased (0.904045 --> 0.904008).         Saving model ...\n",
      "Epoch: 1513 \tTraining Loss: 0.890675 \tValidation Loss: 0.903971\n",
      "Validation loss decreased (0.904008 --> 0.903971).         Saving model ...\n",
      "Epoch: 1514 \tTraining Loss: 0.890635 \tValidation Loss: 0.903934\n",
      "Validation loss decreased (0.903971 --> 0.903934).         Saving model ...\n",
      "Epoch: 1515 \tTraining Loss: 0.890596 \tValidation Loss: 0.903897\n",
      "Validation loss decreased (0.903934 --> 0.903897).         Saving model ...\n",
      "Epoch: 1516 \tTraining Loss: 0.890556 \tValidation Loss: 0.903860\n",
      "Validation loss decreased (0.903897 --> 0.903860).         Saving model ...\n",
      "Epoch: 1517 \tTraining Loss: 0.890517 \tValidation Loss: 0.903823\n",
      "Validation loss decreased (0.903860 --> 0.903823).         Saving model ...\n",
      "Epoch: 1518 \tTraining Loss: 0.890477 \tValidation Loss: 0.903786\n",
      "Validation loss decreased (0.903823 --> 0.903786).         Saving model ...\n",
      "Epoch: 1519 \tTraining Loss: 0.890438 \tValidation Loss: 0.903749\n",
      "Validation loss decreased (0.903786 --> 0.903749).         Saving model ...\n",
      "Epoch: 1520 \tTraining Loss: 0.890398 \tValidation Loss: 0.903712\n",
      "Validation loss decreased (0.903749 --> 0.903712).         Saving model ...\n",
      "Epoch: 1521 \tTraining Loss: 0.890359 \tValidation Loss: 0.903676\n",
      "Validation loss decreased (0.903712 --> 0.903676).         Saving model ...\n",
      "Epoch: 1522 \tTraining Loss: 0.890320 \tValidation Loss: 0.903639\n",
      "Validation loss decreased (0.903676 --> 0.903639).         Saving model ...\n",
      "Epoch: 1523 \tTraining Loss: 0.890280 \tValidation Loss: 0.903602\n",
      "Validation loss decreased (0.903639 --> 0.903602).         Saving model ...\n",
      "Epoch: 1524 \tTraining Loss: 0.890241 \tValidation Loss: 0.903566\n",
      "Validation loss decreased (0.903602 --> 0.903566).         Saving model ...\n",
      "Epoch: 1525 \tTraining Loss: 0.890202 \tValidation Loss: 0.903529\n",
      "Validation loss decreased (0.903566 --> 0.903529).         Saving model ...\n",
      "Epoch: 1526 \tTraining Loss: 0.890163 \tValidation Loss: 0.903493\n",
      "Validation loss decreased (0.903529 --> 0.903493).         Saving model ...\n",
      "Epoch: 1527 \tTraining Loss: 0.890124 \tValidation Loss: 0.903456\n",
      "Validation loss decreased (0.903493 --> 0.903456).         Saving model ...\n",
      "Epoch: 1528 \tTraining Loss: 0.890084 \tValidation Loss: 0.903419\n",
      "Validation loss decreased (0.903456 --> 0.903419).         Saving model ...\n",
      "Epoch: 1529 \tTraining Loss: 0.890045 \tValidation Loss: 0.903383\n",
      "Validation loss decreased (0.903419 --> 0.903383).         Saving model ...\n",
      "Epoch: 1530 \tTraining Loss: 0.890006 \tValidation Loss: 0.903347\n",
      "Validation loss decreased (0.903383 --> 0.903347).         Saving model ...\n",
      "Epoch: 1531 \tTraining Loss: 0.889967 \tValidation Loss: 0.903310\n",
      "Validation loss decreased (0.903347 --> 0.903310).         Saving model ...\n",
      "Epoch: 1532 \tTraining Loss: 0.889928 \tValidation Loss: 0.903274\n",
      "Validation loss decreased (0.903310 --> 0.903274).         Saving model ...\n",
      "Epoch: 1533 \tTraining Loss: 0.889889 \tValidation Loss: 0.903237\n",
      "Validation loss decreased (0.903274 --> 0.903237).         Saving model ...\n",
      "Epoch: 1534 \tTraining Loss: 0.889851 \tValidation Loss: 0.903201\n",
      "Validation loss decreased (0.903237 --> 0.903201).         Saving model ...\n",
      "Epoch: 1535 \tTraining Loss: 0.889812 \tValidation Loss: 0.903165\n",
      "Validation loss decreased (0.903201 --> 0.903165).         Saving model ...\n",
      "Epoch: 1536 \tTraining Loss: 0.889773 \tValidation Loss: 0.903129\n",
      "Validation loss decreased (0.903165 --> 0.903129).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1537 \tTraining Loss: 0.889734 \tValidation Loss: 0.903092\n",
      "Validation loss decreased (0.903129 --> 0.903092).         Saving model ...\n",
      "Epoch: 1538 \tTraining Loss: 0.889695 \tValidation Loss: 0.903056\n",
      "Validation loss decreased (0.903092 --> 0.903056).         Saving model ...\n",
      "Epoch: 1539 \tTraining Loss: 0.889657 \tValidation Loss: 0.903020\n",
      "Validation loss decreased (0.903056 --> 0.903020).         Saving model ...\n",
      "Epoch: 1540 \tTraining Loss: 0.889618 \tValidation Loss: 0.902984\n",
      "Validation loss decreased (0.903020 --> 0.902984).         Saving model ...\n",
      "Epoch: 1541 \tTraining Loss: 0.889579 \tValidation Loss: 0.902948\n",
      "Validation loss decreased (0.902984 --> 0.902948).         Saving model ...\n",
      "Epoch: 1542 \tTraining Loss: 0.889541 \tValidation Loss: 0.902912\n",
      "Validation loss decreased (0.902948 --> 0.902912).         Saving model ...\n",
      "Epoch: 1543 \tTraining Loss: 0.889502 \tValidation Loss: 0.902876\n",
      "Validation loss decreased (0.902912 --> 0.902876).         Saving model ...\n",
      "Epoch: 1544 \tTraining Loss: 0.889463 \tValidation Loss: 0.902840\n",
      "Validation loss decreased (0.902876 --> 0.902840).         Saving model ...\n",
      "Epoch: 1545 \tTraining Loss: 0.889425 \tValidation Loss: 0.902804\n",
      "Validation loss decreased (0.902840 --> 0.902804).         Saving model ...\n",
      "Epoch: 1546 \tTraining Loss: 0.889386 \tValidation Loss: 0.902768\n",
      "Validation loss decreased (0.902804 --> 0.902768).         Saving model ...\n",
      "Epoch: 1547 \tTraining Loss: 0.889348 \tValidation Loss: 0.902732\n",
      "Validation loss decreased (0.902768 --> 0.902732).         Saving model ...\n",
      "Epoch: 1548 \tTraining Loss: 0.889310 \tValidation Loss: 0.902696\n",
      "Validation loss decreased (0.902732 --> 0.902696).         Saving model ...\n",
      "Epoch: 1549 \tTraining Loss: 0.889271 \tValidation Loss: 0.902661\n",
      "Validation loss decreased (0.902696 --> 0.902661).         Saving model ...\n",
      "Epoch: 1550 \tTraining Loss: 0.889233 \tValidation Loss: 0.902625\n",
      "Validation loss decreased (0.902661 --> 0.902625).         Saving model ...\n",
      "Epoch: 1551 \tTraining Loss: 0.889194 \tValidation Loss: 0.902589\n",
      "Validation loss decreased (0.902625 --> 0.902589).         Saving model ...\n",
      "Epoch: 1552 \tTraining Loss: 0.889156 \tValidation Loss: 0.902553\n",
      "Validation loss decreased (0.902589 --> 0.902553).         Saving model ...\n",
      "Epoch: 1553 \tTraining Loss: 0.889118 \tValidation Loss: 0.902518\n",
      "Validation loss decreased (0.902553 --> 0.902518).         Saving model ...\n",
      "Epoch: 1554 \tTraining Loss: 0.889080 \tValidation Loss: 0.902482\n",
      "Validation loss decreased (0.902518 --> 0.902482).         Saving model ...\n",
      "Epoch: 1555 \tTraining Loss: 0.889042 \tValidation Loss: 0.902446\n",
      "Validation loss decreased (0.902482 --> 0.902446).         Saving model ...\n",
      "Epoch: 1556 \tTraining Loss: 0.889003 \tValidation Loss: 0.902411\n",
      "Validation loss decreased (0.902446 --> 0.902411).         Saving model ...\n",
      "Epoch: 1557 \tTraining Loss: 0.888965 \tValidation Loss: 0.902375\n",
      "Validation loss decreased (0.902411 --> 0.902375).         Saving model ...\n",
      "Epoch: 1558 \tTraining Loss: 0.888927 \tValidation Loss: 0.902340\n",
      "Validation loss decreased (0.902375 --> 0.902340).         Saving model ...\n",
      "Epoch: 1559 \tTraining Loss: 0.888889 \tValidation Loss: 0.902304\n",
      "Validation loss decreased (0.902340 --> 0.902304).         Saving model ...\n",
      "Epoch: 1560 \tTraining Loss: 0.888851 \tValidation Loss: 0.902269\n",
      "Validation loss decreased (0.902304 --> 0.902269).         Saving model ...\n",
      "Epoch: 1561 \tTraining Loss: 0.888813 \tValidation Loss: 0.902233\n",
      "Validation loss decreased (0.902269 --> 0.902233).         Saving model ...\n",
      "Epoch: 1562 \tTraining Loss: 0.888775 \tValidation Loss: 0.902198\n",
      "Validation loss decreased (0.902233 --> 0.902198).         Saving model ...\n",
      "Epoch: 1563 \tTraining Loss: 0.888737 \tValidation Loss: 0.902163\n",
      "Validation loss decreased (0.902198 --> 0.902163).         Saving model ...\n",
      "Epoch: 1564 \tTraining Loss: 0.888699 \tValidation Loss: 0.902127\n",
      "Validation loss decreased (0.902163 --> 0.902127).         Saving model ...\n",
      "Epoch: 1565 \tTraining Loss: 0.888662 \tValidation Loss: 0.902092\n",
      "Validation loss decreased (0.902127 --> 0.902092).         Saving model ...\n",
      "Epoch: 1566 \tTraining Loss: 0.888624 \tValidation Loss: 0.902057\n",
      "Validation loss decreased (0.902092 --> 0.902057).         Saving model ...\n",
      "Epoch: 1567 \tTraining Loss: 0.888586 \tValidation Loss: 0.902022\n",
      "Validation loss decreased (0.902057 --> 0.902022).         Saving model ...\n",
      "Epoch: 1568 \tTraining Loss: 0.888548 \tValidation Loss: 0.901986\n",
      "Validation loss decreased (0.902022 --> 0.901986).         Saving model ...\n",
      "Epoch: 1569 \tTraining Loss: 0.888511 \tValidation Loss: 0.901951\n",
      "Validation loss decreased (0.901986 --> 0.901951).         Saving model ...\n",
      "Epoch: 1570 \tTraining Loss: 0.888473 \tValidation Loss: 0.901916\n",
      "Validation loss decreased (0.901951 --> 0.901916).         Saving model ...\n",
      "Epoch: 1571 \tTraining Loss: 0.888435 \tValidation Loss: 0.901881\n",
      "Validation loss decreased (0.901916 --> 0.901881).         Saving model ...\n",
      "Epoch: 1572 \tTraining Loss: 0.888398 \tValidation Loss: 0.901846\n",
      "Validation loss decreased (0.901881 --> 0.901846).         Saving model ...\n",
      "Epoch: 1573 \tTraining Loss: 0.888360 \tValidation Loss: 0.901811\n",
      "Validation loss decreased (0.901846 --> 0.901811).         Saving model ...\n",
      "Epoch: 1574 \tTraining Loss: 0.888323 \tValidation Loss: 0.901776\n",
      "Validation loss decreased (0.901811 --> 0.901776).         Saving model ...\n",
      "Epoch: 1575 \tTraining Loss: 0.888285 \tValidation Loss: 0.901741\n",
      "Validation loss decreased (0.901776 --> 0.901741).         Saving model ...\n",
      "Epoch: 1576 \tTraining Loss: 0.888248 \tValidation Loss: 0.901706\n",
      "Validation loss decreased (0.901741 --> 0.901706).         Saving model ...\n",
      "Epoch: 1577 \tTraining Loss: 0.888210 \tValidation Loss: 0.901671\n",
      "Validation loss decreased (0.901706 --> 0.901671).         Saving model ...\n",
      "Epoch: 1578 \tTraining Loss: 0.888173 \tValidation Loss: 0.901636\n",
      "Validation loss decreased (0.901671 --> 0.901636).         Saving model ...\n",
      "Epoch: 1579 \tTraining Loss: 0.888135 \tValidation Loss: 0.901602\n",
      "Validation loss decreased (0.901636 --> 0.901602).         Saving model ...\n",
      "Epoch: 1580 \tTraining Loss: 0.888098 \tValidation Loss: 0.901567\n",
      "Validation loss decreased (0.901602 --> 0.901567).         Saving model ...\n",
      "Epoch: 1581 \tTraining Loss: 0.888061 \tValidation Loss: 0.901532\n",
      "Validation loss decreased (0.901567 --> 0.901532).         Saving model ...\n",
      "Epoch: 1582 \tTraining Loss: 0.888023 \tValidation Loss: 0.901497\n",
      "Validation loss decreased (0.901532 --> 0.901497).         Saving model ...\n",
      "Epoch: 1583 \tTraining Loss: 0.887986 \tValidation Loss: 0.901462\n",
      "Validation loss decreased (0.901497 --> 0.901462).         Saving model ...\n",
      "Epoch: 1584 \tTraining Loss: 0.887949 \tValidation Loss: 0.901428\n",
      "Validation loss decreased (0.901462 --> 0.901428).         Saving model ...\n",
      "Epoch: 1585 \tTraining Loss: 0.887912 \tValidation Loss: 0.901393\n",
      "Validation loss decreased (0.901428 --> 0.901393).         Saving model ...\n",
      "Epoch: 1586 \tTraining Loss: 0.887875 \tValidation Loss: 0.901358\n",
      "Validation loss decreased (0.901393 --> 0.901358).         Saving model ...\n",
      "Epoch: 1587 \tTraining Loss: 0.887837 \tValidation Loss: 0.901324\n",
      "Validation loss decreased (0.901358 --> 0.901324).         Saving model ...\n",
      "Epoch: 1588 \tTraining Loss: 0.887800 \tValidation Loss: 0.901289\n",
      "Validation loss decreased (0.901324 --> 0.901289).         Saving model ...\n",
      "Epoch: 1589 \tTraining Loss: 0.887763 \tValidation Loss: 0.901255\n",
      "Validation loss decreased (0.901289 --> 0.901255).         Saving model ...\n",
      "Epoch: 1590 \tTraining Loss: 0.887726 \tValidation Loss: 0.901220\n",
      "Validation loss decreased (0.901255 --> 0.901220).         Saving model ...\n",
      "Epoch: 1591 \tTraining Loss: 0.887689 \tValidation Loss: 0.901186\n",
      "Validation loss decreased (0.901220 --> 0.901186).         Saving model ...\n",
      "Epoch: 1592 \tTraining Loss: 0.887652 \tValidation Loss: 0.901151\n",
      "Validation loss decreased (0.901186 --> 0.901151).         Saving model ...\n",
      "Epoch: 1593 \tTraining Loss: 0.887615 \tValidation Loss: 0.901117\n",
      "Validation loss decreased (0.901151 --> 0.901117).         Saving model ...\n",
      "Epoch: 1594 \tTraining Loss: 0.887579 \tValidation Loss: 0.901083\n",
      "Validation loss decreased (0.901117 --> 0.901083).         Saving model ...\n",
      "Epoch: 1595 \tTraining Loss: 0.887542 \tValidation Loss: 0.901048\n",
      "Validation loss decreased (0.901083 --> 0.901048).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1596 \tTraining Loss: 0.887505 \tValidation Loss: 0.901014\n",
      "Validation loss decreased (0.901048 --> 0.901014).         Saving model ...\n",
      "Epoch: 1597 \tTraining Loss: 0.887468 \tValidation Loss: 0.900980\n",
      "Validation loss decreased (0.901014 --> 0.900980).         Saving model ...\n",
      "Epoch: 1598 \tTraining Loss: 0.887431 \tValidation Loss: 0.900945\n",
      "Validation loss decreased (0.900980 --> 0.900945).         Saving model ...\n",
      "Epoch: 1599 \tTraining Loss: 0.887395 \tValidation Loss: 0.900911\n",
      "Validation loss decreased (0.900945 --> 0.900911).         Saving model ...\n",
      "Epoch: 1600 \tTraining Loss: 0.887358 \tValidation Loss: 0.900877\n",
      "Validation loss decreased (0.900911 --> 0.900877).         Saving model ...\n",
      "Epoch: 1601 \tTraining Loss: 0.887321 \tValidation Loss: 0.900843\n",
      "Validation loss decreased (0.900877 --> 0.900843).         Saving model ...\n",
      "Epoch: 1602 \tTraining Loss: 0.887285 \tValidation Loss: 0.900809\n",
      "Validation loss decreased (0.900843 --> 0.900809).         Saving model ...\n",
      "Epoch: 1603 \tTraining Loss: 0.887248 \tValidation Loss: 0.900775\n",
      "Validation loss decreased (0.900809 --> 0.900775).         Saving model ...\n",
      "Epoch: 1604 \tTraining Loss: 0.887211 \tValidation Loss: 0.900741\n",
      "Validation loss decreased (0.900775 --> 0.900741).         Saving model ...\n",
      "Epoch: 1605 \tTraining Loss: 0.887175 \tValidation Loss: 0.900707\n",
      "Validation loss decreased (0.900741 --> 0.900707).         Saving model ...\n",
      "Epoch: 1606 \tTraining Loss: 0.887138 \tValidation Loss: 0.900673\n",
      "Validation loss decreased (0.900707 --> 0.900673).         Saving model ...\n",
      "Epoch: 1607 \tTraining Loss: 0.887102 \tValidation Loss: 0.900639\n",
      "Validation loss decreased (0.900673 --> 0.900639).         Saving model ...\n",
      "Epoch: 1608 \tTraining Loss: 0.887065 \tValidation Loss: 0.900605\n",
      "Validation loss decreased (0.900639 --> 0.900605).         Saving model ...\n",
      "Epoch: 1609 \tTraining Loss: 0.887029 \tValidation Loss: 0.900571\n",
      "Validation loss decreased (0.900605 --> 0.900571).         Saving model ...\n",
      "Epoch: 1610 \tTraining Loss: 0.886993 \tValidation Loss: 0.900537\n",
      "Validation loss decreased (0.900571 --> 0.900537).         Saving model ...\n",
      "Epoch: 1611 \tTraining Loss: 0.886956 \tValidation Loss: 0.900503\n",
      "Validation loss decreased (0.900537 --> 0.900503).         Saving model ...\n",
      "Epoch: 1612 \tTraining Loss: 0.886920 \tValidation Loss: 0.900469\n",
      "Validation loss decreased (0.900503 --> 0.900469).         Saving model ...\n",
      "Epoch: 1613 \tTraining Loss: 0.886884 \tValidation Loss: 0.900435\n",
      "Validation loss decreased (0.900469 --> 0.900435).         Saving model ...\n",
      "Epoch: 1614 \tTraining Loss: 0.886847 \tValidation Loss: 0.900402\n",
      "Validation loss decreased (0.900435 --> 0.900402).         Saving model ...\n",
      "Epoch: 1615 \tTraining Loss: 0.886811 \tValidation Loss: 0.900368\n",
      "Validation loss decreased (0.900402 --> 0.900368).         Saving model ...\n",
      "Epoch: 1616 \tTraining Loss: 0.886775 \tValidation Loss: 0.900334\n",
      "Validation loss decreased (0.900368 --> 0.900334).         Saving model ...\n",
      "Epoch: 1617 \tTraining Loss: 0.886739 \tValidation Loss: 0.900301\n",
      "Validation loss decreased (0.900334 --> 0.900301).         Saving model ...\n",
      "Epoch: 1618 \tTraining Loss: 0.886703 \tValidation Loss: 0.900267\n",
      "Validation loss decreased (0.900301 --> 0.900267).         Saving model ...\n",
      "Epoch: 1619 \tTraining Loss: 0.886667 \tValidation Loss: 0.900233\n",
      "Validation loss decreased (0.900267 --> 0.900233).         Saving model ...\n",
      "Epoch: 1620 \tTraining Loss: 0.886630 \tValidation Loss: 0.900200\n",
      "Validation loss decreased (0.900233 --> 0.900200).         Saving model ...\n",
      "Epoch: 1621 \tTraining Loss: 0.886594 \tValidation Loss: 0.900166\n",
      "Validation loss decreased (0.900200 --> 0.900166).         Saving model ...\n",
      "Epoch: 1622 \tTraining Loss: 0.886558 \tValidation Loss: 0.900133\n",
      "Validation loss decreased (0.900166 --> 0.900133).         Saving model ...\n",
      "Epoch: 1623 \tTraining Loss: 0.886522 \tValidation Loss: 0.900099\n",
      "Validation loss decreased (0.900133 --> 0.900099).         Saving model ...\n",
      "Epoch: 1624 \tTraining Loss: 0.886486 \tValidation Loss: 0.900066\n",
      "Validation loss decreased (0.900099 --> 0.900066).         Saving model ...\n",
      "Epoch: 1625 \tTraining Loss: 0.886451 \tValidation Loss: 0.900032\n",
      "Validation loss decreased (0.900066 --> 0.900032).         Saving model ...\n",
      "Epoch: 1626 \tTraining Loss: 0.886415 \tValidation Loss: 0.899999\n",
      "Validation loss decreased (0.900032 --> 0.899999).         Saving model ...\n",
      "Epoch: 1627 \tTraining Loss: 0.886379 \tValidation Loss: 0.899966\n",
      "Validation loss decreased (0.899999 --> 0.899966).         Saving model ...\n",
      "Epoch: 1628 \tTraining Loss: 0.886343 \tValidation Loss: 0.899932\n",
      "Validation loss decreased (0.899966 --> 0.899932).         Saving model ...\n",
      "Epoch: 1629 \tTraining Loss: 0.886307 \tValidation Loss: 0.899899\n",
      "Validation loss decreased (0.899932 --> 0.899899).         Saving model ...\n",
      "Epoch: 1630 \tTraining Loss: 0.886271 \tValidation Loss: 0.899866\n",
      "Validation loss decreased (0.899899 --> 0.899866).         Saving model ...\n",
      "Epoch: 1631 \tTraining Loss: 0.886236 \tValidation Loss: 0.899832\n",
      "Validation loss decreased (0.899866 --> 0.899832).         Saving model ...\n",
      "Epoch: 1632 \tTraining Loss: 0.886200 \tValidation Loss: 0.899799\n",
      "Validation loss decreased (0.899832 --> 0.899799).         Saving model ...\n",
      "Epoch: 1633 \tTraining Loss: 0.886164 \tValidation Loss: 0.899766\n",
      "Validation loss decreased (0.899799 --> 0.899766).         Saving model ...\n",
      "Epoch: 1634 \tTraining Loss: 0.886129 \tValidation Loss: 0.899733\n",
      "Validation loss decreased (0.899766 --> 0.899733).         Saving model ...\n",
      "Epoch: 1635 \tTraining Loss: 0.886093 \tValidation Loss: 0.899700\n",
      "Validation loss decreased (0.899733 --> 0.899700).         Saving model ...\n",
      "Epoch: 1636 \tTraining Loss: 0.886057 \tValidation Loss: 0.899667\n",
      "Validation loss decreased (0.899700 --> 0.899667).         Saving model ...\n",
      "Epoch: 1637 \tTraining Loss: 0.886022 \tValidation Loss: 0.899633\n",
      "Validation loss decreased (0.899667 --> 0.899633).         Saving model ...\n",
      "Epoch: 1638 \tTraining Loss: 0.885986 \tValidation Loss: 0.899600\n",
      "Validation loss decreased (0.899633 --> 0.899600).         Saving model ...\n",
      "Epoch: 1639 \tTraining Loss: 0.885951 \tValidation Loss: 0.899567\n",
      "Validation loss decreased (0.899600 --> 0.899567).         Saving model ...\n",
      "Epoch: 1640 \tTraining Loss: 0.885915 \tValidation Loss: 0.899534\n",
      "Validation loss decreased (0.899567 --> 0.899534).         Saving model ...\n",
      "Epoch: 1641 \tTraining Loss: 0.885880 \tValidation Loss: 0.899501\n",
      "Validation loss decreased (0.899534 --> 0.899501).         Saving model ...\n",
      "Epoch: 1642 \tTraining Loss: 0.885845 \tValidation Loss: 0.899468\n",
      "Validation loss decreased (0.899501 --> 0.899468).         Saving model ...\n",
      "Epoch: 1643 \tTraining Loss: 0.885809 \tValidation Loss: 0.899436\n",
      "Validation loss decreased (0.899468 --> 0.899436).         Saving model ...\n",
      "Epoch: 1644 \tTraining Loss: 0.885774 \tValidation Loss: 0.899403\n",
      "Validation loss decreased (0.899436 --> 0.899403).         Saving model ...\n",
      "Epoch: 1645 \tTraining Loss: 0.885739 \tValidation Loss: 0.899370\n",
      "Validation loss decreased (0.899403 --> 0.899370).         Saving model ...\n",
      "Epoch: 1646 \tTraining Loss: 0.885703 \tValidation Loss: 0.899337\n",
      "Validation loss decreased (0.899370 --> 0.899337).         Saving model ...\n",
      "Epoch: 1647 \tTraining Loss: 0.885668 \tValidation Loss: 0.899304\n",
      "Validation loss decreased (0.899337 --> 0.899304).         Saving model ...\n",
      "Epoch: 1648 \tTraining Loss: 0.885633 \tValidation Loss: 0.899271\n",
      "Validation loss decreased (0.899304 --> 0.899271).         Saving model ...\n",
      "Epoch: 1649 \tTraining Loss: 0.885598 \tValidation Loss: 0.899239\n",
      "Validation loss decreased (0.899271 --> 0.899239).         Saving model ...\n",
      "Epoch: 1650 \tTraining Loss: 0.885562 \tValidation Loss: 0.899206\n",
      "Validation loss decreased (0.899239 --> 0.899206).         Saving model ...\n",
      "Epoch: 1651 \tTraining Loss: 0.885527 \tValidation Loss: 0.899173\n",
      "Validation loss decreased (0.899206 --> 0.899173).         Saving model ...\n",
      "Epoch: 1652 \tTraining Loss: 0.885492 \tValidation Loss: 0.899141\n",
      "Validation loss decreased (0.899173 --> 0.899141).         Saving model ...\n",
      "Epoch: 1653 \tTraining Loss: 0.885457 \tValidation Loss: 0.899108\n",
      "Validation loss decreased (0.899141 --> 0.899108).         Saving model ...\n",
      "Epoch: 1654 \tTraining Loss: 0.885422 \tValidation Loss: 0.899075\n",
      "Validation loss decreased (0.899108 --> 0.899075).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1655 \tTraining Loss: 0.885387 \tValidation Loss: 0.899043\n",
      "Validation loss decreased (0.899075 --> 0.899043).         Saving model ...\n",
      "Epoch: 1656 \tTraining Loss: 0.885352 \tValidation Loss: 0.899010\n",
      "Validation loss decreased (0.899043 --> 0.899010).         Saving model ...\n",
      "Epoch: 1657 \tTraining Loss: 0.885317 \tValidation Loss: 0.898978\n",
      "Validation loss decreased (0.899010 --> 0.898978).         Saving model ...\n",
      "Epoch: 1658 \tTraining Loss: 0.885282 \tValidation Loss: 0.898945\n",
      "Validation loss decreased (0.898978 --> 0.898945).         Saving model ...\n",
      "Epoch: 1659 \tTraining Loss: 0.885247 \tValidation Loss: 0.898913\n",
      "Validation loss decreased (0.898945 --> 0.898913).         Saving model ...\n",
      "Epoch: 1660 \tTraining Loss: 0.885212 \tValidation Loss: 0.898880\n",
      "Validation loss decreased (0.898913 --> 0.898880).         Saving model ...\n",
      "Epoch: 1661 \tTraining Loss: 0.885178 \tValidation Loss: 0.898848\n",
      "Validation loss decreased (0.898880 --> 0.898848).         Saving model ...\n",
      "Epoch: 1662 \tTraining Loss: 0.885143 \tValidation Loss: 0.898816\n",
      "Validation loss decreased (0.898848 --> 0.898816).         Saving model ...\n",
      "Epoch: 1663 \tTraining Loss: 0.885108 \tValidation Loss: 0.898783\n",
      "Validation loss decreased (0.898816 --> 0.898783).         Saving model ...\n",
      "Epoch: 1664 \tTraining Loss: 0.885073 \tValidation Loss: 0.898751\n",
      "Validation loss decreased (0.898783 --> 0.898751).         Saving model ...\n",
      "Epoch: 1665 \tTraining Loss: 0.885038 \tValidation Loss: 0.898719\n",
      "Validation loss decreased (0.898751 --> 0.898719).         Saving model ...\n",
      "Epoch: 1666 \tTraining Loss: 0.885004 \tValidation Loss: 0.898686\n",
      "Validation loss decreased (0.898719 --> 0.898686).         Saving model ...\n",
      "Epoch: 1667 \tTraining Loss: 0.884969 \tValidation Loss: 0.898654\n",
      "Validation loss decreased (0.898686 --> 0.898654).         Saving model ...\n",
      "Epoch: 1668 \tTraining Loss: 0.884934 \tValidation Loss: 0.898622\n",
      "Validation loss decreased (0.898654 --> 0.898622).         Saving model ...\n",
      "Epoch: 1669 \tTraining Loss: 0.884900 \tValidation Loss: 0.898590\n",
      "Validation loss decreased (0.898622 --> 0.898590).         Saving model ...\n",
      "Epoch: 1670 \tTraining Loss: 0.884865 \tValidation Loss: 0.898558\n",
      "Validation loss decreased (0.898590 --> 0.898558).         Saving model ...\n",
      "Epoch: 1671 \tTraining Loss: 0.884831 \tValidation Loss: 0.898526\n",
      "Validation loss decreased (0.898558 --> 0.898526).         Saving model ...\n",
      "Epoch: 1672 \tTraining Loss: 0.884796 \tValidation Loss: 0.898493\n",
      "Validation loss decreased (0.898526 --> 0.898493).         Saving model ...\n",
      "Epoch: 1673 \tTraining Loss: 0.884762 \tValidation Loss: 0.898461\n",
      "Validation loss decreased (0.898493 --> 0.898461).         Saving model ...\n",
      "Epoch: 1674 \tTraining Loss: 0.884727 \tValidation Loss: 0.898429\n",
      "Validation loss decreased (0.898461 --> 0.898429).         Saving model ...\n",
      "Epoch: 1675 \tTraining Loss: 0.884693 \tValidation Loss: 0.898397\n",
      "Validation loss decreased (0.898429 --> 0.898397).         Saving model ...\n",
      "Epoch: 1676 \tTraining Loss: 0.884658 \tValidation Loss: 0.898365\n",
      "Validation loss decreased (0.898397 --> 0.898365).         Saving model ...\n",
      "Epoch: 1677 \tTraining Loss: 0.884624 \tValidation Loss: 0.898333\n",
      "Validation loss decreased (0.898365 --> 0.898333).         Saving model ...\n",
      "Epoch: 1678 \tTraining Loss: 0.884590 \tValidation Loss: 0.898302\n",
      "Validation loss decreased (0.898333 --> 0.898302).         Saving model ...\n",
      "Epoch: 1679 \tTraining Loss: 0.884555 \tValidation Loss: 0.898270\n",
      "Validation loss decreased (0.898302 --> 0.898270).         Saving model ...\n",
      "Epoch: 1680 \tTraining Loss: 0.884521 \tValidation Loss: 0.898238\n",
      "Validation loss decreased (0.898270 --> 0.898238).         Saving model ...\n",
      "Epoch: 1681 \tTraining Loss: 0.884487 \tValidation Loss: 0.898206\n",
      "Validation loss decreased (0.898238 --> 0.898206).         Saving model ...\n",
      "Epoch: 1682 \tTraining Loss: 0.884453 \tValidation Loss: 0.898174\n",
      "Validation loss decreased (0.898206 --> 0.898174).         Saving model ...\n",
      "Epoch: 1683 \tTraining Loss: 0.884418 \tValidation Loss: 0.898142\n",
      "Validation loss decreased (0.898174 --> 0.898142).         Saving model ...\n",
      "Epoch: 1684 \tTraining Loss: 0.884384 \tValidation Loss: 0.898111\n",
      "Validation loss decreased (0.898142 --> 0.898111).         Saving model ...\n",
      "Epoch: 1685 \tTraining Loss: 0.884350 \tValidation Loss: 0.898079\n",
      "Validation loss decreased (0.898111 --> 0.898079).         Saving model ...\n",
      "Epoch: 1686 \tTraining Loss: 0.884316 \tValidation Loss: 0.898047\n",
      "Validation loss decreased (0.898079 --> 0.898047).         Saving model ...\n",
      "Epoch: 1687 \tTraining Loss: 0.884282 \tValidation Loss: 0.898015\n",
      "Validation loss decreased (0.898047 --> 0.898015).         Saving model ...\n",
      "Epoch: 1688 \tTraining Loss: 0.884248 \tValidation Loss: 0.897984\n",
      "Validation loss decreased (0.898015 --> 0.897984).         Saving model ...\n",
      "Epoch: 1689 \tTraining Loss: 0.884214 \tValidation Loss: 0.897952\n",
      "Validation loss decreased (0.897984 --> 0.897952).         Saving model ...\n",
      "Epoch: 1690 \tTraining Loss: 0.884180 \tValidation Loss: 0.897921\n",
      "Validation loss decreased (0.897952 --> 0.897921).         Saving model ...\n",
      "Epoch: 1691 \tTraining Loss: 0.884146 \tValidation Loss: 0.897889\n",
      "Validation loss decreased (0.897921 --> 0.897889).         Saving model ...\n",
      "Epoch: 1692 \tTraining Loss: 0.884112 \tValidation Loss: 0.897857\n",
      "Validation loss decreased (0.897889 --> 0.897857).         Saving model ...\n",
      "Epoch: 1693 \tTraining Loss: 0.884078 \tValidation Loss: 0.897826\n",
      "Validation loss decreased (0.897857 --> 0.897826).         Saving model ...\n",
      "Epoch: 1694 \tTraining Loss: 0.884044 \tValidation Loss: 0.897794\n",
      "Validation loss decreased (0.897826 --> 0.897794).         Saving model ...\n",
      "Epoch: 1695 \tTraining Loss: 0.884010 \tValidation Loss: 0.897763\n",
      "Validation loss decreased (0.897794 --> 0.897763).         Saving model ...\n",
      "Epoch: 1696 \tTraining Loss: 0.883976 \tValidation Loss: 0.897732\n",
      "Validation loss decreased (0.897763 --> 0.897732).         Saving model ...\n",
      "Epoch: 1697 \tTraining Loss: 0.883942 \tValidation Loss: 0.897700\n",
      "Validation loss decreased (0.897732 --> 0.897700).         Saving model ...\n",
      "Epoch: 1698 \tTraining Loss: 0.883909 \tValidation Loss: 0.897669\n",
      "Validation loss decreased (0.897700 --> 0.897669).         Saving model ...\n",
      "Epoch: 1699 \tTraining Loss: 0.883875 \tValidation Loss: 0.897637\n",
      "Validation loss decreased (0.897669 --> 0.897637).         Saving model ...\n",
      "Epoch: 1700 \tTraining Loss: 0.883841 \tValidation Loss: 0.897606\n",
      "Validation loss decreased (0.897637 --> 0.897606).         Saving model ...\n",
      "Epoch: 1701 \tTraining Loss: 0.883807 \tValidation Loss: 0.897575\n",
      "Validation loss decreased (0.897606 --> 0.897575).         Saving model ...\n",
      "Epoch: 1702 \tTraining Loss: 0.883774 \tValidation Loss: 0.897543\n",
      "Validation loss decreased (0.897575 --> 0.897543).         Saving model ...\n",
      "Epoch: 1703 \tTraining Loss: 0.883740 \tValidation Loss: 0.897512\n",
      "Validation loss decreased (0.897543 --> 0.897512).         Saving model ...\n",
      "Epoch: 1704 \tTraining Loss: 0.883706 \tValidation Loss: 0.897481\n",
      "Validation loss decreased (0.897512 --> 0.897481).         Saving model ...\n",
      "Epoch: 1705 \tTraining Loss: 0.883673 \tValidation Loss: 0.897450\n",
      "Validation loss decreased (0.897481 --> 0.897450).         Saving model ...\n",
      "Epoch: 1706 \tTraining Loss: 0.883639 \tValidation Loss: 0.897419\n",
      "Validation loss decreased (0.897450 --> 0.897419).         Saving model ...\n",
      "Epoch: 1707 \tTraining Loss: 0.883606 \tValidation Loss: 0.897387\n",
      "Validation loss decreased (0.897419 --> 0.897387).         Saving model ...\n",
      "Epoch: 1708 \tTraining Loss: 0.883572 \tValidation Loss: 0.897356\n",
      "Validation loss decreased (0.897387 --> 0.897356).         Saving model ...\n",
      "Epoch: 1709 \tTraining Loss: 0.883539 \tValidation Loss: 0.897325\n",
      "Validation loss decreased (0.897356 --> 0.897325).         Saving model ...\n",
      "Epoch: 1710 \tTraining Loss: 0.883505 \tValidation Loss: 0.897294\n",
      "Validation loss decreased (0.897325 --> 0.897294).         Saving model ...\n",
      "Epoch: 1711 \tTraining Loss: 0.883472 \tValidation Loss: 0.897263\n",
      "Validation loss decreased (0.897294 --> 0.897263).         Saving model ...\n",
      "Epoch: 1712 \tTraining Loss: 0.883438 \tValidation Loss: 0.897232\n",
      "Validation loss decreased (0.897263 --> 0.897232).         Saving model ...\n",
      "Epoch: 1713 \tTraining Loss: 0.883405 \tValidation Loss: 0.897201\n",
      "Validation loss decreased (0.897232 --> 0.897201).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1714 \tTraining Loss: 0.883372 \tValidation Loss: 0.897170\n",
      "Validation loss decreased (0.897201 --> 0.897170).         Saving model ...\n",
      "Epoch: 1715 \tTraining Loss: 0.883338 \tValidation Loss: 0.897139\n",
      "Validation loss decreased (0.897170 --> 0.897139).         Saving model ...\n",
      "Epoch: 1716 \tTraining Loss: 0.883305 \tValidation Loss: 0.897108\n",
      "Validation loss decreased (0.897139 --> 0.897108).         Saving model ...\n",
      "Epoch: 1717 \tTraining Loss: 0.883272 \tValidation Loss: 0.897077\n",
      "Validation loss decreased (0.897108 --> 0.897077).         Saving model ...\n",
      "Epoch: 1718 \tTraining Loss: 0.883238 \tValidation Loss: 0.897047\n",
      "Validation loss decreased (0.897077 --> 0.897047).         Saving model ...\n",
      "Epoch: 1719 \tTraining Loss: 0.883205 \tValidation Loss: 0.897016\n",
      "Validation loss decreased (0.897047 --> 0.897016).         Saving model ...\n",
      "Epoch: 1720 \tTraining Loss: 0.883172 \tValidation Loss: 0.896985\n",
      "Validation loss decreased (0.897016 --> 0.896985).         Saving model ...\n",
      "Epoch: 1721 \tTraining Loss: 0.883139 \tValidation Loss: 0.896954\n",
      "Validation loss decreased (0.896985 --> 0.896954).         Saving model ...\n",
      "Epoch: 1722 \tTraining Loss: 0.883106 \tValidation Loss: 0.896923\n",
      "Validation loss decreased (0.896954 --> 0.896923).         Saving model ...\n",
      "Epoch: 1723 \tTraining Loss: 0.883073 \tValidation Loss: 0.896893\n",
      "Validation loss decreased (0.896923 --> 0.896893).         Saving model ...\n",
      "Epoch: 1724 \tTraining Loss: 0.883039 \tValidation Loss: 0.896862\n",
      "Validation loss decreased (0.896893 --> 0.896862).         Saving model ...\n",
      "Epoch: 1725 \tTraining Loss: 0.883006 \tValidation Loss: 0.896831\n",
      "Validation loss decreased (0.896862 --> 0.896831).         Saving model ...\n",
      "Epoch: 1726 \tTraining Loss: 0.882973 \tValidation Loss: 0.896801\n",
      "Validation loss decreased (0.896831 --> 0.896801).         Saving model ...\n",
      "Epoch: 1727 \tTraining Loss: 0.882940 \tValidation Loss: 0.896770\n",
      "Validation loss decreased (0.896801 --> 0.896770).         Saving model ...\n",
      "Epoch: 1728 \tTraining Loss: 0.882907 \tValidation Loss: 0.896739\n",
      "Validation loss decreased (0.896770 --> 0.896739).         Saving model ...\n",
      "Epoch: 1729 \tTraining Loss: 0.882874 \tValidation Loss: 0.896709\n",
      "Validation loss decreased (0.896739 --> 0.896709).         Saving model ...\n",
      "Epoch: 1730 \tTraining Loss: 0.882842 \tValidation Loss: 0.896678\n",
      "Validation loss decreased (0.896709 --> 0.896678).         Saving model ...\n",
      "Epoch: 1731 \tTraining Loss: 0.882809 \tValidation Loss: 0.896648\n",
      "Validation loss decreased (0.896678 --> 0.896648).         Saving model ...\n",
      "Epoch: 1732 \tTraining Loss: 0.882776 \tValidation Loss: 0.896617\n",
      "Validation loss decreased (0.896648 --> 0.896617).         Saving model ...\n",
      "Epoch: 1733 \tTraining Loss: 0.882743 \tValidation Loss: 0.896587\n",
      "Validation loss decreased (0.896617 --> 0.896587).         Saving model ...\n",
      "Epoch: 1734 \tTraining Loss: 0.882710 \tValidation Loss: 0.896556\n",
      "Validation loss decreased (0.896587 --> 0.896556).         Saving model ...\n",
      "Epoch: 1735 \tTraining Loss: 0.882677 \tValidation Loss: 0.896526\n",
      "Validation loss decreased (0.896556 --> 0.896526).         Saving model ...\n",
      "Epoch: 1736 \tTraining Loss: 0.882644 \tValidation Loss: 0.896495\n",
      "Validation loss decreased (0.896526 --> 0.896495).         Saving model ...\n",
      "Epoch: 1737 \tTraining Loss: 0.882612 \tValidation Loss: 0.896465\n",
      "Validation loss decreased (0.896495 --> 0.896465).         Saving model ...\n",
      "Epoch: 1738 \tTraining Loss: 0.882579 \tValidation Loss: 0.896435\n",
      "Validation loss decreased (0.896465 --> 0.896435).         Saving model ...\n",
      "Epoch: 1739 \tTraining Loss: 0.882546 \tValidation Loss: 0.896404\n",
      "Validation loss decreased (0.896435 --> 0.896404).         Saving model ...\n",
      "Epoch: 1740 \tTraining Loss: 0.882514 \tValidation Loss: 0.896374\n",
      "Validation loss decreased (0.896404 --> 0.896374).         Saving model ...\n",
      "Epoch: 1741 \tTraining Loss: 0.882481 \tValidation Loss: 0.896344\n",
      "Validation loss decreased (0.896374 --> 0.896344).         Saving model ...\n",
      "Epoch: 1742 \tTraining Loss: 0.882448 \tValidation Loss: 0.896314\n",
      "Validation loss decreased (0.896344 --> 0.896314).         Saving model ...\n",
      "Epoch: 1743 \tTraining Loss: 0.882416 \tValidation Loss: 0.896283\n",
      "Validation loss decreased (0.896314 --> 0.896283).         Saving model ...\n",
      "Epoch: 1744 \tTraining Loss: 0.882383 \tValidation Loss: 0.896253\n",
      "Validation loss decreased (0.896283 --> 0.896253).         Saving model ...\n",
      "Epoch: 1745 \tTraining Loss: 0.882351 \tValidation Loss: 0.896223\n",
      "Validation loss decreased (0.896253 --> 0.896223).         Saving model ...\n",
      "Epoch: 1746 \tTraining Loss: 0.882318 \tValidation Loss: 0.896193\n",
      "Validation loss decreased (0.896223 --> 0.896193).         Saving model ...\n",
      "Epoch: 1747 \tTraining Loss: 0.882286 \tValidation Loss: 0.896163\n",
      "Validation loss decreased (0.896193 --> 0.896163).         Saving model ...\n",
      "Epoch: 1748 \tTraining Loss: 0.882253 \tValidation Loss: 0.896133\n",
      "Validation loss decreased (0.896163 --> 0.896133).         Saving model ...\n",
      "Epoch: 1749 \tTraining Loss: 0.882221 \tValidation Loss: 0.896103\n",
      "Validation loss decreased (0.896133 --> 0.896103).         Saving model ...\n",
      "Epoch: 1750 \tTraining Loss: 0.882188 \tValidation Loss: 0.896073\n",
      "Validation loss decreased (0.896103 --> 0.896073).         Saving model ...\n",
      "Epoch: 1751 \tTraining Loss: 0.882156 \tValidation Loss: 0.896043\n",
      "Validation loss decreased (0.896073 --> 0.896043).         Saving model ...\n",
      "Epoch: 1752 \tTraining Loss: 0.882124 \tValidation Loss: 0.896013\n",
      "Validation loss decreased (0.896043 --> 0.896013).         Saving model ...\n",
      "Epoch: 1753 \tTraining Loss: 0.882091 \tValidation Loss: 0.895983\n",
      "Validation loss decreased (0.896013 --> 0.895983).         Saving model ...\n",
      "Epoch: 1754 \tTraining Loss: 0.882059 \tValidation Loss: 0.895953\n",
      "Validation loss decreased (0.895983 --> 0.895953).         Saving model ...\n",
      "Epoch: 1755 \tTraining Loss: 0.882027 \tValidation Loss: 0.895923\n",
      "Validation loss decreased (0.895953 --> 0.895923).         Saving model ...\n",
      "Epoch: 1756 \tTraining Loss: 0.881995 \tValidation Loss: 0.895893\n",
      "Validation loss decreased (0.895923 --> 0.895893).         Saving model ...\n",
      "Epoch: 1757 \tTraining Loss: 0.881962 \tValidation Loss: 0.895863\n",
      "Validation loss decreased (0.895893 --> 0.895863).         Saving model ...\n",
      "Epoch: 1758 \tTraining Loss: 0.881930 \tValidation Loss: 0.895833\n",
      "Validation loss decreased (0.895863 --> 0.895833).         Saving model ...\n",
      "Epoch: 1759 \tTraining Loss: 0.881898 \tValidation Loss: 0.895803\n",
      "Validation loss decreased (0.895833 --> 0.895803).         Saving model ...\n",
      "Epoch: 1760 \tTraining Loss: 0.881866 \tValidation Loss: 0.895773\n",
      "Validation loss decreased (0.895803 --> 0.895773).         Saving model ...\n",
      "Epoch: 1761 \tTraining Loss: 0.881834 \tValidation Loss: 0.895744\n",
      "Validation loss decreased (0.895773 --> 0.895744).         Saving model ...\n",
      "Epoch: 1762 \tTraining Loss: 0.881802 \tValidation Loss: 0.895714\n",
      "Validation loss decreased (0.895744 --> 0.895714).         Saving model ...\n",
      "Epoch: 1763 \tTraining Loss: 0.881769 \tValidation Loss: 0.895684\n",
      "Validation loss decreased (0.895714 --> 0.895684).         Saving model ...\n",
      "Epoch: 1764 \tTraining Loss: 0.881737 \tValidation Loss: 0.895654\n",
      "Validation loss decreased (0.895684 --> 0.895654).         Saving model ...\n",
      "Epoch: 1765 \tTraining Loss: 0.881705 \tValidation Loss: 0.895625\n",
      "Validation loss decreased (0.895654 --> 0.895625).         Saving model ...\n",
      "Epoch: 1766 \tTraining Loss: 0.881673 \tValidation Loss: 0.895595\n",
      "Validation loss decreased (0.895625 --> 0.895595).         Saving model ...\n",
      "Epoch: 1767 \tTraining Loss: 0.881641 \tValidation Loss: 0.895565\n",
      "Validation loss decreased (0.895595 --> 0.895565).         Saving model ...\n",
      "Epoch: 1768 \tTraining Loss: 0.881609 \tValidation Loss: 0.895536\n",
      "Validation loss decreased (0.895565 --> 0.895536).         Saving model ...\n",
      "Epoch: 1769 \tTraining Loss: 0.881578 \tValidation Loss: 0.895506\n",
      "Validation loss decreased (0.895536 --> 0.895506).         Saving model ...\n",
      "Epoch: 1770 \tTraining Loss: 0.881546 \tValidation Loss: 0.895477\n",
      "Validation loss decreased (0.895506 --> 0.895477).         Saving model ...\n",
      "Epoch: 1771 \tTraining Loss: 0.881514 \tValidation Loss: 0.895447\n",
      "Validation loss decreased (0.895477 --> 0.895447).         Saving model ...\n",
      "Epoch: 1772 \tTraining Loss: 0.881482 \tValidation Loss: 0.895418\n",
      "Validation loss decreased (0.895447 --> 0.895418).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1773 \tTraining Loss: 0.881450 \tValidation Loss: 0.895388\n",
      "Validation loss decreased (0.895418 --> 0.895388).         Saving model ...\n",
      "Epoch: 1774 \tTraining Loss: 0.881418 \tValidation Loss: 0.895359\n",
      "Validation loss decreased (0.895388 --> 0.895359).         Saving model ...\n",
      "Epoch: 1775 \tTraining Loss: 0.881387 \tValidation Loss: 0.895329\n",
      "Validation loss decreased (0.895359 --> 0.895329).         Saving model ...\n",
      "Epoch: 1776 \tTraining Loss: 0.881355 \tValidation Loss: 0.895300\n",
      "Validation loss decreased (0.895329 --> 0.895300).         Saving model ...\n",
      "Epoch: 1777 \tTraining Loss: 0.881323 \tValidation Loss: 0.895271\n",
      "Validation loss decreased (0.895300 --> 0.895271).         Saving model ...\n",
      "Epoch: 1778 \tTraining Loss: 0.881291 \tValidation Loss: 0.895241\n",
      "Validation loss decreased (0.895271 --> 0.895241).         Saving model ...\n",
      "Epoch: 1779 \tTraining Loss: 0.881260 \tValidation Loss: 0.895212\n",
      "Validation loss decreased (0.895241 --> 0.895212).         Saving model ...\n",
      "Epoch: 1780 \tTraining Loss: 0.881228 \tValidation Loss: 0.895182\n",
      "Validation loss decreased (0.895212 --> 0.895182).         Saving model ...\n",
      "Epoch: 1781 \tTraining Loss: 0.881196 \tValidation Loss: 0.895153\n",
      "Validation loss decreased (0.895182 --> 0.895153).         Saving model ...\n",
      "Epoch: 1782 \tTraining Loss: 0.881165 \tValidation Loss: 0.895124\n",
      "Validation loss decreased (0.895153 --> 0.895124).         Saving model ...\n",
      "Epoch: 1783 \tTraining Loss: 0.881133 \tValidation Loss: 0.895095\n",
      "Validation loss decreased (0.895124 --> 0.895095).         Saving model ...\n",
      "Epoch: 1784 \tTraining Loss: 0.881102 \tValidation Loss: 0.895065\n",
      "Validation loss decreased (0.895095 --> 0.895065).         Saving model ...\n",
      "Epoch: 1785 \tTraining Loss: 0.881070 \tValidation Loss: 0.895036\n",
      "Validation loss decreased (0.895065 --> 0.895036).         Saving model ...\n",
      "Epoch: 1786 \tTraining Loss: 0.881039 \tValidation Loss: 0.895007\n",
      "Validation loss decreased (0.895036 --> 0.895007).         Saving model ...\n",
      "Epoch: 1787 \tTraining Loss: 0.881007 \tValidation Loss: 0.894978\n",
      "Validation loss decreased (0.895007 --> 0.894978).         Saving model ...\n",
      "Epoch: 1788 \tTraining Loss: 0.880976 \tValidation Loss: 0.894949\n",
      "Validation loss decreased (0.894978 --> 0.894949).         Saving model ...\n",
      "Epoch: 1789 \tTraining Loss: 0.880944 \tValidation Loss: 0.894920\n",
      "Validation loss decreased (0.894949 --> 0.894920).         Saving model ...\n",
      "Epoch: 1790 \tTraining Loss: 0.880913 \tValidation Loss: 0.894891\n",
      "Validation loss decreased (0.894920 --> 0.894891).         Saving model ...\n",
      "Epoch: 1791 \tTraining Loss: 0.880881 \tValidation Loss: 0.894862\n",
      "Validation loss decreased (0.894891 --> 0.894862).         Saving model ...\n",
      "Epoch: 1792 \tTraining Loss: 0.880850 \tValidation Loss: 0.894833\n",
      "Validation loss decreased (0.894862 --> 0.894833).         Saving model ...\n",
      "Epoch: 1793 \tTraining Loss: 0.880819 \tValidation Loss: 0.894804\n",
      "Validation loss decreased (0.894833 --> 0.894804).         Saving model ...\n",
      "Epoch: 1794 \tTraining Loss: 0.880787 \tValidation Loss: 0.894775\n",
      "Validation loss decreased (0.894804 --> 0.894775).         Saving model ...\n",
      "Epoch: 1795 \tTraining Loss: 0.880756 \tValidation Loss: 0.894746\n",
      "Validation loss decreased (0.894775 --> 0.894746).         Saving model ...\n",
      "Epoch: 1796 \tTraining Loss: 0.880725 \tValidation Loss: 0.894717\n",
      "Validation loss decreased (0.894746 --> 0.894717).         Saving model ...\n",
      "Epoch: 1797 \tTraining Loss: 0.880694 \tValidation Loss: 0.894688\n",
      "Validation loss decreased (0.894717 --> 0.894688).         Saving model ...\n",
      "Epoch: 1798 \tTraining Loss: 0.880662 \tValidation Loss: 0.894659\n",
      "Validation loss decreased (0.894688 --> 0.894659).         Saving model ...\n",
      "Epoch: 1799 \tTraining Loss: 0.880631 \tValidation Loss: 0.894630\n",
      "Validation loss decreased (0.894659 --> 0.894630).         Saving model ...\n",
      "Epoch: 1800 \tTraining Loss: 0.880600 \tValidation Loss: 0.894601\n",
      "Validation loss decreased (0.894630 --> 0.894601).         Saving model ...\n",
      "Epoch: 1801 \tTraining Loss: 0.880569 \tValidation Loss: 0.894572\n",
      "Validation loss decreased (0.894601 --> 0.894572).         Saving model ...\n",
      "Epoch: 1802 \tTraining Loss: 0.880538 \tValidation Loss: 0.894544\n",
      "Validation loss decreased (0.894572 --> 0.894544).         Saving model ...\n",
      "Epoch: 1803 \tTraining Loss: 0.880507 \tValidation Loss: 0.894515\n",
      "Validation loss decreased (0.894544 --> 0.894515).         Saving model ...\n",
      "Epoch: 1804 \tTraining Loss: 0.880476 \tValidation Loss: 0.894486\n",
      "Validation loss decreased (0.894515 --> 0.894486).         Saving model ...\n",
      "Epoch: 1805 \tTraining Loss: 0.880445 \tValidation Loss: 0.894457\n",
      "Validation loss decreased (0.894486 --> 0.894457).         Saving model ...\n",
      "Epoch: 1806 \tTraining Loss: 0.880414 \tValidation Loss: 0.894429\n",
      "Validation loss decreased (0.894457 --> 0.894429).         Saving model ...\n",
      "Epoch: 1807 \tTraining Loss: 0.880383 \tValidation Loss: 0.894400\n",
      "Validation loss decreased (0.894429 --> 0.894400).         Saving model ...\n",
      "Epoch: 1808 \tTraining Loss: 0.880352 \tValidation Loss: 0.894371\n",
      "Validation loss decreased (0.894400 --> 0.894371).         Saving model ...\n",
      "Epoch: 1809 \tTraining Loss: 0.880321 \tValidation Loss: 0.894343\n",
      "Validation loss decreased (0.894371 --> 0.894343).         Saving model ...\n",
      "Epoch: 1810 \tTraining Loss: 0.880290 \tValidation Loss: 0.894314\n",
      "Validation loss decreased (0.894343 --> 0.894314).         Saving model ...\n",
      "Epoch: 1811 \tTraining Loss: 0.880259 \tValidation Loss: 0.894285\n",
      "Validation loss decreased (0.894314 --> 0.894285).         Saving model ...\n",
      "Epoch: 1812 \tTraining Loss: 0.880228 \tValidation Loss: 0.894257\n",
      "Validation loss decreased (0.894285 --> 0.894257).         Saving model ...\n",
      "Epoch: 1813 \tTraining Loss: 0.880197 \tValidation Loss: 0.894228\n",
      "Validation loss decreased (0.894257 --> 0.894228).         Saving model ...\n",
      "Epoch: 1814 \tTraining Loss: 0.880166 \tValidation Loss: 0.894200\n",
      "Validation loss decreased (0.894228 --> 0.894200).         Saving model ...\n",
      "Epoch: 1815 \tTraining Loss: 0.880136 \tValidation Loss: 0.894171\n",
      "Validation loss decreased (0.894200 --> 0.894171).         Saving model ...\n",
      "Epoch: 1816 \tTraining Loss: 0.880105 \tValidation Loss: 0.894143\n",
      "Validation loss decreased (0.894171 --> 0.894143).         Saving model ...\n",
      "Epoch: 1817 \tTraining Loss: 0.880074 \tValidation Loss: 0.894114\n",
      "Validation loss decreased (0.894143 --> 0.894114).         Saving model ...\n",
      "Epoch: 1818 \tTraining Loss: 0.880043 \tValidation Loss: 0.894086\n",
      "Validation loss decreased (0.894114 --> 0.894086).         Saving model ...\n",
      "Epoch: 1819 \tTraining Loss: 0.880013 \tValidation Loss: 0.894057\n",
      "Validation loss decreased (0.894086 --> 0.894057).         Saving model ...\n",
      "Epoch: 1820 \tTraining Loss: 0.879982 \tValidation Loss: 0.894029\n",
      "Validation loss decreased (0.894057 --> 0.894029).         Saving model ...\n",
      "Epoch: 1821 \tTraining Loss: 0.879951 \tValidation Loss: 0.894001\n",
      "Validation loss decreased (0.894029 --> 0.894001).         Saving model ...\n",
      "Epoch: 1822 \tTraining Loss: 0.879921 \tValidation Loss: 0.893972\n",
      "Validation loss decreased (0.894001 --> 0.893972).         Saving model ...\n",
      "Epoch: 1823 \tTraining Loss: 0.879890 \tValidation Loss: 0.893944\n",
      "Validation loss decreased (0.893972 --> 0.893944).         Saving model ...\n",
      "Epoch: 1824 \tTraining Loss: 0.879859 \tValidation Loss: 0.893916\n",
      "Validation loss decreased (0.893944 --> 0.893916).         Saving model ...\n",
      "Epoch: 1825 \tTraining Loss: 0.879829 \tValidation Loss: 0.893887\n",
      "Validation loss decreased (0.893916 --> 0.893887).         Saving model ...\n",
      "Epoch: 1826 \tTraining Loss: 0.879798 \tValidation Loss: 0.893859\n",
      "Validation loss decreased (0.893887 --> 0.893859).         Saving model ...\n",
      "Epoch: 1827 \tTraining Loss: 0.879768 \tValidation Loss: 0.893831\n",
      "Validation loss decreased (0.893859 --> 0.893831).         Saving model ...\n",
      "Epoch: 1828 \tTraining Loss: 0.879737 \tValidation Loss: 0.893803\n",
      "Validation loss decreased (0.893831 --> 0.893803).         Saving model ...\n",
      "Epoch: 1829 \tTraining Loss: 0.879707 \tValidation Loss: 0.893774\n",
      "Validation loss decreased (0.893803 --> 0.893774).         Saving model ...\n",
      "Epoch: 1830 \tTraining Loss: 0.879676 \tValidation Loss: 0.893746\n",
      "Validation loss decreased (0.893774 --> 0.893746).         Saving model ...\n",
      "Epoch: 1831 \tTraining Loss: 0.879646 \tValidation Loss: 0.893718\n",
      "Validation loss decreased (0.893746 --> 0.893718).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1832 \tTraining Loss: 0.879615 \tValidation Loss: 0.893690\n",
      "Validation loss decreased (0.893718 --> 0.893690).         Saving model ...\n",
      "Epoch: 1833 \tTraining Loss: 0.879585 \tValidation Loss: 0.893662\n",
      "Validation loss decreased (0.893690 --> 0.893662).         Saving model ...\n",
      "Epoch: 1834 \tTraining Loss: 0.879555 \tValidation Loss: 0.893634\n",
      "Validation loss decreased (0.893662 --> 0.893634).         Saving model ...\n",
      "Epoch: 1835 \tTraining Loss: 0.879524 \tValidation Loss: 0.893606\n",
      "Validation loss decreased (0.893634 --> 0.893606).         Saving model ...\n",
      "Epoch: 1836 \tTraining Loss: 0.879494 \tValidation Loss: 0.893578\n",
      "Validation loss decreased (0.893606 --> 0.893578).         Saving model ...\n",
      "Epoch: 1837 \tTraining Loss: 0.879464 \tValidation Loss: 0.893550\n",
      "Validation loss decreased (0.893578 --> 0.893550).         Saving model ...\n",
      "Epoch: 1838 \tTraining Loss: 0.879433 \tValidation Loss: 0.893522\n",
      "Validation loss decreased (0.893550 --> 0.893522).         Saving model ...\n",
      "Epoch: 1839 \tTraining Loss: 0.879403 \tValidation Loss: 0.893494\n",
      "Validation loss decreased (0.893522 --> 0.893494).         Saving model ...\n",
      "Epoch: 1840 \tTraining Loss: 0.879373 \tValidation Loss: 0.893466\n",
      "Validation loss decreased (0.893494 --> 0.893466).         Saving model ...\n",
      "Epoch: 1841 \tTraining Loss: 0.879343 \tValidation Loss: 0.893438\n",
      "Validation loss decreased (0.893466 --> 0.893438).         Saving model ...\n",
      "Epoch: 1842 \tTraining Loss: 0.879313 \tValidation Loss: 0.893410\n",
      "Validation loss decreased (0.893438 --> 0.893410).         Saving model ...\n",
      "Epoch: 1843 \tTraining Loss: 0.879282 \tValidation Loss: 0.893382\n",
      "Validation loss decreased (0.893410 --> 0.893382).         Saving model ...\n",
      "Epoch: 1844 \tTraining Loss: 0.879252 \tValidation Loss: 0.893354\n",
      "Validation loss decreased (0.893382 --> 0.893354).         Saving model ...\n",
      "Epoch: 1845 \tTraining Loss: 0.879222 \tValidation Loss: 0.893326\n",
      "Validation loss decreased (0.893354 --> 0.893326).         Saving model ...\n",
      "Epoch: 1846 \tTraining Loss: 0.879192 \tValidation Loss: 0.893299\n",
      "Validation loss decreased (0.893326 --> 0.893299).         Saving model ...\n",
      "Epoch: 1847 \tTraining Loss: 0.879162 \tValidation Loss: 0.893271\n",
      "Validation loss decreased (0.893299 --> 0.893271).         Saving model ...\n",
      "Epoch: 1848 \tTraining Loss: 0.879132 \tValidation Loss: 0.893243\n",
      "Validation loss decreased (0.893271 --> 0.893243).         Saving model ...\n",
      "Epoch: 1849 \tTraining Loss: 0.879102 \tValidation Loss: 0.893215\n",
      "Validation loss decreased (0.893243 --> 0.893215).         Saving model ...\n",
      "Epoch: 1850 \tTraining Loss: 0.879072 \tValidation Loss: 0.893188\n",
      "Validation loss decreased (0.893215 --> 0.893188).         Saving model ...\n",
      "Epoch: 1851 \tTraining Loss: 0.879042 \tValidation Loss: 0.893160\n",
      "Validation loss decreased (0.893188 --> 0.893160).         Saving model ...\n",
      "Epoch: 1852 \tTraining Loss: 0.879012 \tValidation Loss: 0.893132\n",
      "Validation loss decreased (0.893160 --> 0.893132).         Saving model ...\n",
      "Epoch: 1853 \tTraining Loss: 0.878982 \tValidation Loss: 0.893105\n",
      "Validation loss decreased (0.893132 --> 0.893105).         Saving model ...\n",
      "Epoch: 1854 \tTraining Loss: 0.878952 \tValidation Loss: 0.893077\n",
      "Validation loss decreased (0.893105 --> 0.893077).         Saving model ...\n",
      "Epoch: 1855 \tTraining Loss: 0.878922 \tValidation Loss: 0.893049\n",
      "Validation loss decreased (0.893077 --> 0.893049).         Saving model ...\n",
      "Epoch: 1856 \tTraining Loss: 0.878892 \tValidation Loss: 0.893022\n",
      "Validation loss decreased (0.893049 --> 0.893022).         Saving model ...\n",
      "Epoch: 1857 \tTraining Loss: 0.878862 \tValidation Loss: 0.892994\n",
      "Validation loss decreased (0.893022 --> 0.892994).         Saving model ...\n",
      "Epoch: 1858 \tTraining Loss: 0.878833 \tValidation Loss: 0.892967\n",
      "Validation loss decreased (0.892994 --> 0.892967).         Saving model ...\n",
      "Epoch: 1859 \tTraining Loss: 0.878803 \tValidation Loss: 0.892939\n",
      "Validation loss decreased (0.892967 --> 0.892939).         Saving model ...\n",
      "Epoch: 1860 \tTraining Loss: 0.878773 \tValidation Loss: 0.892912\n",
      "Validation loss decreased (0.892939 --> 0.892912).         Saving model ...\n",
      "Epoch: 1861 \tTraining Loss: 0.878743 \tValidation Loss: 0.892884\n",
      "Validation loss decreased (0.892912 --> 0.892884).         Saving model ...\n",
      "Epoch: 1862 \tTraining Loss: 0.878714 \tValidation Loss: 0.892857\n",
      "Validation loss decreased (0.892884 --> 0.892857).         Saving model ...\n",
      "Epoch: 1863 \tTraining Loss: 0.878684 \tValidation Loss: 0.892829\n",
      "Validation loss decreased (0.892857 --> 0.892829).         Saving model ...\n",
      "Epoch: 1864 \tTraining Loss: 0.878654 \tValidation Loss: 0.892802\n",
      "Validation loss decreased (0.892829 --> 0.892802).         Saving model ...\n",
      "Epoch: 1865 \tTraining Loss: 0.878625 \tValidation Loss: 0.892774\n",
      "Validation loss decreased (0.892802 --> 0.892774).         Saving model ...\n",
      "Epoch: 1866 \tTraining Loss: 0.878595 \tValidation Loss: 0.892747\n",
      "Validation loss decreased (0.892774 --> 0.892747).         Saving model ...\n",
      "Epoch: 1867 \tTraining Loss: 0.878565 \tValidation Loss: 0.892720\n",
      "Validation loss decreased (0.892747 --> 0.892720).         Saving model ...\n",
      "Epoch: 1868 \tTraining Loss: 0.878536 \tValidation Loss: 0.892692\n",
      "Validation loss decreased (0.892720 --> 0.892692).         Saving model ...\n",
      "Epoch: 1869 \tTraining Loss: 0.878506 \tValidation Loss: 0.892665\n",
      "Validation loss decreased (0.892692 --> 0.892665).         Saving model ...\n",
      "Epoch: 1870 \tTraining Loss: 0.878477 \tValidation Loss: 0.892638\n",
      "Validation loss decreased (0.892665 --> 0.892638).         Saving model ...\n",
      "Epoch: 1871 \tTraining Loss: 0.878447 \tValidation Loss: 0.892610\n",
      "Validation loss decreased (0.892638 --> 0.892610).         Saving model ...\n",
      "Epoch: 1872 \tTraining Loss: 0.878417 \tValidation Loss: 0.892583\n",
      "Validation loss decreased (0.892610 --> 0.892583).         Saving model ...\n",
      "Epoch: 1873 \tTraining Loss: 0.878388 \tValidation Loss: 0.892556\n",
      "Validation loss decreased (0.892583 --> 0.892556).         Saving model ...\n",
      "Epoch: 1874 \tTraining Loss: 0.878358 \tValidation Loss: 0.892529\n",
      "Validation loss decreased (0.892556 --> 0.892529).         Saving model ...\n",
      "Epoch: 1875 \tTraining Loss: 0.878329 \tValidation Loss: 0.892502\n",
      "Validation loss decreased (0.892529 --> 0.892502).         Saving model ...\n",
      "Epoch: 1876 \tTraining Loss: 0.878300 \tValidation Loss: 0.892474\n",
      "Validation loss decreased (0.892502 --> 0.892474).         Saving model ...\n",
      "Epoch: 1877 \tTraining Loss: 0.878270 \tValidation Loss: 0.892447\n",
      "Validation loss decreased (0.892474 --> 0.892447).         Saving model ...\n",
      "Epoch: 1878 \tTraining Loss: 0.878241 \tValidation Loss: 0.892420\n",
      "Validation loss decreased (0.892447 --> 0.892420).         Saving model ...\n",
      "Epoch: 1879 \tTraining Loss: 0.878211 \tValidation Loss: 0.892393\n",
      "Validation loss decreased (0.892420 --> 0.892393).         Saving model ...\n",
      "Epoch: 1880 \tTraining Loss: 0.878182 \tValidation Loss: 0.892366\n",
      "Validation loss decreased (0.892393 --> 0.892366).         Saving model ...\n",
      "Epoch: 1881 \tTraining Loss: 0.878153 \tValidation Loss: 0.892339\n",
      "Validation loss decreased (0.892366 --> 0.892339).         Saving model ...\n",
      "Epoch: 1882 \tTraining Loss: 0.878124 \tValidation Loss: 0.892312\n",
      "Validation loss decreased (0.892339 --> 0.892312).         Saving model ...\n",
      "Epoch: 1883 \tTraining Loss: 0.878094 \tValidation Loss: 0.892285\n",
      "Validation loss decreased (0.892312 --> 0.892285).         Saving model ...\n",
      "Epoch: 1884 \tTraining Loss: 0.878065 \tValidation Loss: 0.892258\n",
      "Validation loss decreased (0.892285 --> 0.892258).         Saving model ...\n",
      "Epoch: 1885 \tTraining Loss: 0.878036 \tValidation Loss: 0.892231\n",
      "Validation loss decreased (0.892258 --> 0.892231).         Saving model ...\n",
      "Epoch: 1886 \tTraining Loss: 0.878007 \tValidation Loss: 0.892204\n",
      "Validation loss decreased (0.892231 --> 0.892204).         Saving model ...\n",
      "Epoch: 1887 \tTraining Loss: 0.877977 \tValidation Loss: 0.892177\n",
      "Validation loss decreased (0.892204 --> 0.892177).         Saving model ...\n",
      "Epoch: 1888 \tTraining Loss: 0.877948 \tValidation Loss: 0.892150\n",
      "Validation loss decreased (0.892177 --> 0.892150).         Saving model ...\n",
      "Epoch: 1889 \tTraining Loss: 0.877919 \tValidation Loss: 0.892123\n",
      "Validation loss decreased (0.892150 --> 0.892123).         Saving model ...\n",
      "Epoch: 1890 \tTraining Loss: 0.877890 \tValidation Loss: 0.892096\n",
      "Validation loss decreased (0.892123 --> 0.892096).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1891 \tTraining Loss: 0.877861 \tValidation Loss: 0.892069\n",
      "Validation loss decreased (0.892096 --> 0.892069).         Saving model ...\n",
      "Epoch: 1892 \tTraining Loss: 0.877832 \tValidation Loss: 0.892042\n",
      "Validation loss decreased (0.892069 --> 0.892042).         Saving model ...\n",
      "Epoch: 1893 \tTraining Loss: 0.877803 \tValidation Loss: 0.892016\n",
      "Validation loss decreased (0.892042 --> 0.892016).         Saving model ...\n",
      "Epoch: 1894 \tTraining Loss: 0.877774 \tValidation Loss: 0.891989\n",
      "Validation loss decreased (0.892016 --> 0.891989).         Saving model ...\n",
      "Epoch: 1895 \tTraining Loss: 0.877745 \tValidation Loss: 0.891962\n",
      "Validation loss decreased (0.891989 --> 0.891962).         Saving model ...\n",
      "Epoch: 1896 \tTraining Loss: 0.877716 \tValidation Loss: 0.891935\n",
      "Validation loss decreased (0.891962 --> 0.891935).         Saving model ...\n",
      "Epoch: 1897 \tTraining Loss: 0.877687 \tValidation Loss: 0.891909\n",
      "Validation loss decreased (0.891935 --> 0.891909).         Saving model ...\n",
      "Epoch: 1898 \tTraining Loss: 0.877658 \tValidation Loss: 0.891882\n",
      "Validation loss decreased (0.891909 --> 0.891882).         Saving model ...\n",
      "Epoch: 1899 \tTraining Loss: 0.877629 \tValidation Loss: 0.891855\n",
      "Validation loss decreased (0.891882 --> 0.891855).         Saving model ...\n",
      "Epoch: 1900 \tTraining Loss: 0.877600 \tValidation Loss: 0.891829\n",
      "Validation loss decreased (0.891855 --> 0.891829).         Saving model ...\n",
      "Epoch: 1901 \tTraining Loss: 0.877571 \tValidation Loss: 0.891802\n",
      "Validation loss decreased (0.891829 --> 0.891802).         Saving model ...\n",
      "Epoch: 1902 \tTraining Loss: 0.877542 \tValidation Loss: 0.891775\n",
      "Validation loss decreased (0.891802 --> 0.891775).         Saving model ...\n",
      "Epoch: 1903 \tTraining Loss: 0.877513 \tValidation Loss: 0.891749\n",
      "Validation loss decreased (0.891775 --> 0.891749).         Saving model ...\n",
      "Epoch: 1904 \tTraining Loss: 0.877484 \tValidation Loss: 0.891722\n",
      "Validation loss decreased (0.891749 --> 0.891722).         Saving model ...\n",
      "Epoch: 1905 \tTraining Loss: 0.877456 \tValidation Loss: 0.891696\n",
      "Validation loss decreased (0.891722 --> 0.891696).         Saving model ...\n",
      "Epoch: 1906 \tTraining Loss: 0.877427 \tValidation Loss: 0.891669\n",
      "Validation loss decreased (0.891696 --> 0.891669).         Saving model ...\n",
      "Epoch: 1907 \tTraining Loss: 0.877398 \tValidation Loss: 0.891643\n",
      "Validation loss decreased (0.891669 --> 0.891643).         Saving model ...\n",
      "Epoch: 1908 \tTraining Loss: 0.877369 \tValidation Loss: 0.891616\n",
      "Validation loss decreased (0.891643 --> 0.891616).         Saving model ...\n",
      "Epoch: 1909 \tTraining Loss: 0.877341 \tValidation Loss: 0.891590\n",
      "Validation loss decreased (0.891616 --> 0.891590).         Saving model ...\n",
      "Epoch: 1910 \tTraining Loss: 0.877312 \tValidation Loss: 0.891563\n",
      "Validation loss decreased (0.891590 --> 0.891563).         Saving model ...\n",
      "Epoch: 1911 \tTraining Loss: 0.877283 \tValidation Loss: 0.891537\n",
      "Validation loss decreased (0.891563 --> 0.891537).         Saving model ...\n",
      "Epoch: 1912 \tTraining Loss: 0.877255 \tValidation Loss: 0.891510\n",
      "Validation loss decreased (0.891537 --> 0.891510).         Saving model ...\n",
      "Epoch: 1913 \tTraining Loss: 0.877226 \tValidation Loss: 0.891484\n",
      "Validation loss decreased (0.891510 --> 0.891484).         Saving model ...\n",
      "Epoch: 1914 \tTraining Loss: 0.877197 \tValidation Loss: 0.891457\n",
      "Validation loss decreased (0.891484 --> 0.891457).         Saving model ...\n",
      "Epoch: 1915 \tTraining Loss: 0.877169 \tValidation Loss: 0.891431\n",
      "Validation loss decreased (0.891457 --> 0.891431).         Saving model ...\n",
      "Epoch: 1916 \tTraining Loss: 0.877140 \tValidation Loss: 0.891405\n",
      "Validation loss decreased (0.891431 --> 0.891405).         Saving model ...\n",
      "Epoch: 1917 \tTraining Loss: 0.877112 \tValidation Loss: 0.891378\n",
      "Validation loss decreased (0.891405 --> 0.891378).         Saving model ...\n",
      "Epoch: 1918 \tTraining Loss: 0.877083 \tValidation Loss: 0.891352\n",
      "Validation loss decreased (0.891378 --> 0.891352).         Saving model ...\n",
      "Epoch: 1919 \tTraining Loss: 0.877054 \tValidation Loss: 0.891326\n",
      "Validation loss decreased (0.891352 --> 0.891326).         Saving model ...\n",
      "Epoch: 1920 \tTraining Loss: 0.877026 \tValidation Loss: 0.891300\n",
      "Validation loss decreased (0.891326 --> 0.891300).         Saving model ...\n",
      "Epoch: 1921 \tTraining Loss: 0.876997 \tValidation Loss: 0.891273\n",
      "Validation loss decreased (0.891300 --> 0.891273).         Saving model ...\n",
      "Epoch: 1922 \tTraining Loss: 0.876969 \tValidation Loss: 0.891247\n",
      "Validation loss decreased (0.891273 --> 0.891247).         Saving model ...\n",
      "Epoch: 1923 \tTraining Loss: 0.876941 \tValidation Loss: 0.891221\n",
      "Validation loss decreased (0.891247 --> 0.891221).         Saving model ...\n",
      "Epoch: 1924 \tTraining Loss: 0.876912 \tValidation Loss: 0.891195\n",
      "Validation loss decreased (0.891221 --> 0.891195).         Saving model ...\n",
      "Epoch: 1925 \tTraining Loss: 0.876884 \tValidation Loss: 0.891169\n",
      "Validation loss decreased (0.891195 --> 0.891169).         Saving model ...\n",
      "Epoch: 1926 \tTraining Loss: 0.876855 \tValidation Loss: 0.891142\n",
      "Validation loss decreased (0.891169 --> 0.891142).         Saving model ...\n",
      "Epoch: 1927 \tTraining Loss: 0.876827 \tValidation Loss: 0.891116\n",
      "Validation loss decreased (0.891142 --> 0.891116).         Saving model ...\n",
      "Epoch: 1928 \tTraining Loss: 0.876799 \tValidation Loss: 0.891090\n",
      "Validation loss decreased (0.891116 --> 0.891090).         Saving model ...\n",
      "Epoch: 1929 \tTraining Loss: 0.876770 \tValidation Loss: 0.891064\n",
      "Validation loss decreased (0.891090 --> 0.891064).         Saving model ...\n",
      "Epoch: 1930 \tTraining Loss: 0.876742 \tValidation Loss: 0.891038\n",
      "Validation loss decreased (0.891064 --> 0.891038).         Saving model ...\n",
      "Epoch: 1931 \tTraining Loss: 0.876714 \tValidation Loss: 0.891012\n",
      "Validation loss decreased (0.891038 --> 0.891012).         Saving model ...\n",
      "Epoch: 1932 \tTraining Loss: 0.876686 \tValidation Loss: 0.890986\n",
      "Validation loss decreased (0.891012 --> 0.890986).         Saving model ...\n",
      "Epoch: 1933 \tTraining Loss: 0.876657 \tValidation Loss: 0.890960\n",
      "Validation loss decreased (0.890986 --> 0.890960).         Saving model ...\n",
      "Epoch: 1934 \tTraining Loss: 0.876629 \tValidation Loss: 0.890934\n",
      "Validation loss decreased (0.890960 --> 0.890934).         Saving model ...\n",
      "Epoch: 1935 \tTraining Loss: 0.876601 \tValidation Loss: 0.890908\n",
      "Validation loss decreased (0.890934 --> 0.890908).         Saving model ...\n",
      "Epoch: 1936 \tTraining Loss: 0.876573 \tValidation Loss: 0.890882\n",
      "Validation loss decreased (0.890908 --> 0.890882).         Saving model ...\n",
      "Epoch: 1937 \tTraining Loss: 0.876545 \tValidation Loss: 0.890856\n",
      "Validation loss decreased (0.890882 --> 0.890856).         Saving model ...\n",
      "Epoch: 1938 \tTraining Loss: 0.876517 \tValidation Loss: 0.890830\n",
      "Validation loss decreased (0.890856 --> 0.890830).         Saving model ...\n",
      "Epoch: 1939 \tTraining Loss: 0.876488 \tValidation Loss: 0.890804\n",
      "Validation loss decreased (0.890830 --> 0.890804).         Saving model ...\n",
      "Epoch: 1940 \tTraining Loss: 0.876460 \tValidation Loss: 0.890778\n",
      "Validation loss decreased (0.890804 --> 0.890778).         Saving model ...\n",
      "Epoch: 1941 \tTraining Loss: 0.876432 \tValidation Loss: 0.890753\n",
      "Validation loss decreased (0.890778 --> 0.890753).         Saving model ...\n",
      "Epoch: 1942 \tTraining Loss: 0.876404 \tValidation Loss: 0.890727\n",
      "Validation loss decreased (0.890753 --> 0.890727).         Saving model ...\n",
      "Epoch: 1943 \tTraining Loss: 0.876376 \tValidation Loss: 0.890701\n",
      "Validation loss decreased (0.890727 --> 0.890701).         Saving model ...\n",
      "Epoch: 1944 \tTraining Loss: 0.876348 \tValidation Loss: 0.890675\n",
      "Validation loss decreased (0.890701 --> 0.890675).         Saving model ...\n",
      "Epoch: 1945 \tTraining Loss: 0.876320 \tValidation Loss: 0.890649\n",
      "Validation loss decreased (0.890675 --> 0.890649).         Saving model ...\n",
      "Epoch: 1946 \tTraining Loss: 0.876292 \tValidation Loss: 0.890624\n",
      "Validation loss decreased (0.890649 --> 0.890624).         Saving model ...\n",
      "Epoch: 1947 \tTraining Loss: 0.876264 \tValidation Loss: 0.890598\n",
      "Validation loss decreased (0.890624 --> 0.890598).         Saving model ...\n",
      "Epoch: 1948 \tTraining Loss: 0.876236 \tValidation Loss: 0.890572\n",
      "Validation loss decreased (0.890598 --> 0.890572).         Saving model ...\n",
      "Epoch: 1949 \tTraining Loss: 0.876208 \tValidation Loss: 0.890546\n",
      "Validation loss decreased (0.890572 --> 0.890546).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1950 \tTraining Loss: 0.876181 \tValidation Loss: 0.890521\n",
      "Validation loss decreased (0.890546 --> 0.890521).         Saving model ...\n",
      "Epoch: 1951 \tTraining Loss: 0.876153 \tValidation Loss: 0.890495\n",
      "Validation loss decreased (0.890521 --> 0.890495).         Saving model ...\n",
      "Epoch: 1952 \tTraining Loss: 0.876125 \tValidation Loss: 0.890469\n",
      "Validation loss decreased (0.890495 --> 0.890469).         Saving model ...\n",
      "Epoch: 1953 \tTraining Loss: 0.876097 \tValidation Loss: 0.890444\n",
      "Validation loss decreased (0.890469 --> 0.890444).         Saving model ...\n",
      "Epoch: 1954 \tTraining Loss: 0.876069 \tValidation Loss: 0.890418\n",
      "Validation loss decreased (0.890444 --> 0.890418).         Saving model ...\n",
      "Epoch: 1955 \tTraining Loss: 0.876041 \tValidation Loss: 0.890393\n",
      "Validation loss decreased (0.890418 --> 0.890393).         Saving model ...\n",
      "Epoch: 1956 \tTraining Loss: 0.876014 \tValidation Loss: 0.890367\n",
      "Validation loss decreased (0.890393 --> 0.890367).         Saving model ...\n",
      "Epoch: 1957 \tTraining Loss: 0.875986 \tValidation Loss: 0.890341\n",
      "Validation loss decreased (0.890367 --> 0.890341).         Saving model ...\n",
      "Epoch: 1958 \tTraining Loss: 0.875958 \tValidation Loss: 0.890316\n",
      "Validation loss decreased (0.890341 --> 0.890316).         Saving model ...\n",
      "Epoch: 1959 \tTraining Loss: 0.875930 \tValidation Loss: 0.890290\n",
      "Validation loss decreased (0.890316 --> 0.890290).         Saving model ...\n",
      "Epoch: 1960 \tTraining Loss: 0.875903 \tValidation Loss: 0.890265\n",
      "Validation loss decreased (0.890290 --> 0.890265).         Saving model ...\n",
      "Epoch: 1961 \tTraining Loss: 0.875875 \tValidation Loss: 0.890240\n",
      "Validation loss decreased (0.890265 --> 0.890240).         Saving model ...\n",
      "Epoch: 1962 \tTraining Loss: 0.875847 \tValidation Loss: 0.890214\n",
      "Validation loss decreased (0.890240 --> 0.890214).         Saving model ...\n",
      "Epoch: 1963 \tTraining Loss: 0.875820 \tValidation Loss: 0.890189\n",
      "Validation loss decreased (0.890214 --> 0.890189).         Saving model ...\n",
      "Epoch: 1964 \tTraining Loss: 0.875792 \tValidation Loss: 0.890163\n",
      "Validation loss decreased (0.890189 --> 0.890163).         Saving model ...\n",
      "Epoch: 1965 \tTraining Loss: 0.875765 \tValidation Loss: 0.890138\n",
      "Validation loss decreased (0.890163 --> 0.890138).         Saving model ...\n",
      "Epoch: 1966 \tTraining Loss: 0.875737 \tValidation Loss: 0.890112\n",
      "Validation loss decreased (0.890138 --> 0.890112).         Saving model ...\n",
      "Epoch: 1967 \tTraining Loss: 0.875709 \tValidation Loss: 0.890087\n",
      "Validation loss decreased (0.890112 --> 0.890087).         Saving model ...\n",
      "Epoch: 1968 \tTraining Loss: 0.875682 \tValidation Loss: 0.890062\n",
      "Validation loss decreased (0.890087 --> 0.890062).         Saving model ...\n",
      "Epoch: 1969 \tTraining Loss: 0.875654 \tValidation Loss: 0.890036\n",
      "Validation loss decreased (0.890062 --> 0.890036).         Saving model ...\n",
      "Epoch: 1970 \tTraining Loss: 0.875627 \tValidation Loss: 0.890011\n",
      "Validation loss decreased (0.890036 --> 0.890011).         Saving model ...\n",
      "Epoch: 1971 \tTraining Loss: 0.875599 \tValidation Loss: 0.889986\n",
      "Validation loss decreased (0.890011 --> 0.889986).         Saving model ...\n",
      "Epoch: 1972 \tTraining Loss: 0.875572 \tValidation Loss: 0.889961\n",
      "Validation loss decreased (0.889986 --> 0.889961).         Saving model ...\n",
      "Epoch: 1973 \tTraining Loss: 0.875544 \tValidation Loss: 0.889935\n",
      "Validation loss decreased (0.889961 --> 0.889935).         Saving model ...\n",
      "Epoch: 1974 \tTraining Loss: 0.875517 \tValidation Loss: 0.889910\n",
      "Validation loss decreased (0.889935 --> 0.889910).         Saving model ...\n",
      "Epoch: 1975 \tTraining Loss: 0.875490 \tValidation Loss: 0.889885\n",
      "Validation loss decreased (0.889910 --> 0.889885).         Saving model ...\n",
      "Epoch: 1976 \tTraining Loss: 0.875462 \tValidation Loss: 0.889860\n",
      "Validation loss decreased (0.889885 --> 0.889860).         Saving model ...\n",
      "Epoch: 1977 \tTraining Loss: 0.875435 \tValidation Loss: 0.889834\n",
      "Validation loss decreased (0.889860 --> 0.889834).         Saving model ...\n",
      "Epoch: 1978 \tTraining Loss: 0.875408 \tValidation Loss: 0.889809\n",
      "Validation loss decreased (0.889834 --> 0.889809).         Saving model ...\n",
      "Epoch: 1979 \tTraining Loss: 0.875380 \tValidation Loss: 0.889784\n",
      "Validation loss decreased (0.889809 --> 0.889784).         Saving model ...\n",
      "Epoch: 1980 \tTraining Loss: 0.875353 \tValidation Loss: 0.889759\n",
      "Validation loss decreased (0.889784 --> 0.889759).         Saving model ...\n",
      "Epoch: 1981 \tTraining Loss: 0.875326 \tValidation Loss: 0.889734\n",
      "Validation loss decreased (0.889759 --> 0.889734).         Saving model ...\n",
      "Epoch: 1982 \tTraining Loss: 0.875298 \tValidation Loss: 0.889709\n",
      "Validation loss decreased (0.889734 --> 0.889709).         Saving model ...\n",
      "Epoch: 1983 \tTraining Loss: 0.875271 \tValidation Loss: 0.889684\n",
      "Validation loss decreased (0.889709 --> 0.889684).         Saving model ...\n",
      "Epoch: 1984 \tTraining Loss: 0.875244 \tValidation Loss: 0.889659\n",
      "Validation loss decreased (0.889684 --> 0.889659).         Saving model ...\n",
      "Epoch: 1985 \tTraining Loss: 0.875217 \tValidation Loss: 0.889634\n",
      "Validation loss decreased (0.889659 --> 0.889634).         Saving model ...\n",
      "Epoch: 1986 \tTraining Loss: 0.875189 \tValidation Loss: 0.889609\n",
      "Validation loss decreased (0.889634 --> 0.889609).         Saving model ...\n",
      "Epoch: 1987 \tTraining Loss: 0.875162 \tValidation Loss: 0.889584\n",
      "Validation loss decreased (0.889609 --> 0.889584).         Saving model ...\n",
      "Epoch: 1988 \tTraining Loss: 0.875135 \tValidation Loss: 0.889559\n",
      "Validation loss decreased (0.889584 --> 0.889559).         Saving model ...\n",
      "Epoch: 1989 \tTraining Loss: 0.875108 \tValidation Loss: 0.889534\n",
      "Validation loss decreased (0.889559 --> 0.889534).         Saving model ...\n",
      "Epoch: 1990 \tTraining Loss: 0.875081 \tValidation Loss: 0.889509\n",
      "Validation loss decreased (0.889534 --> 0.889509).         Saving model ...\n",
      "Epoch: 1991 \tTraining Loss: 0.875054 \tValidation Loss: 0.889484\n",
      "Validation loss decreased (0.889509 --> 0.889484).         Saving model ...\n",
      "Epoch: 1992 \tTraining Loss: 0.875027 \tValidation Loss: 0.889459\n",
      "Validation loss decreased (0.889484 --> 0.889459).         Saving model ...\n",
      "Epoch: 1993 \tTraining Loss: 0.875000 \tValidation Loss: 0.889434\n",
      "Validation loss decreased (0.889459 --> 0.889434).         Saving model ...\n",
      "Epoch: 1994 \tTraining Loss: 0.874973 \tValidation Loss: 0.889409\n",
      "Validation loss decreased (0.889434 --> 0.889409).         Saving model ...\n",
      "Epoch: 1995 \tTraining Loss: 0.874946 \tValidation Loss: 0.889385\n",
      "Validation loss decreased (0.889409 --> 0.889385).         Saving model ...\n",
      "Epoch: 1996 \tTraining Loss: 0.874919 \tValidation Loss: 0.889360\n",
      "Validation loss decreased (0.889385 --> 0.889360).         Saving model ...\n",
      "Epoch: 1997 \tTraining Loss: 0.874892 \tValidation Loss: 0.889335\n",
      "Validation loss decreased (0.889360 --> 0.889335).         Saving model ...\n",
      "Epoch: 1998 \tTraining Loss: 0.874865 \tValidation Loss: 0.889310\n",
      "Validation loss decreased (0.889335 --> 0.889310).         Saving model ...\n",
      "Epoch: 1999 \tTraining Loss: 0.874838 \tValidation Loss: 0.889285\n",
      "Validation loss decreased (0.889310 --> 0.889285).         Saving model ...\n",
      "Epoch: 2000 \tTraining Loss: 0.874811 \tValidation Loss: 0.889261\n",
      "Validation loss decreased (0.889285 --> 0.889261).         Saving model ...\n",
      "Epoch: 2001 \tTraining Loss: 0.874784 \tValidation Loss: 0.889236\n",
      "Validation loss decreased (0.889261 --> 0.889236).         Saving model ...\n",
      "Epoch: 2002 \tTraining Loss: 0.874757 \tValidation Loss: 0.889211\n",
      "Validation loss decreased (0.889236 --> 0.889211).         Saving model ...\n",
      "Epoch: 2003 \tTraining Loss: 0.874730 \tValidation Loss: 0.889187\n",
      "Validation loss decreased (0.889211 --> 0.889187).         Saving model ...\n",
      "Epoch: 2004 \tTraining Loss: 0.874703 \tValidation Loss: 0.889162\n",
      "Validation loss decreased (0.889187 --> 0.889162).         Saving model ...\n",
      "Epoch: 2005 \tTraining Loss: 0.874676 \tValidation Loss: 0.889137\n",
      "Validation loss decreased (0.889162 --> 0.889137).         Saving model ...\n",
      "Epoch: 2006 \tTraining Loss: 0.874650 \tValidation Loss: 0.889113\n",
      "Validation loss decreased (0.889137 --> 0.889113).         Saving model ...\n",
      "Epoch: 2007 \tTraining Loss: 0.874623 \tValidation Loss: 0.889088\n",
      "Validation loss decreased (0.889113 --> 0.889088).         Saving model ...\n",
      "Epoch: 2008 \tTraining Loss: 0.874596 \tValidation Loss: 0.889063\n",
      "Validation loss decreased (0.889088 --> 0.889063).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2009 \tTraining Loss: 0.874569 \tValidation Loss: 0.889039\n",
      "Validation loss decreased (0.889063 --> 0.889039).         Saving model ...\n",
      "Epoch: 2010 \tTraining Loss: 0.874542 \tValidation Loss: 0.889014\n",
      "Validation loss decreased (0.889039 --> 0.889014).         Saving model ...\n",
      "Epoch: 2011 \tTraining Loss: 0.874516 \tValidation Loss: 0.888990\n",
      "Validation loss decreased (0.889014 --> 0.888990).         Saving model ...\n",
      "Epoch: 2012 \tTraining Loss: 0.874489 \tValidation Loss: 0.888965\n",
      "Validation loss decreased (0.888990 --> 0.888965).         Saving model ...\n",
      "Epoch: 2013 \tTraining Loss: 0.874462 \tValidation Loss: 0.888941\n",
      "Validation loss decreased (0.888965 --> 0.888941).         Saving model ...\n",
      "Epoch: 2014 \tTraining Loss: 0.874436 \tValidation Loss: 0.888916\n",
      "Validation loss decreased (0.888941 --> 0.888916).         Saving model ...\n",
      "Epoch: 2015 \tTraining Loss: 0.874409 \tValidation Loss: 0.888892\n",
      "Validation loss decreased (0.888916 --> 0.888892).         Saving model ...\n",
      "Epoch: 2016 \tTraining Loss: 0.874382 \tValidation Loss: 0.888867\n",
      "Validation loss decreased (0.888892 --> 0.888867).         Saving model ...\n",
      "Epoch: 2017 \tTraining Loss: 0.874356 \tValidation Loss: 0.888843\n",
      "Validation loss decreased (0.888867 --> 0.888843).         Saving model ...\n",
      "Epoch: 2018 \tTraining Loss: 0.874329 \tValidation Loss: 0.888818\n",
      "Validation loss decreased (0.888843 --> 0.888818).         Saving model ...\n",
      "Epoch: 2019 \tTraining Loss: 0.874303 \tValidation Loss: 0.888794\n",
      "Validation loss decreased (0.888818 --> 0.888794).         Saving model ...\n",
      "Epoch: 2020 \tTraining Loss: 0.874276 \tValidation Loss: 0.888770\n",
      "Validation loss decreased (0.888794 --> 0.888770).         Saving model ...\n",
      "Epoch: 2021 \tTraining Loss: 0.874249 \tValidation Loss: 0.888745\n",
      "Validation loss decreased (0.888770 --> 0.888745).         Saving model ...\n",
      "Epoch: 2022 \tTraining Loss: 0.874223 \tValidation Loss: 0.888721\n",
      "Validation loss decreased (0.888745 --> 0.888721).         Saving model ...\n",
      "Epoch: 2023 \tTraining Loss: 0.874196 \tValidation Loss: 0.888696\n",
      "Validation loss decreased (0.888721 --> 0.888696).         Saving model ...\n",
      "Epoch: 2024 \tTraining Loss: 0.874170 \tValidation Loss: 0.888672\n",
      "Validation loss decreased (0.888696 --> 0.888672).         Saving model ...\n",
      "Epoch: 2025 \tTraining Loss: 0.874143 \tValidation Loss: 0.888648\n",
      "Validation loss decreased (0.888672 --> 0.888648).         Saving model ...\n",
      "Epoch: 2026 \tTraining Loss: 0.874117 \tValidation Loss: 0.888624\n",
      "Validation loss decreased (0.888648 --> 0.888624).         Saving model ...\n",
      "Epoch: 2027 \tTraining Loss: 0.874091 \tValidation Loss: 0.888599\n",
      "Validation loss decreased (0.888624 --> 0.888599).         Saving model ...\n",
      "Epoch: 2028 \tTraining Loss: 0.874064 \tValidation Loss: 0.888575\n",
      "Validation loss decreased (0.888599 --> 0.888575).         Saving model ...\n",
      "Epoch: 2029 \tTraining Loss: 0.874038 \tValidation Loss: 0.888551\n",
      "Validation loss decreased (0.888575 --> 0.888551).         Saving model ...\n",
      "Epoch: 2030 \tTraining Loss: 0.874011 \tValidation Loss: 0.888527\n",
      "Validation loss decreased (0.888551 --> 0.888527).         Saving model ...\n",
      "Epoch: 2031 \tTraining Loss: 0.873985 \tValidation Loss: 0.888502\n",
      "Validation loss decreased (0.888527 --> 0.888502).         Saving model ...\n",
      "Epoch: 2032 \tTraining Loss: 0.873959 \tValidation Loss: 0.888478\n",
      "Validation loss decreased (0.888502 --> 0.888478).         Saving model ...\n",
      "Epoch: 2033 \tTraining Loss: 0.873932 \tValidation Loss: 0.888454\n",
      "Validation loss decreased (0.888478 --> 0.888454).         Saving model ...\n",
      "Epoch: 2034 \tTraining Loss: 0.873906 \tValidation Loss: 0.888430\n",
      "Validation loss decreased (0.888454 --> 0.888430).         Saving model ...\n",
      "Epoch: 2035 \tTraining Loss: 0.873880 \tValidation Loss: 0.888406\n",
      "Validation loss decreased (0.888430 --> 0.888406).         Saving model ...\n",
      "Epoch: 2036 \tTraining Loss: 0.873854 \tValidation Loss: 0.888382\n",
      "Validation loss decreased (0.888406 --> 0.888382).         Saving model ...\n",
      "Epoch: 2037 \tTraining Loss: 0.873827 \tValidation Loss: 0.888358\n",
      "Validation loss decreased (0.888382 --> 0.888358).         Saving model ...\n",
      "Epoch: 2038 \tTraining Loss: 0.873801 \tValidation Loss: 0.888334\n",
      "Validation loss decreased (0.888358 --> 0.888334).         Saving model ...\n",
      "Epoch: 2039 \tTraining Loss: 0.873775 \tValidation Loss: 0.888310\n",
      "Validation loss decreased (0.888334 --> 0.888310).         Saving model ...\n",
      "Epoch: 2040 \tTraining Loss: 0.873749 \tValidation Loss: 0.888285\n",
      "Validation loss decreased (0.888310 --> 0.888285).         Saving model ...\n",
      "Epoch: 2041 \tTraining Loss: 0.873722 \tValidation Loss: 0.888261\n",
      "Validation loss decreased (0.888285 --> 0.888261).         Saving model ...\n",
      "Epoch: 2042 \tTraining Loss: 0.873696 \tValidation Loss: 0.888237\n",
      "Validation loss decreased (0.888261 --> 0.888237).         Saving model ...\n",
      "Epoch: 2043 \tTraining Loss: 0.873670 \tValidation Loss: 0.888213\n",
      "Validation loss decreased (0.888237 --> 0.888213).         Saving model ...\n",
      "Epoch: 2044 \tTraining Loss: 0.873644 \tValidation Loss: 0.888189\n",
      "Validation loss decreased (0.888213 --> 0.888189).         Saving model ...\n",
      "Epoch: 2045 \tTraining Loss: 0.873618 \tValidation Loss: 0.888166\n",
      "Validation loss decreased (0.888189 --> 0.888166).         Saving model ...\n",
      "Epoch: 2046 \tTraining Loss: 0.873592 \tValidation Loss: 0.888142\n",
      "Validation loss decreased (0.888166 --> 0.888142).         Saving model ...\n",
      "Epoch: 2047 \tTraining Loss: 0.873566 \tValidation Loss: 0.888118\n",
      "Validation loss decreased (0.888142 --> 0.888118).         Saving model ...\n",
      "Epoch: 2048 \tTraining Loss: 0.873540 \tValidation Loss: 0.888094\n",
      "Validation loss decreased (0.888118 --> 0.888094).         Saving model ...\n",
      "Epoch: 2049 \tTraining Loss: 0.873514 \tValidation Loss: 0.888070\n",
      "Validation loss decreased (0.888094 --> 0.888070).         Saving model ...\n",
      "Epoch: 2050 \tTraining Loss: 0.873488 \tValidation Loss: 0.888046\n",
      "Validation loss decreased (0.888070 --> 0.888046).         Saving model ...\n",
      "Epoch: 2051 \tTraining Loss: 0.873462 \tValidation Loss: 0.888022\n",
      "Validation loss decreased (0.888046 --> 0.888022).         Saving model ...\n",
      "Epoch: 2052 \tTraining Loss: 0.873436 \tValidation Loss: 0.887998\n",
      "Validation loss decreased (0.888022 --> 0.887998).         Saving model ...\n",
      "Epoch: 2053 \tTraining Loss: 0.873410 \tValidation Loss: 0.887975\n",
      "Validation loss decreased (0.887998 --> 0.887975).         Saving model ...\n",
      "Epoch: 2054 \tTraining Loss: 0.873384 \tValidation Loss: 0.887951\n",
      "Validation loss decreased (0.887975 --> 0.887951).         Saving model ...\n",
      "Epoch: 2055 \tTraining Loss: 0.873358 \tValidation Loss: 0.887927\n",
      "Validation loss decreased (0.887951 --> 0.887927).         Saving model ...\n",
      "Epoch: 2056 \tTraining Loss: 0.873332 \tValidation Loss: 0.887903\n",
      "Validation loss decreased (0.887927 --> 0.887903).         Saving model ...\n",
      "Epoch: 2057 \tTraining Loss: 0.873306 \tValidation Loss: 0.887879\n",
      "Validation loss decreased (0.887903 --> 0.887879).         Saving model ...\n",
      "Epoch: 2058 \tTraining Loss: 0.873280 \tValidation Loss: 0.887856\n",
      "Validation loss decreased (0.887879 --> 0.887856).         Saving model ...\n",
      "Epoch: 2059 \tTraining Loss: 0.873254 \tValidation Loss: 0.887832\n",
      "Validation loss decreased (0.887856 --> 0.887832).         Saving model ...\n",
      "Epoch: 2060 \tTraining Loss: 0.873228 \tValidation Loss: 0.887808\n",
      "Validation loss decreased (0.887832 --> 0.887808).         Saving model ...\n",
      "Epoch: 2061 \tTraining Loss: 0.873202 \tValidation Loss: 0.887785\n",
      "Validation loss decreased (0.887808 --> 0.887785).         Saving model ...\n",
      "Epoch: 2062 \tTraining Loss: 0.873177 \tValidation Loss: 0.887761\n",
      "Validation loss decreased (0.887785 --> 0.887761).         Saving model ...\n",
      "Epoch: 2063 \tTraining Loss: 0.873151 \tValidation Loss: 0.887737\n",
      "Validation loss decreased (0.887761 --> 0.887737).         Saving model ...\n",
      "Epoch: 2064 \tTraining Loss: 0.873125 \tValidation Loss: 0.887714\n",
      "Validation loss decreased (0.887737 --> 0.887714).         Saving model ...\n",
      "Epoch: 2065 \tTraining Loss: 0.873099 \tValidation Loss: 0.887690\n",
      "Validation loss decreased (0.887714 --> 0.887690).         Saving model ...\n",
      "Epoch: 2066 \tTraining Loss: 0.873074 \tValidation Loss: 0.887667\n",
      "Validation loss decreased (0.887690 --> 0.887667).         Saving model ...\n",
      "Epoch: 2067 \tTraining Loss: 0.873048 \tValidation Loss: 0.887643\n",
      "Validation loss decreased (0.887667 --> 0.887643).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2068 \tTraining Loss: 0.873022 \tValidation Loss: 0.887619\n",
      "Validation loss decreased (0.887643 --> 0.887619).         Saving model ...\n",
      "Epoch: 2069 \tTraining Loss: 0.872996 \tValidation Loss: 0.887596\n",
      "Validation loss decreased (0.887619 --> 0.887596).         Saving model ...\n",
      "Epoch: 2070 \tTraining Loss: 0.872971 \tValidation Loss: 0.887572\n",
      "Validation loss decreased (0.887596 --> 0.887572).         Saving model ...\n",
      "Epoch: 2071 \tTraining Loss: 0.872945 \tValidation Loss: 0.887549\n",
      "Validation loss decreased (0.887572 --> 0.887549).         Saving model ...\n",
      "Epoch: 2072 \tTraining Loss: 0.872919 \tValidation Loss: 0.887525\n",
      "Validation loss decreased (0.887549 --> 0.887525).         Saving model ...\n",
      "Epoch: 2073 \tTraining Loss: 0.872894 \tValidation Loss: 0.887502\n",
      "Validation loss decreased (0.887525 --> 0.887502).         Saving model ...\n",
      "Epoch: 2074 \tTraining Loss: 0.872868 \tValidation Loss: 0.887478\n",
      "Validation loss decreased (0.887502 --> 0.887478).         Saving model ...\n",
      "Epoch: 2075 \tTraining Loss: 0.872843 \tValidation Loss: 0.887455\n",
      "Validation loss decreased (0.887478 --> 0.887455).         Saving model ...\n",
      "Epoch: 2076 \tTraining Loss: 0.872817 \tValidation Loss: 0.887431\n",
      "Validation loss decreased (0.887455 --> 0.887431).         Saving model ...\n",
      "Epoch: 2077 \tTraining Loss: 0.872791 \tValidation Loss: 0.887408\n",
      "Validation loss decreased (0.887431 --> 0.887408).         Saving model ...\n",
      "Epoch: 2078 \tTraining Loss: 0.872766 \tValidation Loss: 0.887385\n",
      "Validation loss decreased (0.887408 --> 0.887385).         Saving model ...\n",
      "Epoch: 2079 \tTraining Loss: 0.872740 \tValidation Loss: 0.887361\n",
      "Validation loss decreased (0.887385 --> 0.887361).         Saving model ...\n",
      "Epoch: 2080 \tTraining Loss: 0.872715 \tValidation Loss: 0.887338\n",
      "Validation loss decreased (0.887361 --> 0.887338).         Saving model ...\n",
      "Epoch: 2081 \tTraining Loss: 0.872689 \tValidation Loss: 0.887315\n",
      "Validation loss decreased (0.887338 --> 0.887315).         Saving model ...\n",
      "Epoch: 2082 \tTraining Loss: 0.872664 \tValidation Loss: 0.887291\n",
      "Validation loss decreased (0.887315 --> 0.887291).         Saving model ...\n",
      "Epoch: 2083 \tTraining Loss: 0.872638 \tValidation Loss: 0.887268\n",
      "Validation loss decreased (0.887291 --> 0.887268).         Saving model ...\n",
      "Epoch: 2084 \tTraining Loss: 0.872613 \tValidation Loss: 0.887245\n",
      "Validation loss decreased (0.887268 --> 0.887245).         Saving model ...\n",
      "Epoch: 2085 \tTraining Loss: 0.872588 \tValidation Loss: 0.887221\n",
      "Validation loss decreased (0.887245 --> 0.887221).         Saving model ...\n",
      "Epoch: 2086 \tTraining Loss: 0.872562 \tValidation Loss: 0.887198\n",
      "Validation loss decreased (0.887221 --> 0.887198).         Saving model ...\n",
      "Epoch: 2087 \tTraining Loss: 0.872537 \tValidation Loss: 0.887175\n",
      "Validation loss decreased (0.887198 --> 0.887175).         Saving model ...\n",
      "Epoch: 2088 \tTraining Loss: 0.872511 \tValidation Loss: 0.887152\n",
      "Validation loss decreased (0.887175 --> 0.887152).         Saving model ...\n",
      "Epoch: 2089 \tTraining Loss: 0.872486 \tValidation Loss: 0.887128\n",
      "Validation loss decreased (0.887152 --> 0.887128).         Saving model ...\n",
      "Epoch: 2090 \tTraining Loss: 0.872461 \tValidation Loss: 0.887105\n",
      "Validation loss decreased (0.887128 --> 0.887105).         Saving model ...\n",
      "Epoch: 2091 \tTraining Loss: 0.872435 \tValidation Loss: 0.887082\n",
      "Validation loss decreased (0.887105 --> 0.887082).         Saving model ...\n",
      "Epoch: 2092 \tTraining Loss: 0.872410 \tValidation Loss: 0.887059\n",
      "Validation loss decreased (0.887082 --> 0.887059).         Saving model ...\n",
      "Epoch: 2093 \tTraining Loss: 0.872385 \tValidation Loss: 0.887036\n",
      "Validation loss decreased (0.887059 --> 0.887036).         Saving model ...\n",
      "Epoch: 2094 \tTraining Loss: 0.872360 \tValidation Loss: 0.887013\n",
      "Validation loss decreased (0.887036 --> 0.887013).         Saving model ...\n",
      "Epoch: 2095 \tTraining Loss: 0.872334 \tValidation Loss: 0.886990\n",
      "Validation loss decreased (0.887013 --> 0.886990).         Saving model ...\n",
      "Epoch: 2096 \tTraining Loss: 0.872309 \tValidation Loss: 0.886966\n",
      "Validation loss decreased (0.886990 --> 0.886966).         Saving model ...\n",
      "Epoch: 2097 \tTraining Loss: 0.872284 \tValidation Loss: 0.886943\n",
      "Validation loss decreased (0.886966 --> 0.886943).         Saving model ...\n",
      "Epoch: 2098 \tTraining Loss: 0.872259 \tValidation Loss: 0.886920\n",
      "Validation loss decreased (0.886943 --> 0.886920).         Saving model ...\n",
      "Epoch: 2099 \tTraining Loss: 0.872233 \tValidation Loss: 0.886897\n",
      "Validation loss decreased (0.886920 --> 0.886897).         Saving model ...\n",
      "Epoch: 2100 \tTraining Loss: 0.872208 \tValidation Loss: 0.886874\n",
      "Validation loss decreased (0.886897 --> 0.886874).         Saving model ...\n",
      "Epoch: 2101 \tTraining Loss: 0.872183 \tValidation Loss: 0.886851\n",
      "Validation loss decreased (0.886874 --> 0.886851).         Saving model ...\n",
      "Epoch: 2102 \tTraining Loss: 0.872158 \tValidation Loss: 0.886828\n",
      "Validation loss decreased (0.886851 --> 0.886828).         Saving model ...\n",
      "Epoch: 2103 \tTraining Loss: 0.872133 \tValidation Loss: 0.886805\n",
      "Validation loss decreased (0.886828 --> 0.886805).         Saving model ...\n",
      "Epoch: 2104 \tTraining Loss: 0.872108 \tValidation Loss: 0.886782\n",
      "Validation loss decreased (0.886805 --> 0.886782).         Saving model ...\n",
      "Epoch: 2105 \tTraining Loss: 0.872083 \tValidation Loss: 0.886759\n",
      "Validation loss decreased (0.886782 --> 0.886759).         Saving model ...\n",
      "Epoch: 2106 \tTraining Loss: 0.872058 \tValidation Loss: 0.886736\n",
      "Validation loss decreased (0.886759 --> 0.886736).         Saving model ...\n",
      "Epoch: 2107 \tTraining Loss: 0.872033 \tValidation Loss: 0.886713\n",
      "Validation loss decreased (0.886736 --> 0.886713).         Saving model ...\n",
      "Epoch: 2108 \tTraining Loss: 0.872007 \tValidation Loss: 0.886690\n",
      "Validation loss decreased (0.886713 --> 0.886690).         Saving model ...\n",
      "Epoch: 2109 \tTraining Loss: 0.871982 \tValidation Loss: 0.886668\n",
      "Validation loss decreased (0.886690 --> 0.886668).         Saving model ...\n",
      "Epoch: 2110 \tTraining Loss: 0.871957 \tValidation Loss: 0.886645\n",
      "Validation loss decreased (0.886668 --> 0.886645).         Saving model ...\n",
      "Epoch: 2111 \tTraining Loss: 0.871932 \tValidation Loss: 0.886622\n",
      "Validation loss decreased (0.886645 --> 0.886622).         Saving model ...\n",
      "Epoch: 2112 \tTraining Loss: 0.871907 \tValidation Loss: 0.886599\n",
      "Validation loss decreased (0.886622 --> 0.886599).         Saving model ...\n",
      "Epoch: 2113 \tTraining Loss: 0.871883 \tValidation Loss: 0.886576\n",
      "Validation loss decreased (0.886599 --> 0.886576).         Saving model ...\n",
      "Epoch: 2114 \tTraining Loss: 0.871858 \tValidation Loss: 0.886553\n",
      "Validation loss decreased (0.886576 --> 0.886553).         Saving model ...\n",
      "Epoch: 2115 \tTraining Loss: 0.871833 \tValidation Loss: 0.886531\n",
      "Validation loss decreased (0.886553 --> 0.886531).         Saving model ...\n",
      "Epoch: 2116 \tTraining Loss: 0.871808 \tValidation Loss: 0.886508\n",
      "Validation loss decreased (0.886531 --> 0.886508).         Saving model ...\n",
      "Epoch: 2117 \tTraining Loss: 0.871783 \tValidation Loss: 0.886485\n",
      "Validation loss decreased (0.886508 --> 0.886485).         Saving model ...\n",
      "Epoch: 2118 \tTraining Loss: 0.871758 \tValidation Loss: 0.886462\n",
      "Validation loss decreased (0.886485 --> 0.886462).         Saving model ...\n",
      "Epoch: 2119 \tTraining Loss: 0.871733 \tValidation Loss: 0.886440\n",
      "Validation loss decreased (0.886462 --> 0.886440).         Saving model ...\n",
      "Epoch: 2120 \tTraining Loss: 0.871708 \tValidation Loss: 0.886417\n",
      "Validation loss decreased (0.886440 --> 0.886417).         Saving model ...\n",
      "Epoch: 2121 \tTraining Loss: 0.871683 \tValidation Loss: 0.886394\n",
      "Validation loss decreased (0.886417 --> 0.886394).         Saving model ...\n",
      "Epoch: 2122 \tTraining Loss: 0.871659 \tValidation Loss: 0.886371\n",
      "Validation loss decreased (0.886394 --> 0.886371).         Saving model ...\n",
      "Epoch: 2123 \tTraining Loss: 0.871634 \tValidation Loss: 0.886349\n",
      "Validation loss decreased (0.886371 --> 0.886349).         Saving model ...\n",
      "Epoch: 2124 \tTraining Loss: 0.871609 \tValidation Loss: 0.886326\n",
      "Validation loss decreased (0.886349 --> 0.886326).         Saving model ...\n",
      "Epoch: 2125 \tTraining Loss: 0.871584 \tValidation Loss: 0.886304\n",
      "Validation loss decreased (0.886326 --> 0.886304).         Saving model ...\n",
      "Epoch: 2126 \tTraining Loss: 0.871560 \tValidation Loss: 0.886281\n",
      "Validation loss decreased (0.886304 --> 0.886281).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2127 \tTraining Loss: 0.871535 \tValidation Loss: 0.886258\n",
      "Validation loss decreased (0.886281 --> 0.886258).         Saving model ...\n",
      "Epoch: 2128 \tTraining Loss: 0.871510 \tValidation Loss: 0.886236\n",
      "Validation loss decreased (0.886258 --> 0.886236).         Saving model ...\n",
      "Epoch: 2129 \tTraining Loss: 0.871485 \tValidation Loss: 0.886213\n",
      "Validation loss decreased (0.886236 --> 0.886213).         Saving model ...\n",
      "Epoch: 2130 \tTraining Loss: 0.871461 \tValidation Loss: 0.886191\n",
      "Validation loss decreased (0.886213 --> 0.886191).         Saving model ...\n",
      "Epoch: 2131 \tTraining Loss: 0.871436 \tValidation Loss: 0.886168\n",
      "Validation loss decreased (0.886191 --> 0.886168).         Saving model ...\n",
      "Epoch: 2132 \tTraining Loss: 0.871411 \tValidation Loss: 0.886145\n",
      "Validation loss decreased (0.886168 --> 0.886145).         Saving model ...\n",
      "Epoch: 2133 \tTraining Loss: 0.871387 \tValidation Loss: 0.886123\n",
      "Validation loss decreased (0.886145 --> 0.886123).         Saving model ...\n",
      "Epoch: 2134 \tTraining Loss: 0.871362 \tValidation Loss: 0.886100\n",
      "Validation loss decreased (0.886123 --> 0.886100).         Saving model ...\n",
      "Epoch: 2135 \tTraining Loss: 0.871338 \tValidation Loss: 0.886078\n",
      "Validation loss decreased (0.886100 --> 0.886078).         Saving model ...\n",
      "Epoch: 2136 \tTraining Loss: 0.871313 \tValidation Loss: 0.886055\n",
      "Validation loss decreased (0.886078 --> 0.886055).         Saving model ...\n",
      "Epoch: 2137 \tTraining Loss: 0.871288 \tValidation Loss: 0.886033\n",
      "Validation loss decreased (0.886055 --> 0.886033).         Saving model ...\n",
      "Epoch: 2138 \tTraining Loss: 0.871264 \tValidation Loss: 0.886011\n",
      "Validation loss decreased (0.886033 --> 0.886011).         Saving model ...\n",
      "Epoch: 2139 \tTraining Loss: 0.871239 \tValidation Loss: 0.885988\n",
      "Validation loss decreased (0.886011 --> 0.885988).         Saving model ...\n",
      "Epoch: 2140 \tTraining Loss: 0.871215 \tValidation Loss: 0.885966\n",
      "Validation loss decreased (0.885988 --> 0.885966).         Saving model ...\n",
      "Epoch: 2141 \tTraining Loss: 0.871190 \tValidation Loss: 0.885943\n",
      "Validation loss decreased (0.885966 --> 0.885943).         Saving model ...\n",
      "Epoch: 2142 \tTraining Loss: 0.871166 \tValidation Loss: 0.885921\n",
      "Validation loss decreased (0.885943 --> 0.885921).         Saving model ...\n",
      "Epoch: 2143 \tTraining Loss: 0.871141 \tValidation Loss: 0.885899\n",
      "Validation loss decreased (0.885921 --> 0.885899).         Saving model ...\n",
      "Epoch: 2144 \tTraining Loss: 0.871117 \tValidation Loss: 0.885876\n",
      "Validation loss decreased (0.885899 --> 0.885876).         Saving model ...\n",
      "Epoch: 2145 \tTraining Loss: 0.871092 \tValidation Loss: 0.885854\n",
      "Validation loss decreased (0.885876 --> 0.885854).         Saving model ...\n",
      "Epoch: 2146 \tTraining Loss: 0.871068 \tValidation Loss: 0.885832\n",
      "Validation loss decreased (0.885854 --> 0.885832).         Saving model ...\n",
      "Epoch: 2147 \tTraining Loss: 0.871044 \tValidation Loss: 0.885809\n",
      "Validation loss decreased (0.885832 --> 0.885809).         Saving model ...\n",
      "Epoch: 2148 \tTraining Loss: 0.871019 \tValidation Loss: 0.885787\n",
      "Validation loss decreased (0.885809 --> 0.885787).         Saving model ...\n",
      "Epoch: 2149 \tTraining Loss: 0.870995 \tValidation Loss: 0.885765\n",
      "Validation loss decreased (0.885787 --> 0.885765).         Saving model ...\n",
      "Epoch: 2150 \tTraining Loss: 0.870970 \tValidation Loss: 0.885743\n",
      "Validation loss decreased (0.885765 --> 0.885743).         Saving model ...\n",
      "Epoch: 2151 \tTraining Loss: 0.870946 \tValidation Loss: 0.885720\n",
      "Validation loss decreased (0.885743 --> 0.885720).         Saving model ...\n",
      "Epoch: 2152 \tTraining Loss: 0.870922 \tValidation Loss: 0.885698\n",
      "Validation loss decreased (0.885720 --> 0.885698).         Saving model ...\n",
      "Epoch: 2153 \tTraining Loss: 0.870897 \tValidation Loss: 0.885676\n",
      "Validation loss decreased (0.885698 --> 0.885676).         Saving model ...\n",
      "Epoch: 2154 \tTraining Loss: 0.870873 \tValidation Loss: 0.885654\n",
      "Validation loss decreased (0.885676 --> 0.885654).         Saving model ...\n",
      "Epoch: 2155 \tTraining Loss: 0.870849 \tValidation Loss: 0.885632\n",
      "Validation loss decreased (0.885654 --> 0.885632).         Saving model ...\n",
      "Epoch: 2156 \tTraining Loss: 0.870825 \tValidation Loss: 0.885609\n",
      "Validation loss decreased (0.885632 --> 0.885609).         Saving model ...\n",
      "Epoch: 2157 \tTraining Loss: 0.870800 \tValidation Loss: 0.885587\n",
      "Validation loss decreased (0.885609 --> 0.885587).         Saving model ...\n",
      "Epoch: 2158 \tTraining Loss: 0.870776 \tValidation Loss: 0.885565\n",
      "Validation loss decreased (0.885587 --> 0.885565).         Saving model ...\n",
      "Epoch: 2159 \tTraining Loss: 0.870752 \tValidation Loss: 0.885543\n",
      "Validation loss decreased (0.885565 --> 0.885543).         Saving model ...\n",
      "Epoch: 2160 \tTraining Loss: 0.870728 \tValidation Loss: 0.885521\n",
      "Validation loss decreased (0.885543 --> 0.885521).         Saving model ...\n",
      "Epoch: 2161 \tTraining Loss: 0.870704 \tValidation Loss: 0.885499\n",
      "Validation loss decreased (0.885521 --> 0.885499).         Saving model ...\n",
      "Epoch: 2162 \tTraining Loss: 0.870679 \tValidation Loss: 0.885477\n",
      "Validation loss decreased (0.885499 --> 0.885477).         Saving model ...\n",
      "Epoch: 2163 \tTraining Loss: 0.870655 \tValidation Loss: 0.885455\n",
      "Validation loss decreased (0.885477 --> 0.885455).         Saving model ...\n",
      "Epoch: 2164 \tTraining Loss: 0.870631 \tValidation Loss: 0.885433\n",
      "Validation loss decreased (0.885455 --> 0.885433).         Saving model ...\n",
      "Epoch: 2165 \tTraining Loss: 0.870607 \tValidation Loss: 0.885411\n",
      "Validation loss decreased (0.885433 --> 0.885411).         Saving model ...\n",
      "Epoch: 2166 \tTraining Loss: 0.870583 \tValidation Loss: 0.885389\n",
      "Validation loss decreased (0.885411 --> 0.885389).         Saving model ...\n",
      "Epoch: 2167 \tTraining Loss: 0.870559 \tValidation Loss: 0.885367\n",
      "Validation loss decreased (0.885389 --> 0.885367).         Saving model ...\n",
      "Epoch: 2168 \tTraining Loss: 0.870535 \tValidation Loss: 0.885345\n",
      "Validation loss decreased (0.885367 --> 0.885345).         Saving model ...\n",
      "Epoch: 2169 \tTraining Loss: 0.870511 \tValidation Loss: 0.885323\n",
      "Validation loss decreased (0.885345 --> 0.885323).         Saving model ...\n",
      "Epoch: 2170 \tTraining Loss: 0.870486 \tValidation Loss: 0.885301\n",
      "Validation loss decreased (0.885323 --> 0.885301).         Saving model ...\n",
      "Epoch: 2171 \tTraining Loss: 0.870462 \tValidation Loss: 0.885279\n",
      "Validation loss decreased (0.885301 --> 0.885279).         Saving model ...\n",
      "Epoch: 2172 \tTraining Loss: 0.870438 \tValidation Loss: 0.885257\n",
      "Validation loss decreased (0.885279 --> 0.885257).         Saving model ...\n",
      "Epoch: 2173 \tTraining Loss: 0.870414 \tValidation Loss: 0.885235\n",
      "Validation loss decreased (0.885257 --> 0.885235).         Saving model ...\n",
      "Epoch: 2174 \tTraining Loss: 0.870390 \tValidation Loss: 0.885213\n",
      "Validation loss decreased (0.885235 --> 0.885213).         Saving model ...\n",
      "Epoch: 2175 \tTraining Loss: 0.870366 \tValidation Loss: 0.885191\n",
      "Validation loss decreased (0.885213 --> 0.885191).         Saving model ...\n",
      "Epoch: 2176 \tTraining Loss: 0.870342 \tValidation Loss: 0.885169\n",
      "Validation loss decreased (0.885191 --> 0.885169).         Saving model ...\n",
      "Epoch: 2177 \tTraining Loss: 0.870319 \tValidation Loss: 0.885148\n",
      "Validation loss decreased (0.885169 --> 0.885148).         Saving model ...\n",
      "Epoch: 2178 \tTraining Loss: 0.870295 \tValidation Loss: 0.885126\n",
      "Validation loss decreased (0.885148 --> 0.885126).         Saving model ...\n",
      "Epoch: 2179 \tTraining Loss: 0.870271 \tValidation Loss: 0.885104\n",
      "Validation loss decreased (0.885126 --> 0.885104).         Saving model ...\n",
      "Epoch: 2180 \tTraining Loss: 0.870247 \tValidation Loss: 0.885082\n",
      "Validation loss decreased (0.885104 --> 0.885082).         Saving model ...\n",
      "Epoch: 2181 \tTraining Loss: 0.870223 \tValidation Loss: 0.885060\n",
      "Validation loss decreased (0.885082 --> 0.885060).         Saving model ...\n",
      "Epoch: 2182 \tTraining Loss: 0.870199 \tValidation Loss: 0.885039\n",
      "Validation loss decreased (0.885060 --> 0.885039).         Saving model ...\n",
      "Epoch: 2183 \tTraining Loss: 0.870175 \tValidation Loss: 0.885017\n",
      "Validation loss decreased (0.885039 --> 0.885017).         Saving model ...\n",
      "Epoch: 2184 \tTraining Loss: 0.870151 \tValidation Loss: 0.884995\n",
      "Validation loss decreased (0.885017 --> 0.884995).         Saving model ...\n",
      "Epoch: 2185 \tTraining Loss: 0.870128 \tValidation Loss: 0.884973\n",
      "Validation loss decreased (0.884995 --> 0.884973).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2186 \tTraining Loss: 0.870104 \tValidation Loss: 0.884952\n",
      "Validation loss decreased (0.884973 --> 0.884952).         Saving model ...\n",
      "Epoch: 2187 \tTraining Loss: 0.870080 \tValidation Loss: 0.884930\n",
      "Validation loss decreased (0.884952 --> 0.884930).         Saving model ...\n",
      "Epoch: 2188 \tTraining Loss: 0.870056 \tValidation Loss: 0.884908\n",
      "Validation loss decreased (0.884930 --> 0.884908).         Saving model ...\n",
      "Epoch: 2189 \tTraining Loss: 0.870032 \tValidation Loss: 0.884887\n",
      "Validation loss decreased (0.884908 --> 0.884887).         Saving model ...\n",
      "Epoch: 2190 \tTraining Loss: 0.870009 \tValidation Loss: 0.884865\n",
      "Validation loss decreased (0.884887 --> 0.884865).         Saving model ...\n",
      "Epoch: 2191 \tTraining Loss: 0.869985 \tValidation Loss: 0.884843\n",
      "Validation loss decreased (0.884865 --> 0.884843).         Saving model ...\n",
      "Epoch: 2192 \tTraining Loss: 0.869961 \tValidation Loss: 0.884822\n",
      "Validation loss decreased (0.884843 --> 0.884822).         Saving model ...\n",
      "Epoch: 2193 \tTraining Loss: 0.869937 \tValidation Loss: 0.884800\n",
      "Validation loss decreased (0.884822 --> 0.884800).         Saving model ...\n",
      "Epoch: 2194 \tTraining Loss: 0.869914 \tValidation Loss: 0.884778\n",
      "Validation loss decreased (0.884800 --> 0.884778).         Saving model ...\n",
      "Epoch: 2195 \tTraining Loss: 0.869890 \tValidation Loss: 0.884757\n",
      "Validation loss decreased (0.884778 --> 0.884757).         Saving model ...\n",
      "Epoch: 2196 \tTraining Loss: 0.869866 \tValidation Loss: 0.884735\n",
      "Validation loss decreased (0.884757 --> 0.884735).         Saving model ...\n",
      "Epoch: 2197 \tTraining Loss: 0.869843 \tValidation Loss: 0.884714\n",
      "Validation loss decreased (0.884735 --> 0.884714).         Saving model ...\n",
      "Epoch: 2198 \tTraining Loss: 0.869819 \tValidation Loss: 0.884692\n",
      "Validation loss decreased (0.884714 --> 0.884692).         Saving model ...\n",
      "Epoch: 2199 \tTraining Loss: 0.869796 \tValidation Loss: 0.884671\n",
      "Validation loss decreased (0.884692 --> 0.884671).         Saving model ...\n",
      "Epoch: 2200 \tTraining Loss: 0.869772 \tValidation Loss: 0.884649\n",
      "Validation loss decreased (0.884671 --> 0.884649).         Saving model ...\n",
      "Epoch: 2201 \tTraining Loss: 0.869748 \tValidation Loss: 0.884628\n",
      "Validation loss decreased (0.884649 --> 0.884628).         Saving model ...\n",
      "Epoch: 2202 \tTraining Loss: 0.869725 \tValidation Loss: 0.884606\n",
      "Validation loss decreased (0.884628 --> 0.884606).         Saving model ...\n",
      "Epoch: 2203 \tTraining Loss: 0.869701 \tValidation Loss: 0.884585\n",
      "Validation loss decreased (0.884606 --> 0.884585).         Saving model ...\n",
      "Epoch: 2204 \tTraining Loss: 0.869678 \tValidation Loss: 0.884563\n",
      "Validation loss decreased (0.884585 --> 0.884563).         Saving model ...\n",
      "Epoch: 2205 \tTraining Loss: 0.869654 \tValidation Loss: 0.884542\n",
      "Validation loss decreased (0.884563 --> 0.884542).         Saving model ...\n",
      "Epoch: 2206 \tTraining Loss: 0.869631 \tValidation Loss: 0.884521\n",
      "Validation loss decreased (0.884542 --> 0.884521).         Saving model ...\n",
      "Epoch: 2207 \tTraining Loss: 0.869607 \tValidation Loss: 0.884499\n",
      "Validation loss decreased (0.884521 --> 0.884499).         Saving model ...\n",
      "Epoch: 2208 \tTraining Loss: 0.869584 \tValidation Loss: 0.884478\n",
      "Validation loss decreased (0.884499 --> 0.884478).         Saving model ...\n",
      "Epoch: 2209 \tTraining Loss: 0.869560 \tValidation Loss: 0.884456\n",
      "Validation loss decreased (0.884478 --> 0.884456).         Saving model ...\n",
      "Epoch: 2210 \tTraining Loss: 0.869537 \tValidation Loss: 0.884435\n",
      "Validation loss decreased (0.884456 --> 0.884435).         Saving model ...\n",
      "Epoch: 2211 \tTraining Loss: 0.869513 \tValidation Loss: 0.884414\n",
      "Validation loss decreased (0.884435 --> 0.884414).         Saving model ...\n",
      "Epoch: 2212 \tTraining Loss: 0.869490 \tValidation Loss: 0.884392\n",
      "Validation loss decreased (0.884414 --> 0.884392).         Saving model ...\n",
      "Epoch: 2213 \tTraining Loss: 0.869466 \tValidation Loss: 0.884371\n",
      "Validation loss decreased (0.884392 --> 0.884371).         Saving model ...\n",
      "Epoch: 2214 \tTraining Loss: 0.869443 \tValidation Loss: 0.884350\n",
      "Validation loss decreased (0.884371 --> 0.884350).         Saving model ...\n",
      "Epoch: 2215 \tTraining Loss: 0.869420 \tValidation Loss: 0.884328\n",
      "Validation loss decreased (0.884350 --> 0.884328).         Saving model ...\n",
      "Epoch: 2216 \tTraining Loss: 0.869396 \tValidation Loss: 0.884307\n",
      "Validation loss decreased (0.884328 --> 0.884307).         Saving model ...\n",
      "Epoch: 2217 \tTraining Loss: 0.869373 \tValidation Loss: 0.884286\n",
      "Validation loss decreased (0.884307 --> 0.884286).         Saving model ...\n",
      "Epoch: 2218 \tTraining Loss: 0.869350 \tValidation Loss: 0.884265\n",
      "Validation loss decreased (0.884286 --> 0.884265).         Saving model ...\n",
      "Epoch: 2219 \tTraining Loss: 0.869326 \tValidation Loss: 0.884243\n",
      "Validation loss decreased (0.884265 --> 0.884243).         Saving model ...\n",
      "Epoch: 2220 \tTraining Loss: 0.869303 \tValidation Loss: 0.884222\n",
      "Validation loss decreased (0.884243 --> 0.884222).         Saving model ...\n",
      "Epoch: 2221 \tTraining Loss: 0.869280 \tValidation Loss: 0.884201\n",
      "Validation loss decreased (0.884222 --> 0.884201).         Saving model ...\n",
      "Epoch: 2222 \tTraining Loss: 0.869256 \tValidation Loss: 0.884180\n",
      "Validation loss decreased (0.884201 --> 0.884180).         Saving model ...\n",
      "Epoch: 2223 \tTraining Loss: 0.869233 \tValidation Loss: 0.884159\n",
      "Validation loss decreased (0.884180 --> 0.884159).         Saving model ...\n",
      "Epoch: 2224 \tTraining Loss: 0.869210 \tValidation Loss: 0.884137\n",
      "Validation loss decreased (0.884159 --> 0.884137).         Saving model ...\n",
      "Epoch: 2225 \tTraining Loss: 0.869187 \tValidation Loss: 0.884116\n",
      "Validation loss decreased (0.884137 --> 0.884116).         Saving model ...\n",
      "Epoch: 2226 \tTraining Loss: 0.869163 \tValidation Loss: 0.884095\n",
      "Validation loss decreased (0.884116 --> 0.884095).         Saving model ...\n",
      "Epoch: 2227 \tTraining Loss: 0.869140 \tValidation Loss: 0.884074\n",
      "Validation loss decreased (0.884095 --> 0.884074).         Saving model ...\n",
      "Epoch: 2228 \tTraining Loss: 0.869117 \tValidation Loss: 0.884053\n",
      "Validation loss decreased (0.884074 --> 0.884053).         Saving model ...\n",
      "Epoch: 2229 \tTraining Loss: 0.869094 \tValidation Loss: 0.884032\n",
      "Validation loss decreased (0.884053 --> 0.884032).         Saving model ...\n",
      "Epoch: 2230 \tTraining Loss: 0.869071 \tValidation Loss: 0.884011\n",
      "Validation loss decreased (0.884032 --> 0.884011).         Saving model ...\n",
      "Epoch: 2231 \tTraining Loss: 0.869048 \tValidation Loss: 0.883990\n",
      "Validation loss decreased (0.884011 --> 0.883990).         Saving model ...\n",
      "Epoch: 2232 \tTraining Loss: 0.869024 \tValidation Loss: 0.883969\n",
      "Validation loss decreased (0.883990 --> 0.883969).         Saving model ...\n",
      "Epoch: 2233 \tTraining Loss: 0.869001 \tValidation Loss: 0.883948\n",
      "Validation loss decreased (0.883969 --> 0.883948).         Saving model ...\n",
      "Epoch: 2234 \tTraining Loss: 0.868978 \tValidation Loss: 0.883927\n",
      "Validation loss decreased (0.883948 --> 0.883927).         Saving model ...\n",
      "Epoch: 2235 \tTraining Loss: 0.868955 \tValidation Loss: 0.883905\n",
      "Validation loss decreased (0.883927 --> 0.883905).         Saving model ...\n",
      "Epoch: 2236 \tTraining Loss: 0.868932 \tValidation Loss: 0.883884\n",
      "Validation loss decreased (0.883905 --> 0.883884).         Saving model ...\n",
      "Epoch: 2237 \tTraining Loss: 0.868909 \tValidation Loss: 0.883864\n",
      "Validation loss decreased (0.883884 --> 0.883864).         Saving model ...\n",
      "Epoch: 2238 \tTraining Loss: 0.868886 \tValidation Loss: 0.883843\n",
      "Validation loss decreased (0.883864 --> 0.883843).         Saving model ...\n",
      "Epoch: 2239 \tTraining Loss: 0.868863 \tValidation Loss: 0.883822\n",
      "Validation loss decreased (0.883843 --> 0.883822).         Saving model ...\n",
      "Epoch: 2240 \tTraining Loss: 0.868840 \tValidation Loss: 0.883801\n",
      "Validation loss decreased (0.883822 --> 0.883801).         Saving model ...\n",
      "Epoch: 2241 \tTraining Loss: 0.868817 \tValidation Loss: 0.883780\n",
      "Validation loss decreased (0.883801 --> 0.883780).         Saving model ...\n",
      "Epoch: 2242 \tTraining Loss: 0.868794 \tValidation Loss: 0.883759\n",
      "Validation loss decreased (0.883780 --> 0.883759).         Saving model ...\n",
      "Epoch: 2243 \tTraining Loss: 0.868771 \tValidation Loss: 0.883738\n",
      "Validation loss decreased (0.883759 --> 0.883738).         Saving model ...\n",
      "Epoch: 2244 \tTraining Loss: 0.868748 \tValidation Loss: 0.883717\n",
      "Validation loss decreased (0.883738 --> 0.883717).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2245 \tTraining Loss: 0.868725 \tValidation Loss: 0.883696\n",
      "Validation loss decreased (0.883717 --> 0.883696).         Saving model ...\n",
      "Epoch: 2246 \tTraining Loss: 0.868702 \tValidation Loss: 0.883675\n",
      "Validation loss decreased (0.883696 --> 0.883675).         Saving model ...\n",
      "Epoch: 2247 \tTraining Loss: 0.868679 \tValidation Loss: 0.883654\n",
      "Validation loss decreased (0.883675 --> 0.883654).         Saving model ...\n",
      "Epoch: 2248 \tTraining Loss: 0.868656 \tValidation Loss: 0.883634\n",
      "Validation loss decreased (0.883654 --> 0.883634).         Saving model ...\n",
      "Epoch: 2249 \tTraining Loss: 0.868633 \tValidation Loss: 0.883613\n",
      "Validation loss decreased (0.883634 --> 0.883613).         Saving model ...\n",
      "Epoch: 2250 \tTraining Loss: 0.868610 \tValidation Loss: 0.883592\n",
      "Validation loss decreased (0.883613 --> 0.883592).         Saving model ...\n",
      "Epoch: 2251 \tTraining Loss: 0.868588 \tValidation Loss: 0.883571\n",
      "Validation loss decreased (0.883592 --> 0.883571).         Saving model ...\n",
      "Epoch: 2252 \tTraining Loss: 0.868565 \tValidation Loss: 0.883550\n",
      "Validation loss decreased (0.883571 --> 0.883550).         Saving model ...\n",
      "Epoch: 2253 \tTraining Loss: 0.868542 \tValidation Loss: 0.883530\n",
      "Validation loss decreased (0.883550 --> 0.883530).         Saving model ...\n",
      "Epoch: 2254 \tTraining Loss: 0.868519 \tValidation Loss: 0.883509\n",
      "Validation loss decreased (0.883530 --> 0.883509).         Saving model ...\n",
      "Epoch: 2255 \tTraining Loss: 0.868496 \tValidation Loss: 0.883488\n",
      "Validation loss decreased (0.883509 --> 0.883488).         Saving model ...\n",
      "Epoch: 2256 \tTraining Loss: 0.868474 \tValidation Loss: 0.883467\n",
      "Validation loss decreased (0.883488 --> 0.883467).         Saving model ...\n",
      "Epoch: 2257 \tTraining Loss: 0.868451 \tValidation Loss: 0.883447\n",
      "Validation loss decreased (0.883467 --> 0.883447).         Saving model ...\n",
      "Epoch: 2258 \tTraining Loss: 0.868428 \tValidation Loss: 0.883426\n",
      "Validation loss decreased (0.883447 --> 0.883426).         Saving model ...\n",
      "Epoch: 2259 \tTraining Loss: 0.868405 \tValidation Loss: 0.883405\n",
      "Validation loss decreased (0.883426 --> 0.883405).         Saving model ...\n",
      "Epoch: 2260 \tTraining Loss: 0.868382 \tValidation Loss: 0.883385\n",
      "Validation loss decreased (0.883405 --> 0.883385).         Saving model ...\n",
      "Epoch: 2261 \tTraining Loss: 0.868360 \tValidation Loss: 0.883364\n",
      "Validation loss decreased (0.883385 --> 0.883364).         Saving model ...\n",
      "Epoch: 2262 \tTraining Loss: 0.868337 \tValidation Loss: 0.883343\n",
      "Validation loss decreased (0.883364 --> 0.883343).         Saving model ...\n",
      "Epoch: 2263 \tTraining Loss: 0.868314 \tValidation Loss: 0.883323\n",
      "Validation loss decreased (0.883343 --> 0.883323).         Saving model ...\n",
      "Epoch: 2264 \tTraining Loss: 0.868292 \tValidation Loss: 0.883302\n",
      "Validation loss decreased (0.883323 --> 0.883302).         Saving model ...\n",
      "Epoch: 2265 \tTraining Loss: 0.868269 \tValidation Loss: 0.883282\n",
      "Validation loss decreased (0.883302 --> 0.883282).         Saving model ...\n",
      "Epoch: 2266 \tTraining Loss: 0.868246 \tValidation Loss: 0.883261\n",
      "Validation loss decreased (0.883282 --> 0.883261).         Saving model ...\n",
      "Epoch: 2267 \tTraining Loss: 0.868224 \tValidation Loss: 0.883240\n",
      "Validation loss decreased (0.883261 --> 0.883240).         Saving model ...\n",
      "Epoch: 2268 \tTraining Loss: 0.868201 \tValidation Loss: 0.883220\n",
      "Validation loss decreased (0.883240 --> 0.883220).         Saving model ...\n",
      "Epoch: 2269 \tTraining Loss: 0.868178 \tValidation Loss: 0.883199\n",
      "Validation loss decreased (0.883220 --> 0.883199).         Saving model ...\n",
      "Epoch: 2270 \tTraining Loss: 0.868156 \tValidation Loss: 0.883179\n",
      "Validation loss decreased (0.883199 --> 0.883179).         Saving model ...\n",
      "Epoch: 2271 \tTraining Loss: 0.868133 \tValidation Loss: 0.883158\n",
      "Validation loss decreased (0.883179 --> 0.883158).         Saving model ...\n",
      "Epoch: 2272 \tTraining Loss: 0.868111 \tValidation Loss: 0.883138\n",
      "Validation loss decreased (0.883158 --> 0.883138).         Saving model ...\n",
      "Epoch: 2273 \tTraining Loss: 0.868088 \tValidation Loss: 0.883117\n",
      "Validation loss decreased (0.883138 --> 0.883117).         Saving model ...\n",
      "Epoch: 2274 \tTraining Loss: 0.868066 \tValidation Loss: 0.883097\n",
      "Validation loss decreased (0.883117 --> 0.883097).         Saving model ...\n",
      "Epoch: 2275 \tTraining Loss: 0.868043 \tValidation Loss: 0.883076\n",
      "Validation loss decreased (0.883097 --> 0.883076).         Saving model ...\n",
      "Epoch: 2276 \tTraining Loss: 0.868021 \tValidation Loss: 0.883056\n",
      "Validation loss decreased (0.883076 --> 0.883056).         Saving model ...\n",
      "Epoch: 2277 \tTraining Loss: 0.867998 \tValidation Loss: 0.883036\n",
      "Validation loss decreased (0.883056 --> 0.883036).         Saving model ...\n",
      "Epoch: 2278 \tTraining Loss: 0.867976 \tValidation Loss: 0.883015\n",
      "Validation loss decreased (0.883036 --> 0.883015).         Saving model ...\n",
      "Epoch: 2279 \tTraining Loss: 0.867953 \tValidation Loss: 0.882995\n",
      "Validation loss decreased (0.883015 --> 0.882995).         Saving model ...\n",
      "Epoch: 2280 \tTraining Loss: 0.867931 \tValidation Loss: 0.882974\n",
      "Validation loss decreased (0.882995 --> 0.882974).         Saving model ...\n",
      "Epoch: 2281 \tTraining Loss: 0.867908 \tValidation Loss: 0.882954\n",
      "Validation loss decreased (0.882974 --> 0.882954).         Saving model ...\n",
      "Epoch: 2282 \tTraining Loss: 0.867886 \tValidation Loss: 0.882933\n",
      "Validation loss decreased (0.882954 --> 0.882933).         Saving model ...\n",
      "Epoch: 2283 \tTraining Loss: 0.867863 \tValidation Loss: 0.882913\n",
      "Validation loss decreased (0.882933 --> 0.882913).         Saving model ...\n",
      "Epoch: 2284 \tTraining Loss: 0.867841 \tValidation Loss: 0.882893\n",
      "Validation loss decreased (0.882913 --> 0.882893).         Saving model ...\n",
      "Epoch: 2285 \tTraining Loss: 0.867819 \tValidation Loss: 0.882872\n",
      "Validation loss decreased (0.882893 --> 0.882872).         Saving model ...\n",
      "Epoch: 2286 \tTraining Loss: 0.867796 \tValidation Loss: 0.882852\n",
      "Validation loss decreased (0.882872 --> 0.882852).         Saving model ...\n",
      "Epoch: 2287 \tTraining Loss: 0.867774 \tValidation Loss: 0.882832\n",
      "Validation loss decreased (0.882852 --> 0.882832).         Saving model ...\n",
      "Epoch: 2288 \tTraining Loss: 0.867751 \tValidation Loss: 0.882812\n",
      "Validation loss decreased (0.882832 --> 0.882812).         Saving model ...\n",
      "Epoch: 2289 \tTraining Loss: 0.867729 \tValidation Loss: 0.882791\n",
      "Validation loss decreased (0.882812 --> 0.882791).         Saving model ...\n",
      "Epoch: 2290 \tTraining Loss: 0.867707 \tValidation Loss: 0.882771\n",
      "Validation loss decreased (0.882791 --> 0.882771).         Saving model ...\n",
      "Epoch: 2291 \tTraining Loss: 0.867684 \tValidation Loss: 0.882751\n",
      "Validation loss decreased (0.882771 --> 0.882751).         Saving model ...\n",
      "Epoch: 2292 \tTraining Loss: 0.867662 \tValidation Loss: 0.882730\n",
      "Validation loss decreased (0.882751 --> 0.882730).         Saving model ...\n",
      "Epoch: 2293 \tTraining Loss: 0.867640 \tValidation Loss: 0.882710\n",
      "Validation loss decreased (0.882730 --> 0.882710).         Saving model ...\n",
      "Epoch: 2294 \tTraining Loss: 0.867618 \tValidation Loss: 0.882690\n",
      "Validation loss decreased (0.882710 --> 0.882690).         Saving model ...\n",
      "Epoch: 2295 \tTraining Loss: 0.867595 \tValidation Loss: 0.882670\n",
      "Validation loss decreased (0.882690 --> 0.882670).         Saving model ...\n",
      "Epoch: 2296 \tTraining Loss: 0.867573 \tValidation Loss: 0.882650\n",
      "Validation loss decreased (0.882670 --> 0.882650).         Saving model ...\n",
      "Epoch: 2297 \tTraining Loss: 0.867551 \tValidation Loss: 0.882630\n",
      "Validation loss decreased (0.882650 --> 0.882630).         Saving model ...\n",
      "Epoch: 2298 \tTraining Loss: 0.867529 \tValidation Loss: 0.882609\n",
      "Validation loss decreased (0.882630 --> 0.882609).         Saving model ...\n",
      "Epoch: 2299 \tTraining Loss: 0.867506 \tValidation Loss: 0.882589\n",
      "Validation loss decreased (0.882609 --> 0.882589).         Saving model ...\n",
      "Epoch: 2300 \tTraining Loss: 0.867484 \tValidation Loss: 0.882569\n",
      "Validation loss decreased (0.882589 --> 0.882569).         Saving model ...\n",
      "Epoch: 2301 \tTraining Loss: 0.867462 \tValidation Loss: 0.882549\n",
      "Validation loss decreased (0.882569 --> 0.882549).         Saving model ...\n",
      "Epoch: 2302 \tTraining Loss: 0.867440 \tValidation Loss: 0.882529\n",
      "Validation loss decreased (0.882549 --> 0.882529).         Saving model ...\n",
      "Epoch: 2303 \tTraining Loss: 0.867418 \tValidation Loss: 0.882509\n",
      "Validation loss decreased (0.882529 --> 0.882509).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2304 \tTraining Loss: 0.867396 \tValidation Loss: 0.882489\n",
      "Validation loss decreased (0.882509 --> 0.882489).         Saving model ...\n",
      "Epoch: 2305 \tTraining Loss: 0.867373 \tValidation Loss: 0.882469\n",
      "Validation loss decreased (0.882489 --> 0.882469).         Saving model ...\n",
      "Epoch: 2306 \tTraining Loss: 0.867351 \tValidation Loss: 0.882449\n",
      "Validation loss decreased (0.882469 --> 0.882449).         Saving model ...\n",
      "Epoch: 2307 \tTraining Loss: 0.867329 \tValidation Loss: 0.882428\n",
      "Validation loss decreased (0.882449 --> 0.882428).         Saving model ...\n",
      "Epoch: 2308 \tTraining Loss: 0.867307 \tValidation Loss: 0.882408\n",
      "Validation loss decreased (0.882428 --> 0.882408).         Saving model ...\n",
      "Epoch: 2309 \tTraining Loss: 0.867285 \tValidation Loss: 0.882388\n",
      "Validation loss decreased (0.882408 --> 0.882388).         Saving model ...\n",
      "Epoch: 2310 \tTraining Loss: 0.867263 \tValidation Loss: 0.882368\n",
      "Validation loss decreased (0.882388 --> 0.882368).         Saving model ...\n",
      "Epoch: 2311 \tTraining Loss: 0.867241 \tValidation Loss: 0.882348\n",
      "Validation loss decreased (0.882368 --> 0.882348).         Saving model ...\n",
      "Epoch: 2312 \tTraining Loss: 0.867219 \tValidation Loss: 0.882328\n",
      "Validation loss decreased (0.882348 --> 0.882328).         Saving model ...\n",
      "Epoch: 2313 \tTraining Loss: 0.867197 \tValidation Loss: 0.882309\n",
      "Validation loss decreased (0.882328 --> 0.882309).         Saving model ...\n",
      "Epoch: 2314 \tTraining Loss: 0.867175 \tValidation Loss: 0.882288\n",
      "Validation loss decreased (0.882309 --> 0.882288).         Saving model ...\n",
      "Epoch: 2315 \tTraining Loss: 0.867153 \tValidation Loss: 0.882269\n",
      "Validation loss decreased (0.882288 --> 0.882269).         Saving model ...\n",
      "Epoch: 2316 \tTraining Loss: 0.867131 \tValidation Loss: 0.882249\n",
      "Validation loss decreased (0.882269 --> 0.882249).         Saving model ...\n",
      "Epoch: 2317 \tTraining Loss: 0.867109 \tValidation Loss: 0.882229\n",
      "Validation loss decreased (0.882249 --> 0.882229).         Saving model ...\n",
      "Epoch: 2318 \tTraining Loss: 0.867087 \tValidation Loss: 0.882209\n",
      "Validation loss decreased (0.882229 --> 0.882209).         Saving model ...\n",
      "Epoch: 2319 \tTraining Loss: 0.867065 \tValidation Loss: 0.882189\n",
      "Validation loss decreased (0.882209 --> 0.882189).         Saving model ...\n",
      "Epoch: 2320 \tTraining Loss: 0.867043 \tValidation Loss: 0.882169\n",
      "Validation loss decreased (0.882189 --> 0.882169).         Saving model ...\n",
      "Epoch: 2321 \tTraining Loss: 0.867021 \tValidation Loss: 0.882149\n",
      "Validation loss decreased (0.882169 --> 0.882149).         Saving model ...\n",
      "Epoch: 2322 \tTraining Loss: 0.866999 \tValidation Loss: 0.882129\n",
      "Validation loss decreased (0.882149 --> 0.882129).         Saving model ...\n",
      "Epoch: 2323 \tTraining Loss: 0.866977 \tValidation Loss: 0.882110\n",
      "Validation loss decreased (0.882129 --> 0.882110).         Saving model ...\n",
      "Epoch: 2324 \tTraining Loss: 0.866956 \tValidation Loss: 0.882090\n",
      "Validation loss decreased (0.882110 --> 0.882090).         Saving model ...\n",
      "Epoch: 2325 \tTraining Loss: 0.866934 \tValidation Loss: 0.882070\n",
      "Validation loss decreased (0.882090 --> 0.882070).         Saving model ...\n",
      "Epoch: 2326 \tTraining Loss: 0.866912 \tValidation Loss: 0.882050\n",
      "Validation loss decreased (0.882070 --> 0.882050).         Saving model ...\n",
      "Epoch: 2327 \tTraining Loss: 0.866890 \tValidation Loss: 0.882030\n",
      "Validation loss decreased (0.882050 --> 0.882030).         Saving model ...\n",
      "Epoch: 2328 \tTraining Loss: 0.866868 \tValidation Loss: 0.882010\n",
      "Validation loss decreased (0.882030 --> 0.882010).         Saving model ...\n",
      "Epoch: 2329 \tTraining Loss: 0.866846 \tValidation Loss: 0.881991\n",
      "Validation loss decreased (0.882010 --> 0.881991).         Saving model ...\n",
      "Epoch: 2330 \tTraining Loss: 0.866825 \tValidation Loss: 0.881971\n",
      "Validation loss decreased (0.881991 --> 0.881971).         Saving model ...\n",
      "Epoch: 2331 \tTraining Loss: 0.866803 \tValidation Loss: 0.881951\n",
      "Validation loss decreased (0.881971 --> 0.881951).         Saving model ...\n",
      "Epoch: 2332 \tTraining Loss: 0.866781 \tValidation Loss: 0.881931\n",
      "Validation loss decreased (0.881951 --> 0.881931).         Saving model ...\n",
      "Epoch: 2333 \tTraining Loss: 0.866759 \tValidation Loss: 0.881912\n",
      "Validation loss decreased (0.881931 --> 0.881912).         Saving model ...\n",
      "Epoch: 2334 \tTraining Loss: 0.866738 \tValidation Loss: 0.881892\n",
      "Validation loss decreased (0.881912 --> 0.881892).         Saving model ...\n",
      "Epoch: 2335 \tTraining Loss: 0.866716 \tValidation Loss: 0.881872\n",
      "Validation loss decreased (0.881892 --> 0.881872).         Saving model ...\n",
      "Epoch: 2336 \tTraining Loss: 0.866694 \tValidation Loss: 0.881853\n",
      "Validation loss decreased (0.881872 --> 0.881853).         Saving model ...\n",
      "Epoch: 2337 \tTraining Loss: 0.866672 \tValidation Loss: 0.881833\n",
      "Validation loss decreased (0.881853 --> 0.881833).         Saving model ...\n",
      "Epoch: 2338 \tTraining Loss: 0.866651 \tValidation Loss: 0.881813\n",
      "Validation loss decreased (0.881833 --> 0.881813).         Saving model ...\n",
      "Epoch: 2339 \tTraining Loss: 0.866629 \tValidation Loss: 0.881794\n",
      "Validation loss decreased (0.881813 --> 0.881794).         Saving model ...\n",
      "Epoch: 2340 \tTraining Loss: 0.866607 \tValidation Loss: 0.881774\n",
      "Validation loss decreased (0.881794 --> 0.881774).         Saving model ...\n",
      "Epoch: 2341 \tTraining Loss: 0.866586 \tValidation Loss: 0.881755\n",
      "Validation loss decreased (0.881774 --> 0.881755).         Saving model ...\n",
      "Epoch: 2342 \tTraining Loss: 0.866564 \tValidation Loss: 0.881735\n",
      "Validation loss decreased (0.881755 --> 0.881735).         Saving model ...\n",
      "Epoch: 2343 \tTraining Loss: 0.866542 \tValidation Loss: 0.881715\n",
      "Validation loss decreased (0.881735 --> 0.881715).         Saving model ...\n",
      "Epoch: 2344 \tTraining Loss: 0.866521 \tValidation Loss: 0.881696\n",
      "Validation loss decreased (0.881715 --> 0.881696).         Saving model ...\n",
      "Epoch: 2345 \tTraining Loss: 0.866499 \tValidation Loss: 0.881676\n",
      "Validation loss decreased (0.881696 --> 0.881676).         Saving model ...\n",
      "Epoch: 2346 \tTraining Loss: 0.866478 \tValidation Loss: 0.881657\n",
      "Validation loss decreased (0.881676 --> 0.881657).         Saving model ...\n",
      "Epoch: 2347 \tTraining Loss: 0.866456 \tValidation Loss: 0.881637\n",
      "Validation loss decreased (0.881657 --> 0.881637).         Saving model ...\n",
      "Epoch: 2348 \tTraining Loss: 0.866434 \tValidation Loss: 0.881618\n",
      "Validation loss decreased (0.881637 --> 0.881618).         Saving model ...\n",
      "Epoch: 2349 \tTraining Loss: 0.866413 \tValidation Loss: 0.881598\n",
      "Validation loss decreased (0.881618 --> 0.881598).         Saving model ...\n",
      "Epoch: 2350 \tTraining Loss: 0.866391 \tValidation Loss: 0.881579\n",
      "Validation loss decreased (0.881598 --> 0.881579).         Saving model ...\n",
      "Epoch: 2351 \tTraining Loss: 0.866370 \tValidation Loss: 0.881559\n",
      "Validation loss decreased (0.881579 --> 0.881559).         Saving model ...\n",
      "Epoch: 2352 \tTraining Loss: 0.866348 \tValidation Loss: 0.881540\n",
      "Validation loss decreased (0.881559 --> 0.881540).         Saving model ...\n",
      "Epoch: 2353 \tTraining Loss: 0.866327 \tValidation Loss: 0.881520\n",
      "Validation loss decreased (0.881540 --> 0.881520).         Saving model ...\n",
      "Epoch: 2354 \tTraining Loss: 0.866305 \tValidation Loss: 0.881501\n",
      "Validation loss decreased (0.881520 --> 0.881501).         Saving model ...\n",
      "Epoch: 2355 \tTraining Loss: 0.866284 \tValidation Loss: 0.881481\n",
      "Validation loss decreased (0.881501 --> 0.881481).         Saving model ...\n",
      "Epoch: 2356 \tTraining Loss: 0.866262 \tValidation Loss: 0.881462\n",
      "Validation loss decreased (0.881481 --> 0.881462).         Saving model ...\n",
      "Epoch: 2357 \tTraining Loss: 0.866241 \tValidation Loss: 0.881442\n",
      "Validation loss decreased (0.881462 --> 0.881442).         Saving model ...\n",
      "Epoch: 2358 \tTraining Loss: 0.866219 \tValidation Loss: 0.881423\n",
      "Validation loss decreased (0.881442 --> 0.881423).         Saving model ...\n",
      "Epoch: 2359 \tTraining Loss: 0.866198 \tValidation Loss: 0.881404\n",
      "Validation loss decreased (0.881423 --> 0.881404).         Saving model ...\n",
      "Epoch: 2360 \tTraining Loss: 0.866177 \tValidation Loss: 0.881384\n",
      "Validation loss decreased (0.881404 --> 0.881384).         Saving model ...\n",
      "Epoch: 2361 \tTraining Loss: 0.866155 \tValidation Loss: 0.881365\n",
      "Validation loss decreased (0.881384 --> 0.881365).         Saving model ...\n",
      "Epoch: 2362 \tTraining Loss: 0.866134 \tValidation Loss: 0.881346\n",
      "Validation loss decreased (0.881365 --> 0.881346).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2363 \tTraining Loss: 0.866112 \tValidation Loss: 0.881326\n",
      "Validation loss decreased (0.881346 --> 0.881326).         Saving model ...\n",
      "Epoch: 2364 \tTraining Loss: 0.866091 \tValidation Loss: 0.881307\n",
      "Validation loss decreased (0.881326 --> 0.881307).         Saving model ...\n",
      "Epoch: 2365 \tTraining Loss: 0.866070 \tValidation Loss: 0.881288\n",
      "Validation loss decreased (0.881307 --> 0.881288).         Saving model ...\n",
      "Epoch: 2366 \tTraining Loss: 0.866048 \tValidation Loss: 0.881268\n",
      "Validation loss decreased (0.881288 --> 0.881268).         Saving model ...\n",
      "Epoch: 2367 \tTraining Loss: 0.866027 \tValidation Loss: 0.881249\n",
      "Validation loss decreased (0.881268 --> 0.881249).         Saving model ...\n",
      "Epoch: 2368 \tTraining Loss: 0.866006 \tValidation Loss: 0.881230\n",
      "Validation loss decreased (0.881249 --> 0.881230).         Saving model ...\n",
      "Epoch: 2369 \tTraining Loss: 0.865984 \tValidation Loss: 0.881210\n",
      "Validation loss decreased (0.881230 --> 0.881210).         Saving model ...\n",
      "Epoch: 2370 \tTraining Loss: 0.865963 \tValidation Loss: 0.881191\n",
      "Validation loss decreased (0.881210 --> 0.881191).         Saving model ...\n",
      "Epoch: 2371 \tTraining Loss: 0.865942 \tValidation Loss: 0.881172\n",
      "Validation loss decreased (0.881191 --> 0.881172).         Saving model ...\n",
      "Epoch: 2372 \tTraining Loss: 0.865921 \tValidation Loss: 0.881153\n",
      "Validation loss decreased (0.881172 --> 0.881153).         Saving model ...\n",
      "Epoch: 2373 \tTraining Loss: 0.865899 \tValidation Loss: 0.881133\n",
      "Validation loss decreased (0.881153 --> 0.881133).         Saving model ...\n",
      "Epoch: 2374 \tTraining Loss: 0.865878 \tValidation Loss: 0.881114\n",
      "Validation loss decreased (0.881133 --> 0.881114).         Saving model ...\n",
      "Epoch: 2375 \tTraining Loss: 0.865857 \tValidation Loss: 0.881095\n",
      "Validation loss decreased (0.881114 --> 0.881095).         Saving model ...\n",
      "Epoch: 2376 \tTraining Loss: 0.865836 \tValidation Loss: 0.881076\n",
      "Validation loss decreased (0.881095 --> 0.881076).         Saving model ...\n",
      "Epoch: 2377 \tTraining Loss: 0.865814 \tValidation Loss: 0.881057\n",
      "Validation loss decreased (0.881076 --> 0.881057).         Saving model ...\n",
      "Epoch: 2378 \tTraining Loss: 0.865793 \tValidation Loss: 0.881038\n",
      "Validation loss decreased (0.881057 --> 0.881038).         Saving model ...\n",
      "Epoch: 2379 \tTraining Loss: 0.865772 \tValidation Loss: 0.881018\n",
      "Validation loss decreased (0.881038 --> 0.881018).         Saving model ...\n",
      "Epoch: 2380 \tTraining Loss: 0.865751 \tValidation Loss: 0.880999\n",
      "Validation loss decreased (0.881018 --> 0.880999).         Saving model ...\n",
      "Epoch: 2381 \tTraining Loss: 0.865730 \tValidation Loss: 0.880980\n",
      "Validation loss decreased (0.880999 --> 0.880980).         Saving model ...\n",
      "Epoch: 2382 \tTraining Loss: 0.865709 \tValidation Loss: 0.880961\n",
      "Validation loss decreased (0.880980 --> 0.880961).         Saving model ...\n",
      "Epoch: 2383 \tTraining Loss: 0.865687 \tValidation Loss: 0.880942\n",
      "Validation loss decreased (0.880961 --> 0.880942).         Saving model ...\n",
      "Epoch: 2384 \tTraining Loss: 0.865666 \tValidation Loss: 0.880923\n",
      "Validation loss decreased (0.880942 --> 0.880923).         Saving model ...\n",
      "Epoch: 2385 \tTraining Loss: 0.865645 \tValidation Loss: 0.880904\n",
      "Validation loss decreased (0.880923 --> 0.880904).         Saving model ...\n",
      "Epoch: 2386 \tTraining Loss: 0.865624 \tValidation Loss: 0.880885\n",
      "Validation loss decreased (0.880904 --> 0.880885).         Saving model ...\n",
      "Epoch: 2387 \tTraining Loss: 0.865603 \tValidation Loss: 0.880866\n",
      "Validation loss decreased (0.880885 --> 0.880866).         Saving model ...\n",
      "Epoch: 2388 \tTraining Loss: 0.865582 \tValidation Loss: 0.880847\n",
      "Validation loss decreased (0.880866 --> 0.880847).         Saving model ...\n",
      "Epoch: 2389 \tTraining Loss: 0.865561 \tValidation Loss: 0.880828\n",
      "Validation loss decreased (0.880847 --> 0.880828).         Saving model ...\n",
      "Epoch: 2390 \tTraining Loss: 0.865540 \tValidation Loss: 0.880809\n",
      "Validation loss decreased (0.880828 --> 0.880809).         Saving model ...\n",
      "Epoch: 2391 \tTraining Loss: 0.865519 \tValidation Loss: 0.880790\n",
      "Validation loss decreased (0.880809 --> 0.880790).         Saving model ...\n",
      "Epoch: 2392 \tTraining Loss: 0.865498 \tValidation Loss: 0.880771\n",
      "Validation loss decreased (0.880790 --> 0.880771).         Saving model ...\n",
      "Epoch: 2393 \tTraining Loss: 0.865477 \tValidation Loss: 0.880752\n",
      "Validation loss decreased (0.880771 --> 0.880752).         Saving model ...\n",
      "Epoch: 2394 \tTraining Loss: 0.865456 \tValidation Loss: 0.880733\n",
      "Validation loss decreased (0.880752 --> 0.880733).         Saving model ...\n",
      "Epoch: 2395 \tTraining Loss: 0.865435 \tValidation Loss: 0.880714\n",
      "Validation loss decreased (0.880733 --> 0.880714).         Saving model ...\n",
      "Epoch: 2396 \tTraining Loss: 0.865414 \tValidation Loss: 0.880695\n",
      "Validation loss decreased (0.880714 --> 0.880695).         Saving model ...\n",
      "Epoch: 2397 \tTraining Loss: 0.865393 \tValidation Loss: 0.880676\n",
      "Validation loss decreased (0.880695 --> 0.880676).         Saving model ...\n",
      "Epoch: 2398 \tTraining Loss: 0.865372 \tValidation Loss: 0.880657\n",
      "Validation loss decreased (0.880676 --> 0.880657).         Saving model ...\n",
      "Epoch: 2399 \tTraining Loss: 0.865351 \tValidation Loss: 0.880638\n",
      "Validation loss decreased (0.880657 --> 0.880638).         Saving model ...\n",
      "Epoch: 2400 \tTraining Loss: 0.865330 \tValidation Loss: 0.880619\n",
      "Validation loss decreased (0.880638 --> 0.880619).         Saving model ...\n",
      "Epoch: 2401 \tTraining Loss: 0.865309 \tValidation Loss: 0.880600\n",
      "Validation loss decreased (0.880619 --> 0.880600).         Saving model ...\n",
      "Epoch: 2402 \tTraining Loss: 0.865288 \tValidation Loss: 0.880581\n",
      "Validation loss decreased (0.880600 --> 0.880581).         Saving model ...\n",
      "Epoch: 2403 \tTraining Loss: 0.865267 \tValidation Loss: 0.880563\n",
      "Validation loss decreased (0.880581 --> 0.880563).         Saving model ...\n",
      "Epoch: 2404 \tTraining Loss: 0.865246 \tValidation Loss: 0.880544\n",
      "Validation loss decreased (0.880563 --> 0.880544).         Saving model ...\n",
      "Epoch: 2405 \tTraining Loss: 0.865226 \tValidation Loss: 0.880525\n",
      "Validation loss decreased (0.880544 --> 0.880525).         Saving model ...\n",
      "Epoch: 2406 \tTraining Loss: 0.865205 \tValidation Loss: 0.880506\n",
      "Validation loss decreased (0.880525 --> 0.880506).         Saving model ...\n",
      "Epoch: 2407 \tTraining Loss: 0.865184 \tValidation Loss: 0.880487\n",
      "Validation loss decreased (0.880506 --> 0.880487).         Saving model ...\n",
      "Epoch: 2408 \tTraining Loss: 0.865163 \tValidation Loss: 0.880468\n",
      "Validation loss decreased (0.880487 --> 0.880468).         Saving model ...\n",
      "Epoch: 2409 \tTraining Loss: 0.865142 \tValidation Loss: 0.880450\n",
      "Validation loss decreased (0.880468 --> 0.880450).         Saving model ...\n",
      "Epoch: 2410 \tTraining Loss: 0.865121 \tValidation Loss: 0.880431\n",
      "Validation loss decreased (0.880450 --> 0.880431).         Saving model ...\n",
      "Epoch: 2411 \tTraining Loss: 0.865101 \tValidation Loss: 0.880412\n",
      "Validation loss decreased (0.880431 --> 0.880412).         Saving model ...\n",
      "Epoch: 2412 \tTraining Loss: 0.865080 \tValidation Loss: 0.880393\n",
      "Validation loss decreased (0.880412 --> 0.880393).         Saving model ...\n",
      "Epoch: 2413 \tTraining Loss: 0.865059 \tValidation Loss: 0.880375\n",
      "Validation loss decreased (0.880393 --> 0.880375).         Saving model ...\n",
      "Epoch: 2414 \tTraining Loss: 0.865038 \tValidation Loss: 0.880356\n",
      "Validation loss decreased (0.880375 --> 0.880356).         Saving model ...\n",
      "Epoch: 2415 \tTraining Loss: 0.865018 \tValidation Loss: 0.880337\n",
      "Validation loss decreased (0.880356 --> 0.880337).         Saving model ...\n",
      "Epoch: 2416 \tTraining Loss: 0.864997 \tValidation Loss: 0.880318\n",
      "Validation loss decreased (0.880337 --> 0.880318).         Saving model ...\n",
      "Epoch: 2417 \tTraining Loss: 0.864976 \tValidation Loss: 0.880300\n",
      "Validation loss decreased (0.880318 --> 0.880300).         Saving model ...\n",
      "Epoch: 2418 \tTraining Loss: 0.864955 \tValidation Loss: 0.880281\n",
      "Validation loss decreased (0.880300 --> 0.880281).         Saving model ...\n",
      "Epoch: 2419 \tTraining Loss: 0.864935 \tValidation Loss: 0.880262\n",
      "Validation loss decreased (0.880281 --> 0.880262).         Saving model ...\n",
      "Epoch: 2420 \tTraining Loss: 0.864914 \tValidation Loss: 0.880244\n",
      "Validation loss decreased (0.880262 --> 0.880244).         Saving model ...\n",
      "Epoch: 2421 \tTraining Loss: 0.864893 \tValidation Loss: 0.880225\n",
      "Validation loss decreased (0.880244 --> 0.880225).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2422 \tTraining Loss: 0.864873 \tValidation Loss: 0.880206\n",
      "Validation loss decreased (0.880225 --> 0.880206).         Saving model ...\n",
      "Epoch: 2423 \tTraining Loss: 0.864852 \tValidation Loss: 0.880188\n",
      "Validation loss decreased (0.880206 --> 0.880188).         Saving model ...\n",
      "Epoch: 2424 \tTraining Loss: 0.864831 \tValidation Loss: 0.880169\n",
      "Validation loss decreased (0.880188 --> 0.880169).         Saving model ...\n",
      "Epoch: 2425 \tTraining Loss: 0.864811 \tValidation Loss: 0.880151\n",
      "Validation loss decreased (0.880169 --> 0.880151).         Saving model ...\n",
      "Epoch: 2426 \tTraining Loss: 0.864790 \tValidation Loss: 0.880132\n",
      "Validation loss decreased (0.880151 --> 0.880132).         Saving model ...\n",
      "Epoch: 2427 \tTraining Loss: 0.864770 \tValidation Loss: 0.880113\n",
      "Validation loss decreased (0.880132 --> 0.880113).         Saving model ...\n",
      "Epoch: 2428 \tTraining Loss: 0.864749 \tValidation Loss: 0.880095\n",
      "Validation loss decreased (0.880113 --> 0.880095).         Saving model ...\n",
      "Epoch: 2429 \tTraining Loss: 0.864728 \tValidation Loss: 0.880076\n",
      "Validation loss decreased (0.880095 --> 0.880076).         Saving model ...\n",
      "Epoch: 2430 \tTraining Loss: 0.864708 \tValidation Loss: 0.880058\n",
      "Validation loss decreased (0.880076 --> 0.880058).         Saving model ...\n",
      "Epoch: 2431 \tTraining Loss: 0.864687 \tValidation Loss: 0.880039\n",
      "Validation loss decreased (0.880058 --> 0.880039).         Saving model ...\n",
      "Epoch: 2432 \tTraining Loss: 0.864667 \tValidation Loss: 0.880021\n",
      "Validation loss decreased (0.880039 --> 0.880021).         Saving model ...\n",
      "Epoch: 2433 \tTraining Loss: 0.864646 \tValidation Loss: 0.880002\n",
      "Validation loss decreased (0.880021 --> 0.880002).         Saving model ...\n",
      "Epoch: 2434 \tTraining Loss: 0.864626 \tValidation Loss: 0.879984\n",
      "Validation loss decreased (0.880002 --> 0.879984).         Saving model ...\n",
      "Epoch: 2435 \tTraining Loss: 0.864605 \tValidation Loss: 0.879965\n",
      "Validation loss decreased (0.879984 --> 0.879965).         Saving model ...\n",
      "Epoch: 2436 \tTraining Loss: 0.864585 \tValidation Loss: 0.879947\n",
      "Validation loss decreased (0.879965 --> 0.879947).         Saving model ...\n",
      "Epoch: 2437 \tTraining Loss: 0.864564 \tValidation Loss: 0.879928\n",
      "Validation loss decreased (0.879947 --> 0.879928).         Saving model ...\n",
      "Epoch: 2438 \tTraining Loss: 0.864544 \tValidation Loss: 0.879910\n",
      "Validation loss decreased (0.879928 --> 0.879910).         Saving model ...\n",
      "Epoch: 2439 \tTraining Loss: 0.864523 \tValidation Loss: 0.879891\n",
      "Validation loss decreased (0.879910 --> 0.879891).         Saving model ...\n",
      "Epoch: 2440 \tTraining Loss: 0.864503 \tValidation Loss: 0.879873\n",
      "Validation loss decreased (0.879891 --> 0.879873).         Saving model ...\n",
      "Epoch: 2441 \tTraining Loss: 0.864482 \tValidation Loss: 0.879855\n",
      "Validation loss decreased (0.879873 --> 0.879855).         Saving model ...\n",
      "Epoch: 2442 \tTraining Loss: 0.864462 \tValidation Loss: 0.879836\n",
      "Validation loss decreased (0.879855 --> 0.879836).         Saving model ...\n",
      "Epoch: 2443 \tTraining Loss: 0.864442 \tValidation Loss: 0.879818\n",
      "Validation loss decreased (0.879836 --> 0.879818).         Saving model ...\n",
      "Epoch: 2444 \tTraining Loss: 0.864421 \tValidation Loss: 0.879799\n",
      "Validation loss decreased (0.879818 --> 0.879799).         Saving model ...\n",
      "Epoch: 2445 \tTraining Loss: 0.864401 \tValidation Loss: 0.879781\n",
      "Validation loss decreased (0.879799 --> 0.879781).         Saving model ...\n",
      "Epoch: 2446 \tTraining Loss: 0.864380 \tValidation Loss: 0.879763\n",
      "Validation loss decreased (0.879781 --> 0.879763).         Saving model ...\n",
      "Epoch: 2447 \tTraining Loss: 0.864360 \tValidation Loss: 0.879744\n",
      "Validation loss decreased (0.879763 --> 0.879744).         Saving model ...\n",
      "Epoch: 2448 \tTraining Loss: 0.864340 \tValidation Loss: 0.879726\n",
      "Validation loss decreased (0.879744 --> 0.879726).         Saving model ...\n",
      "Epoch: 2449 \tTraining Loss: 0.864319 \tValidation Loss: 0.879707\n",
      "Validation loss decreased (0.879726 --> 0.879707).         Saving model ...\n",
      "Epoch: 2450 \tTraining Loss: 0.864299 \tValidation Loss: 0.879689\n",
      "Validation loss decreased (0.879707 --> 0.879689).         Saving model ...\n",
      "Epoch: 2451 \tTraining Loss: 0.864279 \tValidation Loss: 0.879671\n",
      "Validation loss decreased (0.879689 --> 0.879671).         Saving model ...\n",
      "Epoch: 2452 \tTraining Loss: 0.864258 \tValidation Loss: 0.879653\n",
      "Validation loss decreased (0.879671 --> 0.879653).         Saving model ...\n",
      "Epoch: 2453 \tTraining Loss: 0.864238 \tValidation Loss: 0.879634\n",
      "Validation loss decreased (0.879653 --> 0.879634).         Saving model ...\n",
      "Epoch: 2454 \tTraining Loss: 0.864218 \tValidation Loss: 0.879616\n",
      "Validation loss decreased (0.879634 --> 0.879616).         Saving model ...\n",
      "Epoch: 2455 \tTraining Loss: 0.864197 \tValidation Loss: 0.879598\n",
      "Validation loss decreased (0.879616 --> 0.879598).         Saving model ...\n",
      "Epoch: 2456 \tTraining Loss: 0.864177 \tValidation Loss: 0.879579\n",
      "Validation loss decreased (0.879598 --> 0.879579).         Saving model ...\n",
      "Epoch: 2457 \tTraining Loss: 0.864157 \tValidation Loss: 0.879561\n",
      "Validation loss decreased (0.879579 --> 0.879561).         Saving model ...\n",
      "Epoch: 2458 \tTraining Loss: 0.864137 \tValidation Loss: 0.879543\n",
      "Validation loss decreased (0.879561 --> 0.879543).         Saving model ...\n",
      "Epoch: 2459 \tTraining Loss: 0.864116 \tValidation Loss: 0.879525\n",
      "Validation loss decreased (0.879543 --> 0.879525).         Saving model ...\n",
      "Epoch: 2460 \tTraining Loss: 0.864096 \tValidation Loss: 0.879507\n",
      "Validation loss decreased (0.879525 --> 0.879507).         Saving model ...\n",
      "Epoch: 2461 \tTraining Loss: 0.864076 \tValidation Loss: 0.879488\n",
      "Validation loss decreased (0.879507 --> 0.879488).         Saving model ...\n",
      "Epoch: 2462 \tTraining Loss: 0.864056 \tValidation Loss: 0.879470\n",
      "Validation loss decreased (0.879488 --> 0.879470).         Saving model ...\n",
      "Epoch: 2463 \tTraining Loss: 0.864036 \tValidation Loss: 0.879452\n",
      "Validation loss decreased (0.879470 --> 0.879452).         Saving model ...\n",
      "Epoch: 2464 \tTraining Loss: 0.864015 \tValidation Loss: 0.879434\n",
      "Validation loss decreased (0.879452 --> 0.879434).         Saving model ...\n",
      "Epoch: 2465 \tTraining Loss: 0.863995 \tValidation Loss: 0.879416\n",
      "Validation loss decreased (0.879434 --> 0.879416).         Saving model ...\n",
      "Epoch: 2466 \tTraining Loss: 0.863975 \tValidation Loss: 0.879398\n",
      "Validation loss decreased (0.879416 --> 0.879398).         Saving model ...\n",
      "Epoch: 2467 \tTraining Loss: 0.863955 \tValidation Loss: 0.879379\n",
      "Validation loss decreased (0.879398 --> 0.879379).         Saving model ...\n",
      "Epoch: 2468 \tTraining Loss: 0.863935 \tValidation Loss: 0.879361\n",
      "Validation loss decreased (0.879379 --> 0.879361).         Saving model ...\n",
      "Epoch: 2469 \tTraining Loss: 0.863915 \tValidation Loss: 0.879343\n",
      "Validation loss decreased (0.879361 --> 0.879343).         Saving model ...\n",
      "Epoch: 2470 \tTraining Loss: 0.863895 \tValidation Loss: 0.879325\n",
      "Validation loss decreased (0.879343 --> 0.879325).         Saving model ...\n",
      "Epoch: 2471 \tTraining Loss: 0.863875 \tValidation Loss: 0.879307\n",
      "Validation loss decreased (0.879325 --> 0.879307).         Saving model ...\n",
      "Epoch: 2472 \tTraining Loss: 0.863854 \tValidation Loss: 0.879289\n",
      "Validation loss decreased (0.879307 --> 0.879289).         Saving model ...\n",
      "Epoch: 2473 \tTraining Loss: 0.863834 \tValidation Loss: 0.879271\n",
      "Validation loss decreased (0.879289 --> 0.879271).         Saving model ...\n",
      "Epoch: 2474 \tTraining Loss: 0.863814 \tValidation Loss: 0.879253\n",
      "Validation loss decreased (0.879271 --> 0.879253).         Saving model ...\n",
      "Epoch: 2475 \tTraining Loss: 0.863794 \tValidation Loss: 0.879235\n",
      "Validation loss decreased (0.879253 --> 0.879235).         Saving model ...\n",
      "Epoch: 2476 \tTraining Loss: 0.863774 \tValidation Loss: 0.879217\n",
      "Validation loss decreased (0.879235 --> 0.879217).         Saving model ...\n",
      "Epoch: 2477 \tTraining Loss: 0.863754 \tValidation Loss: 0.879199\n",
      "Validation loss decreased (0.879217 --> 0.879199).         Saving model ...\n",
      "Epoch: 2478 \tTraining Loss: 0.863734 \tValidation Loss: 0.879181\n",
      "Validation loss decreased (0.879199 --> 0.879181).         Saving model ...\n",
      "Epoch: 2479 \tTraining Loss: 0.863714 \tValidation Loss: 0.879163\n",
      "Validation loss decreased (0.879181 --> 0.879163).         Saving model ...\n",
      "Epoch: 2480 \tTraining Loss: 0.863694 \tValidation Loss: 0.879145\n",
      "Validation loss decreased (0.879163 --> 0.879145).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2481 \tTraining Loss: 0.863674 \tValidation Loss: 0.879127\n",
      "Validation loss decreased (0.879145 --> 0.879127).         Saving model ...\n",
      "Epoch: 2482 \tTraining Loss: 0.863654 \tValidation Loss: 0.879109\n",
      "Validation loss decreased (0.879127 --> 0.879109).         Saving model ...\n",
      "Epoch: 2483 \tTraining Loss: 0.863634 \tValidation Loss: 0.879091\n",
      "Validation loss decreased (0.879109 --> 0.879091).         Saving model ...\n",
      "Epoch: 2484 \tTraining Loss: 0.863614 \tValidation Loss: 0.879073\n",
      "Validation loss decreased (0.879091 --> 0.879073).         Saving model ...\n",
      "Epoch: 2485 \tTraining Loss: 0.863594 \tValidation Loss: 0.879055\n",
      "Validation loss decreased (0.879073 --> 0.879055).         Saving model ...\n",
      "Epoch: 2486 \tTraining Loss: 0.863574 \tValidation Loss: 0.879037\n",
      "Validation loss decreased (0.879055 --> 0.879037).         Saving model ...\n",
      "Epoch: 2487 \tTraining Loss: 0.863555 \tValidation Loss: 0.879019\n",
      "Validation loss decreased (0.879037 --> 0.879019).         Saving model ...\n",
      "Epoch: 2488 \tTraining Loss: 0.863535 \tValidation Loss: 0.879001\n",
      "Validation loss decreased (0.879019 --> 0.879001).         Saving model ...\n",
      "Epoch: 2489 \tTraining Loss: 0.863515 \tValidation Loss: 0.878983\n",
      "Validation loss decreased (0.879001 --> 0.878983).         Saving model ...\n",
      "Epoch: 2490 \tTraining Loss: 0.863495 \tValidation Loss: 0.878965\n",
      "Validation loss decreased (0.878983 --> 0.878965).         Saving model ...\n",
      "Epoch: 2491 \tTraining Loss: 0.863475 \tValidation Loss: 0.878948\n",
      "Validation loss decreased (0.878965 --> 0.878948).         Saving model ...\n",
      "Epoch: 2492 \tTraining Loss: 0.863455 \tValidation Loss: 0.878930\n",
      "Validation loss decreased (0.878948 --> 0.878930).         Saving model ...\n",
      "Epoch: 2493 \tTraining Loss: 0.863435 \tValidation Loss: 0.878912\n",
      "Validation loss decreased (0.878930 --> 0.878912).         Saving model ...\n",
      "Epoch: 2494 \tTraining Loss: 0.863415 \tValidation Loss: 0.878894\n",
      "Validation loss decreased (0.878912 --> 0.878894).         Saving model ...\n",
      "Epoch: 2495 \tTraining Loss: 0.863396 \tValidation Loss: 0.878876\n",
      "Validation loss decreased (0.878894 --> 0.878876).         Saving model ...\n",
      "Epoch: 2496 \tTraining Loss: 0.863376 \tValidation Loss: 0.878858\n",
      "Validation loss decreased (0.878876 --> 0.878858).         Saving model ...\n",
      "Epoch: 2497 \tTraining Loss: 0.863356 \tValidation Loss: 0.878841\n",
      "Validation loss decreased (0.878858 --> 0.878841).         Saving model ...\n",
      "Epoch: 2498 \tTraining Loss: 0.863336 \tValidation Loss: 0.878823\n",
      "Validation loss decreased (0.878841 --> 0.878823).         Saving model ...\n",
      "Epoch: 2499 \tTraining Loss: 0.863316 \tValidation Loss: 0.878805\n",
      "Validation loss decreased (0.878823 --> 0.878805).         Saving model ...\n",
      "Epoch: 2500 \tTraining Loss: 0.863297 \tValidation Loss: 0.878787\n",
      "Validation loss decreased (0.878805 --> 0.878787).         Saving model ...\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 2500\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    linear_model.train() # prep model for training\n",
    "    for data, target in train_loader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = linear_model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    linear_model.eval() # prep model for evaluation\n",
    "    for data, target in test_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = linear_model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(test_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}' \\\n",
    "          .format(epoch+1, train_loss, valid_loss))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}). \\\n",
    "        Saving model ...'.format(valid_loss_min, valid_loss))\n",
    "        \n",
    "        torch.save(linear_model.state_dict(), 'bert_model_linear.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3ff6d4a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.77      0.74     13205\n",
      "           1       0.02      0.30      0.04        47\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.37      0.77      0.50       251\n",
      "           4       0.01      0.54      0.01        13\n",
      "           5       0.33      0.50      0.40      6994\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       0.22      0.55      0.31       274\n",
      "           8       0.00      0.00      0.00         0\n",
      "           9       0.00      0.00      0.00         0\n",
      "          10       0.95      0.74      0.83     46896\n",
      "          11       0.01      0.23      0.01        22\n",
      "          12       0.62      0.73      0.67      3795\n",
      "          13       0.00      0.00      0.00         0\n",
      "          14       0.00      0.00      0.00         1\n",
      "          15       0.38      0.65      0.48       580\n",
      "          16       0.31      0.60      0.40       637\n",
      "          17       0.03      0.56      0.06        16\n",
      "          18       0.00      0.00      0.00         0\n",
      "          19       0.56      0.82      0.66      1665\n",
      "          20       0.01      0.67      0.01         9\n",
      "          21       0.01      1.00      0.02         2\n",
      "          22       0.00      0.00      0.00         0\n",
      "          23       0.71      0.76      0.73      1090\n",
      "          24       0.00      0.50      0.00         2\n",
      "          25       0.00      0.00      0.00         0\n",
      "          26       0.88      0.54      0.67      2134\n",
      "          27       0.00      0.00      0.00         0\n",
      "          28       0.01      0.70      0.02        47\n",
      "          29       0.00      0.00      0.00         0\n",
      "          30       0.14      0.47      0.21       156\n",
      "          31       0.57      0.79      0.66       456\n",
      "          32       0.37      0.63      0.47       121\n",
      "          33       0.84      0.69      0.76      2285\n",
      "          34       0.00      0.00      0.00         0\n",
      "          35       0.73      0.75      0.74      4384\n",
      "          36       0.04      0.34      0.07       129\n",
      "          37       0.88      0.78      0.83     80170\n",
      "          38       0.61      0.62      0.61     24372\n",
      "          39       0.00      0.00      0.00         0\n",
      "          40       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.73    189753\n",
      "   macro avg       0.25      0.41      0.27    189753\n",
      "weighted avg       0.81      0.73      0.76    189753\n",
      "\n",
      "Testing Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.75      0.73       719\n",
      "           1       0.02      0.33      0.05         3\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.39      0.71      0.50        17\n",
      "           4       0.00      0.00      0.00         0\n",
      "           5       0.36      0.49      0.41       424\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       0.30      0.65      0.41        20\n",
      "           8       0.00      0.00      0.00         0\n",
      "           9       0.00      0.00      0.00         0\n",
      "          10       0.94      0.74      0.83      2419\n",
      "          11       0.00      0.00      0.00         1\n",
      "          12       0.57      0.72      0.63       194\n",
      "          13       0.00      0.00      0.00         0\n",
      "          14       0.00      0.00      0.00         0\n",
      "          15       0.34      0.58      0.43        31\n",
      "          16       0.34      0.58      0.43        36\n",
      "          17       0.00      0.00      0.00         0\n",
      "          18       0.00      0.00      0.00         0\n",
      "          19       0.49      0.75      0.59        79\n",
      "          20       0.00      0.00      0.00         0\n",
      "          21       0.00      0.00      0.00         0\n",
      "          22       0.00      0.00      0.00         0\n",
      "          23       0.71      0.78      0.75        64\n",
      "          24       0.00      0.00      0.00         1\n",
      "          25       0.00      0.00      0.00         0\n",
      "          26       0.87      0.59      0.70       104\n",
      "          27       0.00      0.00      0.00         0\n",
      "          28       0.01      0.50      0.03         4\n",
      "          29       0.00      0.00      0.00         0\n",
      "          30       0.21      0.46      0.29        13\n",
      "          31       0.41      0.69      0.51        16\n",
      "          32       0.20      0.50      0.29         2\n",
      "          33       0.84      0.70      0.76       115\n",
      "          34       0.00      0.00      0.00         0\n",
      "          35       0.72      0.72      0.72       224\n",
      "          36       0.02      0.33      0.03         3\n",
      "          37       0.88      0.79      0.83      4212\n",
      "          38       0.61      0.62      0.62      1286\n",
      "          39       0.00      0.00      0.00         0\n",
      "          40       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.73      9987\n",
      "   macro avg       0.24      0.32      0.26      9987\n",
      "weighted avg       0.80      0.73      0.76      9987\n",
      "\n"
     ]
    }
   ],
   "source": [
    "linear_model = Net()\n",
    "linear_model.load_state_dict(torch.load(\"bert_model_linear.pt\"))\n",
    "linear_model.eval()\n",
    "\n",
    "pred_list = torch.zeros(0, dtype=torch.long)\n",
    "target_list = torch.zeros(0, dtype=torch.long)\n",
    "train_count = 0\n",
    "count = 0\n",
    "\n",
    "train_pred_list = torch.zeros(0, dtype=torch.long)\n",
    "train_target_list = torch.zeros(0, dtype=torch.long)\n",
    "\n",
    "for data, target in train_loader:\n",
    "    output = linear_model(data)\n",
    "    _, preds = torch.max(output, 1)\n",
    "    train_pred_list = torch.cat([train_pred_list, preds.view(-1)])\n",
    "    train_target_list = torch.cat([train_target_list, target.view(-1)])\n",
    "#     for x in range(16):\n",
    "#         if preds[x] == target[x]: \n",
    "#             train_count+=1\n",
    "            \n",
    "# print(\"Training Accuracy =\", train_count / len(train_loader.dataset))\n",
    "train_result = classification_report(train_pred_list.numpy(), \n",
    "                                     train_target_list.numpy())\n",
    "print(\"Training Classification report: \\n\", train_result)\n",
    "            \n",
    "for data, target in test_loader:\n",
    "    output = linear_model(data)\n",
    "    _, preds = torch.max(output, 1) \n",
    "    pred_list = torch.cat([pred_list, preds.view(-1)])\n",
    "    target_list = torch.cat([target_list, target.view(-1)])\n",
    "#     for x in range(16):\n",
    "#         if preds[x] == target[x]: \n",
    "#             count+=1\n",
    "\n",
    "# print(\"Testing Accuracy =\", count / len(test_loader.dataset))\n",
    "\n",
    "result = classification_report(pred_list.numpy(), target_list.numpy())\n",
    "print(\"Testing Classification report: \\n\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "994588a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.47      0.47        19\n",
      "           1       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         0\n",
      "           5       0.40      0.40      0.40         5\n",
      "           7       0.25      0.50      0.33         2\n",
      "          10       0.67      0.67      0.67         6\n",
      "          11       0.00      0.00      0.00         0\n",
      "          12       0.67      0.67      0.67         9\n",
      "          14       0.00      0.00      0.00         0\n",
      "          15       0.00      0.00      0.00         1\n",
      "          16       0.50      0.50      0.50         2\n",
      "          18       0.00      0.00      0.00         0\n",
      "          19       0.67      0.67      0.67         3\n",
      "          20       0.00      0.00      0.00         0\n",
      "          21       0.00      0.00      0.00         0\n",
      "          23       0.33      1.00      0.50         1\n",
      "          24       0.00      0.00      0.00         0\n",
      "          25       0.00      0.00      0.00         0\n",
      "          26       0.00      0.00      0.00         1\n",
      "          27       0.00      0.00      0.00         0\n",
      "          28       1.00      1.00      1.00         1\n",
      "          30       0.00      0.00      0.00         0\n",
      "          31       0.50      0.50      0.50         2\n",
      "          32       0.00      0.00      0.00         1\n",
      "          33       0.90      0.90      0.90        10\n",
      "          35       0.72      0.68      0.70        19\n",
      "          36       0.00      0.00      0.00         1\n",
      "          37       0.88      0.70      0.78       204\n",
      "          38       0.49      0.61      0.54        46\n",
      "          39       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.66       333\n",
      "   macro avg       0.27      0.30      0.28       333\n",
      "weighted avg       0.76      0.66      0.70       333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "linear_model = Net()\n",
    "linear_model.load_state_dict(torch.load(\"bert_model_linear.pt\"))\n",
    "linear_model.eval()\n",
    "\n",
    "pred_list = torch.zeros(0, dtype=torch.long)\n",
    "target_list = torch.zeros(0, dtype=torch.long)\n",
    "train_count = 0\n",
    "count = 0\n",
    "output_proba = False\n",
    "\n",
    "train_pred_list = torch.zeros(0, dtype=torch.long)\n",
    "train_target_list = torch.zeros(0, dtype=torch.long)\n",
    "            \n",
    "for data, target in test_audio_loader:\n",
    "    output = linear_model(data)\n",
    "    output_np = output.cpu().detach().numpy()\n",
    "    if np.any(output_proba):\n",
    "        output_proba = np.concatenate((output_proba, output_np), axis = 0)\n",
    "    else:\n",
    "        output_proba = output_np\n",
    "    _, preds = torch.max(output, 1) \n",
    "    pred_list = torch.cat([pred_list, preds.view(-1)])\n",
    "    target_list = torch.cat([target_list, target.view(-1)])\n",
    "#     for x in range(16):\n",
    "#         if preds[x] == target[x]: \n",
    "#             count+=1\n",
    "\n",
    "# print(\"Testing Accuracy =\", count / len(test_loader.dataset))\n",
    "\n",
    "result = classification_report(pred_list.numpy(), target_list.numpy())\n",
    "print(\"Testing Classification report: \\n\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f4c7dd94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(333, 41)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_proba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "57d610ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_audio_probabilities.npy', 'wb') as f:\n",
    "    np.save(f, output_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2559ac36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(333, 41)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_proba = np.load('test_audio_probabilities.npy', allow_pickle = True)\n",
    "output_proba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e704f725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module): \n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__() \n",
    "#         self.gru = nn.GRU(input_size=768, hidden_size=32, batch_first=True) \n",
    "        self.fc1 = nn.Linear(768,256)\n",
    "        self.fc2 = nn.Linear(256, 42)\n",
    "#         self.fc3 = nn.Linear(128, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "#         self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "#         out, hn = self.gru(x, torch.randn(1, len(x), 32))\n",
    "#         out = self.fc1(self.relu(x))\n",
    "#         out = self.fc2(self.relu(out))\n",
    "        out = self.relu(self.fc1(x))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "919f2cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=768, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=42, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "two_layer = Net()\n",
    "print(two_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9de5e06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer = torch.optim.SGD(two_layer.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5e0b1725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 2.169632 \tValidation Loss: 2.102942\n",
      "Validation loss decreased (inf --> 2.102942).         Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 2.101434 \tValidation Loss: 2.081255\n",
      "Validation loss decreased (2.102942 --> 2.081255).         Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 2.068668 \tValidation Loss: 2.036421\n",
      "Validation loss decreased (2.081255 --> 2.036421).         Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 2.003332 \tValidation Loss: 1.957762\n",
      "Validation loss decreased (2.036421 --> 1.957762).         Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 1.906732 \tValidation Loss: 1.865701\n",
      "Validation loss decreased (1.957762 --> 1.865701).         Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 1.812231 \tValidation Loss: 1.785919\n",
      "Validation loss decreased (1.865701 --> 1.785919).         Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 1.747003 \tValidation Loss: 1.703382\n",
      "Validation loss decreased (1.785919 --> 1.703382).         Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 1.704493 \tValidation Loss: 1.655697\n",
      "Validation loss decreased (1.703382 --> 1.655697).         Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 1.667531 \tValidation Loss: 1.624703\n",
      "Validation loss decreased (1.655697 --> 1.624703).         Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 1.636615 \tValidation Loss: 1.601311\n",
      "Validation loss decreased (1.624703 --> 1.601311).         Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 1.611939 \tValidation Loss: 1.582741\n",
      "Validation loss decreased (1.601311 --> 1.582741).         Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 1.592090 \tValidation Loss: 1.567606\n",
      "Validation loss decreased (1.582741 --> 1.567606).         Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 1.575481 \tValidation Loss: 1.554493\n",
      "Validation loss decreased (1.567606 --> 1.554493).         Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 1.561138 \tValidation Loss: 1.543752\n",
      "Validation loss decreased (1.554493 --> 1.543752).         Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 1.548473 \tValidation Loss: 1.534388\n",
      "Validation loss decreased (1.543752 --> 1.534388).         Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 1.536977 \tValidation Loss: 1.525995\n",
      "Validation loss decreased (1.534388 --> 1.525995).         Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 1.526321 \tValidation Loss: 1.518081\n",
      "Validation loss decreased (1.525995 --> 1.518081).         Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 1.516384 \tValidation Loss: 1.510694\n",
      "Validation loss decreased (1.518081 --> 1.510694).         Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 1.506876 \tValidation Loss: 1.503265\n",
      "Validation loss decreased (1.510694 --> 1.503265).         Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 1.497749 \tValidation Loss: 1.496068\n",
      "Validation loss decreased (1.503265 --> 1.496068).         Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 1.488935 \tValidation Loss: 1.489010\n",
      "Validation loss decreased (1.496068 --> 1.489010).         Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 1.480511 \tValidation Loss: 1.481770\n",
      "Validation loss decreased (1.489010 --> 1.481770).         Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 1.472242 \tValidation Loss: 1.476331\n",
      "Validation loss decreased (1.481770 --> 1.476331).         Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 1.464390 \tValidation Loss: 1.469405\n",
      "Validation loss decreased (1.476331 --> 1.469405).         Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 1.456796 \tValidation Loss: 1.462270\n",
      "Validation loss decreased (1.469405 --> 1.462270).         Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 1.449598 \tValidation Loss: 1.455322\n",
      "Validation loss decreased (1.462270 --> 1.455322).         Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 1.442531 \tValidation Loss: 1.448366\n",
      "Validation loss decreased (1.455322 --> 1.448366).         Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 1.435893 \tValidation Loss: 1.442617\n",
      "Validation loss decreased (1.448366 --> 1.442617).         Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 1.429618 \tValidation Loss: 1.436055\n",
      "Validation loss decreased (1.442617 --> 1.436055).         Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 1.423544 \tValidation Loss: 1.429568\n",
      "Validation loss decreased (1.436055 --> 1.429568).         Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 1.417697 \tValidation Loss: 1.425492\n",
      "Validation loss decreased (1.429568 --> 1.425492).         Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 1.412044 \tValidation Loss: 1.418519\n",
      "Validation loss decreased (1.425492 --> 1.418519).         Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 1.406318 \tValidation Loss: 1.414672\n",
      "Validation loss decreased (1.418519 --> 1.414672).         Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 1.400559 \tValidation Loss: 1.410898\n",
      "Validation loss decreased (1.414672 --> 1.410898).         Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 1.395038 \tValidation Loss: 1.405836\n",
      "Validation loss decreased (1.410898 --> 1.405836).         Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 1.389585 \tValidation Loss: 1.402105\n",
      "Validation loss decreased (1.405836 --> 1.402105).         Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 1.383925 \tValidation Loss: 1.398375\n",
      "Validation loss decreased (1.402105 --> 1.398375).         Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 1.378282 \tValidation Loss: 1.393834\n",
      "Validation loss decreased (1.398375 --> 1.393834).         Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 1.372598 \tValidation Loss: 1.389193\n",
      "Validation loss decreased (1.393834 --> 1.389193).         Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 1.366976 \tValidation Loss: 1.384650\n",
      "Validation loss decreased (1.389193 --> 1.384650).         Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 1.361289 \tValidation Loss: 1.379453\n",
      "Validation loss decreased (1.384650 --> 1.379453).         Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 1.355579 \tValidation Loss: 1.375104\n",
      "Validation loss decreased (1.379453 --> 1.375104).         Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 1.349801 \tValidation Loss: 1.370321\n",
      "Validation loss decreased (1.375104 --> 1.370321).         Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 1.344139 \tValidation Loss: 1.365411\n",
      "Validation loss decreased (1.370321 --> 1.365411).         Saving model ...\n",
      "Epoch: 45 \tTraining Loss: 1.338304 \tValidation Loss: 1.361491\n",
      "Validation loss decreased (1.365411 --> 1.361491).         Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 1.332551 \tValidation Loss: 1.356685\n",
      "Validation loss decreased (1.361491 --> 1.356685).         Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 1.326770 \tValidation Loss: 1.352523\n",
      "Validation loss decreased (1.356685 --> 1.352523).         Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 1.320959 \tValidation Loss: 1.345361\n",
      "Validation loss decreased (1.352523 --> 1.345361).         Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 1.315269 \tValidation Loss: 1.340241\n",
      "Validation loss decreased (1.345361 --> 1.340241).         Saving model ...\n",
      "Epoch: 50 \tTraining Loss: 1.309570 \tValidation Loss: 1.335716\n",
      "Validation loss decreased (1.340241 --> 1.335716).         Saving model ...\n",
      "Epoch: 51 \tTraining Loss: 1.303946 \tValidation Loss: 1.331216\n",
      "Validation loss decreased (1.335716 --> 1.331216).         Saving model ...\n",
      "Epoch: 52 \tTraining Loss: 1.298439 \tValidation Loss: 1.327026\n",
      "Validation loss decreased (1.331216 --> 1.327026).         Saving model ...\n",
      "Epoch: 53 \tTraining Loss: 1.292893 \tValidation Loss: 1.321864\n",
      "Validation loss decreased (1.327026 --> 1.321864).         Saving model ...\n",
      "Epoch: 54 \tTraining Loss: 1.287507 \tValidation Loss: 1.318132\n",
      "Validation loss decreased (1.321864 --> 1.318132).         Saving model ...\n",
      "Epoch: 55 \tTraining Loss: 1.282166 \tValidation Loss: 1.313607\n",
      "Validation loss decreased (1.318132 --> 1.313607).         Saving model ...\n",
      "Epoch: 56 \tTraining Loss: 1.276963 \tValidation Loss: 1.308915\n",
      "Validation loss decreased (1.313607 --> 1.308915).         Saving model ...\n",
      "Epoch: 57 \tTraining Loss: 1.271826 \tValidation Loss: 1.303774\n",
      "Validation loss decreased (1.308915 --> 1.303774).         Saving model ...\n",
      "Epoch: 58 \tTraining Loss: 1.266750 \tValidation Loss: 1.297788\n",
      "Validation loss decreased (1.303774 --> 1.297788).         Saving model ...\n",
      "Epoch: 59 \tTraining Loss: 1.261795 \tValidation Loss: 1.293573\n",
      "Validation loss decreased (1.297788 --> 1.293573).         Saving model ...\n",
      "Epoch: 60 \tTraining Loss: 1.257042 \tValidation Loss: 1.289096\n",
      "Validation loss decreased (1.293573 --> 1.289096).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61 \tTraining Loss: 1.252388 \tValidation Loss: 1.284673\n",
      "Validation loss decreased (1.289096 --> 1.284673).         Saving model ...\n",
      "Epoch: 62 \tTraining Loss: 1.247852 \tValidation Loss: 1.280270\n",
      "Validation loss decreased (1.284673 --> 1.280270).         Saving model ...\n",
      "Epoch: 63 \tTraining Loss: 1.243405 \tValidation Loss: 1.275612\n",
      "Validation loss decreased (1.280270 --> 1.275612).         Saving model ...\n",
      "Epoch: 64 \tTraining Loss: 1.239090 \tValidation Loss: 1.271332\n",
      "Validation loss decreased (1.275612 --> 1.271332).         Saving model ...\n",
      "Epoch: 65 \tTraining Loss: 1.234912 \tValidation Loss: 1.266129\n",
      "Validation loss decreased (1.271332 --> 1.266129).         Saving model ...\n",
      "Epoch: 66 \tTraining Loss: 1.230897 \tValidation Loss: 1.262105\n",
      "Validation loss decreased (1.266129 --> 1.262105).         Saving model ...\n",
      "Epoch: 67 \tTraining Loss: 1.226922 \tValidation Loss: 1.258300\n",
      "Validation loss decreased (1.262105 --> 1.258300).         Saving model ...\n",
      "Epoch: 68 \tTraining Loss: 1.223114 \tValidation Loss: 1.254181\n",
      "Validation loss decreased (1.258300 --> 1.254181).         Saving model ...\n",
      "Epoch: 69 \tTraining Loss: 1.219428 \tValidation Loss: 1.250361\n",
      "Validation loss decreased (1.254181 --> 1.250361).         Saving model ...\n",
      "Epoch: 70 \tTraining Loss: 1.215823 \tValidation Loss: 1.245744\n",
      "Validation loss decreased (1.250361 --> 1.245744).         Saving model ...\n",
      "Epoch: 71 \tTraining Loss: 1.212309 \tValidation Loss: 1.242849\n",
      "Validation loss decreased (1.245744 --> 1.242849).         Saving model ...\n",
      "Epoch: 72 \tTraining Loss: 1.208855 \tValidation Loss: 1.237769\n",
      "Validation loss decreased (1.242849 --> 1.237769).         Saving model ...\n",
      "Epoch: 73 \tTraining Loss: 1.205579 \tValidation Loss: 1.235362\n",
      "Validation loss decreased (1.237769 --> 1.235362).         Saving model ...\n",
      "Epoch: 74 \tTraining Loss: 1.202393 \tValidation Loss: 1.230997\n",
      "Validation loss decreased (1.235362 --> 1.230997).         Saving model ...\n",
      "Epoch: 75 \tTraining Loss: 1.199238 \tValidation Loss: 1.227794\n",
      "Validation loss decreased (1.230997 --> 1.227794).         Saving model ...\n",
      "Epoch: 76 \tTraining Loss: 1.196205 \tValidation Loss: 1.224700\n",
      "Validation loss decreased (1.227794 --> 1.224700).         Saving model ...\n",
      "Epoch: 77 \tTraining Loss: 1.193276 \tValidation Loss: 1.220938\n",
      "Validation loss decreased (1.224700 --> 1.220938).         Saving model ...\n",
      "Epoch: 78 \tTraining Loss: 1.190350 \tValidation Loss: 1.218202\n",
      "Validation loss decreased (1.220938 --> 1.218202).         Saving model ...\n",
      "Epoch: 79 \tTraining Loss: 1.187452 \tValidation Loss: 1.214755\n",
      "Validation loss decreased (1.218202 --> 1.214755).         Saving model ...\n",
      "Epoch: 80 \tTraining Loss: 1.184718 \tValidation Loss: 1.212438\n",
      "Validation loss decreased (1.214755 --> 1.212438).         Saving model ...\n",
      "Epoch: 81 \tTraining Loss: 1.181966 \tValidation Loss: 1.208006\n",
      "Validation loss decreased (1.212438 --> 1.208006).         Saving model ...\n",
      "Epoch: 82 \tTraining Loss: 1.179331 \tValidation Loss: 1.205447\n",
      "Validation loss decreased (1.208006 --> 1.205447).         Saving model ...\n",
      "Epoch: 83 \tTraining Loss: 1.176769 \tValidation Loss: 1.202662\n",
      "Validation loss decreased (1.205447 --> 1.202662).         Saving model ...\n",
      "Epoch: 84 \tTraining Loss: 1.174273 \tValidation Loss: 1.200458\n",
      "Validation loss decreased (1.202662 --> 1.200458).         Saving model ...\n",
      "Epoch: 85 \tTraining Loss: 1.171799 \tValidation Loss: 1.196269\n",
      "Validation loss decreased (1.200458 --> 1.196269).         Saving model ...\n",
      "Epoch: 86 \tTraining Loss: 1.169382 \tValidation Loss: 1.194904\n",
      "Validation loss decreased (1.196269 --> 1.194904).         Saving model ...\n",
      "Epoch: 87 \tTraining Loss: 1.166987 \tValidation Loss: 1.191188\n",
      "Validation loss decreased (1.194904 --> 1.191188).         Saving model ...\n",
      "Epoch: 88 \tTraining Loss: 1.164655 \tValidation Loss: 1.189483\n",
      "Validation loss decreased (1.191188 --> 1.189483).         Saving model ...\n",
      "Epoch: 89 \tTraining Loss: 1.162377 \tValidation Loss: 1.186337\n",
      "Validation loss decreased (1.189483 --> 1.186337).         Saving model ...\n",
      "Epoch: 90 \tTraining Loss: 1.160086 \tValidation Loss: 1.183523\n",
      "Validation loss decreased (1.186337 --> 1.183523).         Saving model ...\n",
      "Epoch: 91 \tTraining Loss: 1.157864 \tValidation Loss: 1.181675\n",
      "Validation loss decreased (1.183523 --> 1.181675).         Saving model ...\n",
      "Epoch: 92 \tTraining Loss: 1.155658 \tValidation Loss: 1.179264\n",
      "Validation loss decreased (1.181675 --> 1.179264).         Saving model ...\n",
      "Epoch: 93 \tTraining Loss: 1.153499 \tValidation Loss: 1.177124\n",
      "Validation loss decreased (1.179264 --> 1.177124).         Saving model ...\n",
      "Epoch: 94 \tTraining Loss: 1.151371 \tValidation Loss: 1.174944\n",
      "Validation loss decreased (1.177124 --> 1.174944).         Saving model ...\n",
      "Epoch: 95 \tTraining Loss: 1.149262 \tValidation Loss: 1.173074\n",
      "Validation loss decreased (1.174944 --> 1.173074).         Saving model ...\n",
      "Epoch: 96 \tTraining Loss: 1.147191 \tValidation Loss: 1.171011\n",
      "Validation loss decreased (1.173074 --> 1.171011).         Saving model ...\n",
      "Epoch: 97 \tTraining Loss: 1.145118 \tValidation Loss: 1.169197\n",
      "Validation loss decreased (1.171011 --> 1.169197).         Saving model ...\n",
      "Epoch: 98 \tTraining Loss: 1.143081 \tValidation Loss: 1.167048\n",
      "Validation loss decreased (1.169197 --> 1.167048).         Saving model ...\n",
      "Epoch: 99 \tTraining Loss: 1.141055 \tValidation Loss: 1.165251\n",
      "Validation loss decreased (1.167048 --> 1.165251).         Saving model ...\n",
      "Epoch: 100 \tTraining Loss: 1.139046 \tValidation Loss: 1.161224\n",
      "Validation loss decreased (1.165251 --> 1.161224).         Saving model ...\n",
      "Epoch: 101 \tTraining Loss: 1.137063 \tValidation Loss: 1.159054\n",
      "Validation loss decreased (1.161224 --> 1.159054).         Saving model ...\n",
      "Epoch: 102 \tTraining Loss: 1.135101 \tValidation Loss: 1.157296\n",
      "Validation loss decreased (1.159054 --> 1.157296).         Saving model ...\n",
      "Epoch: 103 \tTraining Loss: 1.133168 \tValidation Loss: 1.155782\n",
      "Validation loss decreased (1.157296 --> 1.155782).         Saving model ...\n",
      "Epoch: 104 \tTraining Loss: 1.131265 \tValidation Loss: 1.153835\n",
      "Validation loss decreased (1.155782 --> 1.153835).         Saving model ...\n",
      "Epoch: 105 \tTraining Loss: 1.129356 \tValidation Loss: 1.152089\n",
      "Validation loss decreased (1.153835 --> 1.152089).         Saving model ...\n",
      "Epoch: 106 \tTraining Loss: 1.127474 \tValidation Loss: 1.150324\n",
      "Validation loss decreased (1.152089 --> 1.150324).         Saving model ...\n",
      "Epoch: 107 \tTraining Loss: 1.125620 \tValidation Loss: 1.148422\n",
      "Validation loss decreased (1.150324 --> 1.148422).         Saving model ...\n",
      "Epoch: 108 \tTraining Loss: 1.123753 \tValidation Loss: 1.147232\n",
      "Validation loss decreased (1.148422 --> 1.147232).         Saving model ...\n",
      "Epoch: 109 \tTraining Loss: 1.121954 \tValidation Loss: 1.145378\n",
      "Validation loss decreased (1.147232 --> 1.145378).         Saving model ...\n",
      "Epoch: 110 \tTraining Loss: 1.120149 \tValidation Loss: 1.143268\n",
      "Validation loss decreased (1.145378 --> 1.143268).         Saving model ...\n",
      "Epoch: 111 \tTraining Loss: 1.118349 \tValidation Loss: 1.142071\n",
      "Validation loss decreased (1.143268 --> 1.142071).         Saving model ...\n",
      "Epoch: 112 \tTraining Loss: 1.116510 \tValidation Loss: 1.140728\n",
      "Validation loss decreased (1.142071 --> 1.140728).         Saving model ...\n",
      "Epoch: 113 \tTraining Loss: 1.114750 \tValidation Loss: 1.138864\n",
      "Validation loss decreased (1.140728 --> 1.138864).         Saving model ...\n",
      "Epoch: 114 \tTraining Loss: 1.113011 \tValidation Loss: 1.137436\n",
      "Validation loss decreased (1.138864 --> 1.137436).         Saving model ...\n",
      "Epoch: 115 \tTraining Loss: 1.111289 \tValidation Loss: 1.135528\n",
      "Validation loss decreased (1.137436 --> 1.135528).         Saving model ...\n",
      "Epoch: 116 \tTraining Loss: 1.109556 \tValidation Loss: 1.134664\n",
      "Validation loss decreased (1.135528 --> 1.134664).         Saving model ...\n",
      "Epoch: 117 \tTraining Loss: 1.107823 \tValidation Loss: 1.133301\n",
      "Validation loss decreased (1.134664 --> 1.133301).         Saving model ...\n",
      "Epoch: 118 \tTraining Loss: 1.106163 \tValidation Loss: 1.131394\n",
      "Validation loss decreased (1.133301 --> 1.131394).         Saving model ...\n",
      "Epoch: 119 \tTraining Loss: 1.104473 \tValidation Loss: 1.130253\n",
      "Validation loss decreased (1.131394 --> 1.130253).         Saving model ...\n",
      "Epoch: 120 \tTraining Loss: 1.102828 \tValidation Loss: 1.129076\n",
      "Validation loss decreased (1.130253 --> 1.129076).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 121 \tTraining Loss: 1.101171 \tValidation Loss: 1.127989\n",
      "Validation loss decreased (1.129076 --> 1.127989).         Saving model ...\n",
      "Epoch: 122 \tTraining Loss: 1.099502 \tValidation Loss: 1.126873\n",
      "Validation loss decreased (1.127989 --> 1.126873).         Saving model ...\n",
      "Epoch: 123 \tTraining Loss: 1.097910 \tValidation Loss: 1.125727\n",
      "Validation loss decreased (1.126873 --> 1.125727).         Saving model ...\n",
      "Epoch: 124 \tTraining Loss: 1.096300 \tValidation Loss: 1.124161\n",
      "Validation loss decreased (1.125727 --> 1.124161).         Saving model ...\n",
      "Epoch: 125 \tTraining Loss: 1.094714 \tValidation Loss: 1.123057\n",
      "Validation loss decreased (1.124161 --> 1.123057).         Saving model ...\n",
      "Epoch: 126 \tTraining Loss: 1.093171 \tValidation Loss: 1.121704\n",
      "Validation loss decreased (1.123057 --> 1.121704).         Saving model ...\n",
      "Epoch: 127 \tTraining Loss: 1.091585 \tValidation Loss: 1.120941\n",
      "Validation loss decreased (1.121704 --> 1.120941).         Saving model ...\n",
      "Epoch: 128 \tTraining Loss: 1.090108 \tValidation Loss: 1.119603\n",
      "Validation loss decreased (1.120941 --> 1.119603).         Saving model ...\n",
      "Epoch: 129 \tTraining Loss: 1.088526 \tValidation Loss: 1.118621\n",
      "Validation loss decreased (1.119603 --> 1.118621).         Saving model ...\n",
      "Epoch: 130 \tTraining Loss: 1.087028 \tValidation Loss: 1.117519\n",
      "Validation loss decreased (1.118621 --> 1.117519).         Saving model ...\n",
      "Epoch: 131 \tTraining Loss: 1.085541 \tValidation Loss: 1.116595\n",
      "Validation loss decreased (1.117519 --> 1.116595).         Saving model ...\n",
      "Epoch: 132 \tTraining Loss: 1.084027 \tValidation Loss: 1.115732\n",
      "Validation loss decreased (1.116595 --> 1.115732).         Saving model ...\n",
      "Epoch: 133 \tTraining Loss: 1.082577 \tValidation Loss: 1.115042\n",
      "Validation loss decreased (1.115732 --> 1.115042).         Saving model ...\n",
      "Epoch: 134 \tTraining Loss: 1.081120 \tValidation Loss: 1.113041\n",
      "Validation loss decreased (1.115042 --> 1.113041).         Saving model ...\n",
      "Epoch: 135 \tTraining Loss: 1.079718 \tValidation Loss: 1.112227\n",
      "Validation loss decreased (1.113041 --> 1.112227).         Saving model ...\n",
      "Epoch: 136 \tTraining Loss: 1.078288 \tValidation Loss: 1.111463\n",
      "Validation loss decreased (1.112227 --> 1.111463).         Saving model ...\n",
      "Epoch: 137 \tTraining Loss: 1.076890 \tValidation Loss: 1.110278\n",
      "Validation loss decreased (1.111463 --> 1.110278).         Saving model ...\n",
      "Epoch: 138 \tTraining Loss: 1.075493 \tValidation Loss: 1.109536\n",
      "Validation loss decreased (1.110278 --> 1.109536).         Saving model ...\n",
      "Epoch: 139 \tTraining Loss: 1.074091 \tValidation Loss: 1.108656\n",
      "Validation loss decreased (1.109536 --> 1.108656).         Saving model ...\n",
      "Epoch: 140 \tTraining Loss: 1.072715 \tValidation Loss: 1.107785\n",
      "Validation loss decreased (1.108656 --> 1.107785).         Saving model ...\n",
      "Epoch: 141 \tTraining Loss: 1.071369 \tValidation Loss: 1.107366\n",
      "Validation loss decreased (1.107785 --> 1.107366).         Saving model ...\n",
      "Epoch: 142 \tTraining Loss: 1.070029 \tValidation Loss: 1.106205\n",
      "Validation loss decreased (1.107366 --> 1.106205).         Saving model ...\n",
      "Epoch: 143 \tTraining Loss: 1.068704 \tValidation Loss: 1.104974\n",
      "Validation loss decreased (1.106205 --> 1.104974).         Saving model ...\n",
      "Epoch: 144 \tTraining Loss: 1.067402 \tValidation Loss: 1.104663\n",
      "Validation loss decreased (1.104974 --> 1.104663).         Saving model ...\n",
      "Epoch: 145 \tTraining Loss: 1.066089 \tValidation Loss: 1.103695\n",
      "Validation loss decreased (1.104663 --> 1.103695).         Saving model ...\n",
      "Epoch: 146 \tTraining Loss: 1.064818 \tValidation Loss: 1.103257\n",
      "Validation loss decreased (1.103695 --> 1.103257).         Saving model ...\n",
      "Epoch: 147 \tTraining Loss: 1.063553 \tValidation Loss: 1.102206\n",
      "Validation loss decreased (1.103257 --> 1.102206).         Saving model ...\n",
      "Epoch: 148 \tTraining Loss: 1.062268 \tValidation Loss: 1.101537\n",
      "Validation loss decreased (1.102206 --> 1.101537).         Saving model ...\n",
      "Epoch: 149 \tTraining Loss: 1.061053 \tValidation Loss: 1.100302\n",
      "Validation loss decreased (1.101537 --> 1.100302).         Saving model ...\n",
      "Epoch: 150 \tTraining Loss: 1.059792 \tValidation Loss: 1.099720\n",
      "Validation loss decreased (1.100302 --> 1.099720).         Saving model ...\n",
      "Epoch: 151 \tTraining Loss: 1.058586 \tValidation Loss: 1.098621\n",
      "Validation loss decreased (1.099720 --> 1.098621).         Saving model ...\n",
      "Epoch: 152 \tTraining Loss: 1.057389 \tValidation Loss: 1.098045\n",
      "Validation loss decreased (1.098621 --> 1.098045).         Saving model ...\n",
      "Epoch: 153 \tTraining Loss: 1.056147 \tValidation Loss: 1.097500\n",
      "Validation loss decreased (1.098045 --> 1.097500).         Saving model ...\n",
      "Epoch: 154 \tTraining Loss: 1.054975 \tValidation Loss: 1.097437\n",
      "Validation loss decreased (1.097500 --> 1.097437).         Saving model ...\n",
      "Epoch: 155 \tTraining Loss: 1.053824 \tValidation Loss: 1.096653\n",
      "Validation loss decreased (1.097437 --> 1.096653).         Saving model ...\n",
      "Epoch: 156 \tTraining Loss: 1.052624 \tValidation Loss: 1.096310\n",
      "Validation loss decreased (1.096653 --> 1.096310).         Saving model ...\n",
      "Epoch: 157 \tTraining Loss: 1.051458 \tValidation Loss: 1.095540\n",
      "Validation loss decreased (1.096310 --> 1.095540).         Saving model ...\n",
      "Epoch: 158 \tTraining Loss: 1.050311 \tValidation Loss: 1.095210\n",
      "Validation loss decreased (1.095540 --> 1.095210).         Saving model ...\n",
      "Epoch: 159 \tTraining Loss: 1.049163 \tValidation Loss: 1.094869\n",
      "Validation loss decreased (1.095210 --> 1.094869).         Saving model ...\n",
      "Epoch: 160 \tTraining Loss: 1.048043 \tValidation Loss: 1.094237\n",
      "Validation loss decreased (1.094869 --> 1.094237).         Saving model ...\n",
      "Epoch: 161 \tTraining Loss: 1.046923 \tValidation Loss: 1.093638\n",
      "Validation loss decreased (1.094237 --> 1.093638).         Saving model ...\n",
      "Epoch: 162 \tTraining Loss: 1.045808 \tValidation Loss: 1.093228\n",
      "Validation loss decreased (1.093638 --> 1.093228).         Saving model ...\n",
      "Epoch: 163 \tTraining Loss: 1.044706 \tValidation Loss: 1.092458\n",
      "Validation loss decreased (1.093228 --> 1.092458).         Saving model ...\n",
      "Epoch: 164 \tTraining Loss: 1.043639 \tValidation Loss: 1.092145\n",
      "Validation loss decreased (1.092458 --> 1.092145).         Saving model ...\n",
      "Epoch: 165 \tTraining Loss: 1.042524 \tValidation Loss: 1.090993\n",
      "Validation loss decreased (1.092145 --> 1.090993).         Saving model ...\n",
      "Epoch: 166 \tTraining Loss: 1.041474 \tValidation Loss: 1.091195\n",
      "Epoch: 167 \tTraining Loss: 1.040449 \tValidation Loss: 1.090256\n",
      "Validation loss decreased (1.090993 --> 1.090256).         Saving model ...\n",
      "Epoch: 168 \tTraining Loss: 1.039378 \tValidation Loss: 1.090505\n",
      "Epoch: 169 \tTraining Loss: 1.038305 \tValidation Loss: 1.089251\n",
      "Validation loss decreased (1.090256 --> 1.089251).         Saving model ...\n",
      "Epoch: 170 \tTraining Loss: 1.037275 \tValidation Loss: 1.088309\n",
      "Validation loss decreased (1.089251 --> 1.088309).         Saving model ...\n",
      "Epoch: 171 \tTraining Loss: 1.036270 \tValidation Loss: 1.088223\n",
      "Validation loss decreased (1.088309 --> 1.088223).         Saving model ...\n",
      "Epoch: 172 \tTraining Loss: 1.035217 \tValidation Loss: 1.087927\n",
      "Validation loss decreased (1.088223 --> 1.087927).         Saving model ...\n",
      "Epoch: 173 \tTraining Loss: 1.034197 \tValidation Loss: 1.087133\n",
      "Validation loss decreased (1.087927 --> 1.087133).         Saving model ...\n",
      "Epoch: 174 \tTraining Loss: 1.033196 \tValidation Loss: 1.086765\n",
      "Validation loss decreased (1.087133 --> 1.086765).         Saving model ...\n",
      "Epoch: 175 \tTraining Loss: 1.032206 \tValidation Loss: 1.086492\n",
      "Validation loss decreased (1.086765 --> 1.086492).         Saving model ...\n",
      "Epoch: 176 \tTraining Loss: 1.031253 \tValidation Loss: 1.085977\n",
      "Validation loss decreased (1.086492 --> 1.085977).         Saving model ...\n",
      "Epoch: 177 \tTraining Loss: 1.030256 \tValidation Loss: 1.085745\n",
      "Validation loss decreased (1.085977 --> 1.085745).         Saving model ...\n",
      "Epoch: 178 \tTraining Loss: 1.029293 \tValidation Loss: 1.084701\n",
      "Validation loss decreased (1.085745 --> 1.084701).         Saving model ...\n",
      "Epoch: 179 \tTraining Loss: 1.028341 \tValidation Loss: 1.084629\n",
      "Validation loss decreased (1.084701 --> 1.084629).         Saving model ...\n",
      "Epoch: 180 \tTraining Loss: 1.027403 \tValidation Loss: 1.083886\n",
      "Validation loss decreased (1.084629 --> 1.083886).         Saving model ...\n",
      "Epoch: 181 \tTraining Loss: 1.026468 \tValidation Loss: 1.083608\n",
      "Validation loss decreased (1.083886 --> 1.083608).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 182 \tTraining Loss: 1.025512 \tValidation Loss: 1.083740\n",
      "Epoch: 183 \tTraining Loss: 1.024594 \tValidation Loss: 1.083029\n",
      "Validation loss decreased (1.083608 --> 1.083029).         Saving model ...\n",
      "Epoch: 184 \tTraining Loss: 1.023620 \tValidation Loss: 1.082657\n",
      "Validation loss decreased (1.083029 --> 1.082657).         Saving model ...\n",
      "Epoch: 185 \tTraining Loss: 1.022731 \tValidation Loss: 1.082520\n",
      "Validation loss decreased (1.082657 --> 1.082520).         Saving model ...\n",
      "Epoch: 186 \tTraining Loss: 1.021809 \tValidation Loss: 1.082034\n",
      "Validation loss decreased (1.082520 --> 1.082034).         Saving model ...\n",
      "Epoch: 187 \tTraining Loss: 1.020928 \tValidation Loss: 1.081575\n",
      "Validation loss decreased (1.082034 --> 1.081575).         Saving model ...\n",
      "Epoch: 188 \tTraining Loss: 1.020010 \tValidation Loss: 1.081063\n",
      "Validation loss decreased (1.081575 --> 1.081063).         Saving model ...\n",
      "Epoch: 189 \tTraining Loss: 1.019119 \tValidation Loss: 1.080473\n",
      "Validation loss decreased (1.081063 --> 1.080473).         Saving model ...\n",
      "Epoch: 190 \tTraining Loss: 1.018222 \tValidation Loss: 1.080093\n",
      "Validation loss decreased (1.080473 --> 1.080093).         Saving model ...\n",
      "Epoch: 191 \tTraining Loss: 1.017328 \tValidation Loss: 1.079105\n",
      "Validation loss decreased (1.080093 --> 1.079105).         Saving model ...\n",
      "Epoch: 192 \tTraining Loss: 1.016445 \tValidation Loss: 1.079207\n",
      "Epoch: 193 \tTraining Loss: 1.015588 \tValidation Loss: 1.078817\n",
      "Validation loss decreased (1.079105 --> 1.078817).         Saving model ...\n",
      "Epoch: 194 \tTraining Loss: 1.014731 \tValidation Loss: 1.078148\n",
      "Validation loss decreased (1.078817 --> 1.078148).         Saving model ...\n",
      "Epoch: 195 \tTraining Loss: 1.013904 \tValidation Loss: 1.077966\n",
      "Validation loss decreased (1.078148 --> 1.077966).         Saving model ...\n",
      "Epoch: 196 \tTraining Loss: 1.013083 \tValidation Loss: 1.077185\n",
      "Validation loss decreased (1.077966 --> 1.077185).         Saving model ...\n",
      "Epoch: 197 \tTraining Loss: 1.012238 \tValidation Loss: 1.076566\n",
      "Validation loss decreased (1.077185 --> 1.076566).         Saving model ...\n",
      "Epoch: 198 \tTraining Loss: 1.011414 \tValidation Loss: 1.076734\n",
      "Epoch: 199 \tTraining Loss: 1.010572 \tValidation Loss: 1.076158\n",
      "Validation loss decreased (1.076566 --> 1.076158).         Saving model ...\n",
      "Epoch: 200 \tTraining Loss: 1.009784 \tValidation Loss: 1.075779\n",
      "Validation loss decreased (1.076158 --> 1.075779).         Saving model ...\n",
      "Epoch: 201 \tTraining Loss: 1.008974 \tValidation Loss: 1.075261\n",
      "Validation loss decreased (1.075779 --> 1.075261).         Saving model ...\n",
      "Epoch: 202 \tTraining Loss: 1.008165 \tValidation Loss: 1.074899\n",
      "Validation loss decreased (1.075261 --> 1.074899).         Saving model ...\n",
      "Epoch: 203 \tTraining Loss: 1.007388 \tValidation Loss: 1.074411\n",
      "Validation loss decreased (1.074899 --> 1.074411).         Saving model ...\n",
      "Epoch: 204 \tTraining Loss: 1.006595 \tValidation Loss: 1.073893\n",
      "Validation loss decreased (1.074411 --> 1.073893).         Saving model ...\n",
      "Epoch: 205 \tTraining Loss: 1.005827 \tValidation Loss: 1.073637\n",
      "Validation loss decreased (1.073893 --> 1.073637).         Saving model ...\n",
      "Epoch: 206 \tTraining Loss: 1.005041 \tValidation Loss: 1.073555\n",
      "Validation loss decreased (1.073637 --> 1.073555).         Saving model ...\n",
      "Epoch: 207 \tTraining Loss: 1.004254 \tValidation Loss: 1.072808\n",
      "Validation loss decreased (1.073555 --> 1.072808).         Saving model ...\n",
      "Epoch: 208 \tTraining Loss: 1.003495 \tValidation Loss: 1.072027\n",
      "Validation loss decreased (1.072808 --> 1.072027).         Saving model ...\n",
      "Epoch: 209 \tTraining Loss: 1.002768 \tValidation Loss: 1.071187\n",
      "Validation loss decreased (1.072027 --> 1.071187).         Saving model ...\n",
      "Epoch: 210 \tTraining Loss: 1.001986 \tValidation Loss: 1.071136\n",
      "Validation loss decreased (1.071187 --> 1.071136).         Saving model ...\n",
      "Epoch: 211 \tTraining Loss: 1.001247 \tValidation Loss: 1.071408\n",
      "Epoch: 212 \tTraining Loss: 1.000510 \tValidation Loss: 1.070127\n",
      "Validation loss decreased (1.071136 --> 1.070127).         Saving model ...\n",
      "Epoch: 213 \tTraining Loss: 0.999793 \tValidation Loss: 1.069611\n",
      "Validation loss decreased (1.070127 --> 1.069611).         Saving model ...\n",
      "Epoch: 214 \tTraining Loss: 0.999054 \tValidation Loss: 1.069047\n",
      "Validation loss decreased (1.069611 --> 1.069047).         Saving model ...\n",
      "Epoch: 215 \tTraining Loss: 0.998359 \tValidation Loss: 1.068033\n",
      "Validation loss decreased (1.069047 --> 1.068033).         Saving model ...\n",
      "Epoch: 216 \tTraining Loss: 0.997629 \tValidation Loss: 1.067581\n",
      "Validation loss decreased (1.068033 --> 1.067581).         Saving model ...\n",
      "Epoch: 217 \tTraining Loss: 0.996944 \tValidation Loss: 1.067066\n",
      "Validation loss decreased (1.067581 --> 1.067066).         Saving model ...\n",
      "Epoch: 218 \tTraining Loss: 0.996221 \tValidation Loss: 1.066416\n",
      "Validation loss decreased (1.067066 --> 1.066416).         Saving model ...\n",
      "Epoch: 219 \tTraining Loss: 0.995541 \tValidation Loss: 1.066526\n",
      "Epoch: 220 \tTraining Loss: 0.994837 \tValidation Loss: 1.066563\n",
      "Epoch: 221 \tTraining Loss: 0.994118 \tValidation Loss: 1.065610\n",
      "Validation loss decreased (1.066416 --> 1.065610).         Saving model ...\n",
      "Epoch: 222 \tTraining Loss: 0.993480 \tValidation Loss: 1.066170\n",
      "Epoch: 223 \tTraining Loss: 0.992794 \tValidation Loss: 1.065140\n",
      "Validation loss decreased (1.065610 --> 1.065140).         Saving model ...\n",
      "Epoch: 224 \tTraining Loss: 0.992126 \tValidation Loss: 1.064715\n",
      "Validation loss decreased (1.065140 --> 1.064715).         Saving model ...\n",
      "Epoch: 225 \tTraining Loss: 0.991458 \tValidation Loss: 1.063968\n",
      "Validation loss decreased (1.064715 --> 1.063968).         Saving model ...\n",
      "Epoch: 226 \tTraining Loss: 0.990814 \tValidation Loss: 1.064313\n",
      "Epoch: 227 \tTraining Loss: 0.990166 \tValidation Loss: 1.063485\n",
      "Validation loss decreased (1.063968 --> 1.063485).         Saving model ...\n",
      "Epoch: 228 \tTraining Loss: 0.989530 \tValidation Loss: 1.062930\n",
      "Validation loss decreased (1.063485 --> 1.062930).         Saving model ...\n",
      "Epoch: 229 \tTraining Loss: 0.988880 \tValidation Loss: 1.062365\n",
      "Validation loss decreased (1.062930 --> 1.062365).         Saving model ...\n",
      "Epoch: 230 \tTraining Loss: 0.988266 \tValidation Loss: 1.061931\n",
      "Validation loss decreased (1.062365 --> 1.061931).         Saving model ...\n",
      "Epoch: 231 \tTraining Loss: 0.987611 \tValidation Loss: 1.058880\n",
      "Validation loss decreased (1.061931 --> 1.058880).         Saving model ...\n",
      "Epoch: 232 \tTraining Loss: 0.987015 \tValidation Loss: 1.058316\n",
      "Validation loss decreased (1.058880 --> 1.058316).         Saving model ...\n",
      "Epoch: 233 \tTraining Loss: 0.986349 \tValidation Loss: 1.057671\n",
      "Validation loss decreased (1.058316 --> 1.057671).         Saving model ...\n",
      "Epoch: 234 \tTraining Loss: 0.985741 \tValidation Loss: 1.056963\n",
      "Validation loss decreased (1.057671 --> 1.056963).         Saving model ...\n",
      "Epoch: 235 \tTraining Loss: 0.985123 \tValidation Loss: 1.059159\n",
      "Epoch: 236 \tTraining Loss: 0.984506 \tValidation Loss: 1.059104\n",
      "Epoch: 237 \tTraining Loss: 0.983881 \tValidation Loss: 1.055624\n",
      "Validation loss decreased (1.056963 --> 1.055624).         Saving model ...\n",
      "Epoch: 238 \tTraining Loss: 0.983276 \tValidation Loss: 1.055439\n",
      "Validation loss decreased (1.055624 --> 1.055439).         Saving model ...\n",
      "Epoch: 239 \tTraining Loss: 0.982697 \tValidation Loss: 1.055080\n",
      "Validation loss decreased (1.055439 --> 1.055080).         Saving model ...\n",
      "Epoch: 240 \tTraining Loss: 0.982127 \tValidation Loss: 1.054143\n",
      "Validation loss decreased (1.055080 --> 1.054143).         Saving model ...\n",
      "Epoch: 241 \tTraining Loss: 0.981522 \tValidation Loss: 1.053719\n",
      "Validation loss decreased (1.054143 --> 1.053719).         Saving model ...\n",
      "Epoch: 242 \tTraining Loss: 0.980957 \tValidation Loss: 1.053167\n",
      "Validation loss decreased (1.053719 --> 1.053167).         Saving model ...\n",
      "Epoch: 243 \tTraining Loss: 0.980382 \tValidation Loss: 1.052559\n",
      "Validation loss decreased (1.053167 --> 1.052559).         Saving model ...\n",
      "Epoch: 244 \tTraining Loss: 0.979805 \tValidation Loss: 1.052727\n",
      "Epoch: 245 \tTraining Loss: 0.979263 \tValidation Loss: 1.054440\n",
      "Epoch: 246 \tTraining Loss: 0.978715 \tValidation Loss: 1.054128\n",
      "Epoch: 247 \tTraining Loss: 0.978154 \tValidation Loss: 1.050942\n",
      "Validation loss decreased (1.052559 --> 1.050942).         Saving model ...\n",
      "Epoch: 248 \tTraining Loss: 0.977607 \tValidation Loss: 1.053306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 249 \tTraining Loss: 0.977077 \tValidation Loss: 1.050219\n",
      "Validation loss decreased (1.050942 --> 1.050219).         Saving model ...\n",
      "Epoch: 250 \tTraining Loss: 0.976520 \tValidation Loss: 1.052821\n",
      "Epoch: 251 \tTraining Loss: 0.976012 \tValidation Loss: 1.048938\n",
      "Validation loss decreased (1.050219 --> 1.048938).         Saving model ...\n",
      "Epoch: 252 \tTraining Loss: 0.975466 \tValidation Loss: 1.048927\n",
      "Validation loss decreased (1.048938 --> 1.048927).         Saving model ...\n",
      "Epoch: 253 \tTraining Loss: 0.974938 \tValidation Loss: 1.047253\n",
      "Validation loss decreased (1.048927 --> 1.047253).         Saving model ...\n",
      "Epoch: 254 \tTraining Loss: 0.974398 \tValidation Loss: 1.046868\n",
      "Validation loss decreased (1.047253 --> 1.046868).         Saving model ...\n",
      "Epoch: 255 \tTraining Loss: 0.973899 \tValidation Loss: 1.046378\n",
      "Validation loss decreased (1.046868 --> 1.046378).         Saving model ...\n",
      "Epoch: 256 \tTraining Loss: 0.973361 \tValidation Loss: 1.047119\n",
      "Epoch: 257 \tTraining Loss: 0.972809 \tValidation Loss: 1.045717\n",
      "Validation loss decreased (1.046378 --> 1.045717).         Saving model ...\n",
      "Epoch: 258 \tTraining Loss: 0.972326 \tValidation Loss: 1.045717\n",
      "Validation loss decreased (1.045717 --> 1.045717).         Saving model ...\n",
      "Epoch: 259 \tTraining Loss: 0.971816 \tValidation Loss: 1.045504\n",
      "Validation loss decreased (1.045717 --> 1.045504).         Saving model ...\n",
      "Epoch: 260 \tTraining Loss: 0.971309 \tValidation Loss: 1.044794\n",
      "Validation loss decreased (1.045504 --> 1.044794).         Saving model ...\n",
      "Epoch: 261 \tTraining Loss: 0.970804 \tValidation Loss: 1.043973\n",
      "Validation loss decreased (1.044794 --> 1.043973).         Saving model ...\n",
      "Epoch: 262 \tTraining Loss: 0.970318 \tValidation Loss: 1.047426\n",
      "Epoch: 263 \tTraining Loss: 0.969822 \tValidation Loss: 1.046642\n",
      "Epoch: 264 \tTraining Loss: 0.969348 \tValidation Loss: 1.046602\n",
      "Epoch: 265 \tTraining Loss: 0.968853 \tValidation Loss: 1.042722\n",
      "Validation loss decreased (1.043973 --> 1.042722).         Saving model ...\n",
      "Epoch: 266 \tTraining Loss: 0.968386 \tValidation Loss: 1.042681\n",
      "Validation loss decreased (1.042722 --> 1.042681).         Saving model ...\n",
      "Epoch: 267 \tTraining Loss: 0.967938 \tValidation Loss: 1.040526\n",
      "Validation loss decreased (1.042681 --> 1.040526).         Saving model ...\n",
      "Epoch: 268 \tTraining Loss: 0.967445 \tValidation Loss: 1.041721\n",
      "Epoch: 269 \tTraining Loss: 0.966981 \tValidation Loss: 1.039843\n",
      "Validation loss decreased (1.040526 --> 1.039843).         Saving model ...\n",
      "Epoch: 270 \tTraining Loss: 0.966529 \tValidation Loss: 1.039395\n",
      "Validation loss decreased (1.039843 --> 1.039395).         Saving model ...\n",
      "Epoch: 271 \tTraining Loss: 0.966072 \tValidation Loss: 1.041666\n",
      "Epoch: 272 \tTraining Loss: 0.965608 \tValidation Loss: 1.041404\n",
      "Epoch: 273 \tTraining Loss: 0.965155 \tValidation Loss: 1.038427\n",
      "Validation loss decreased (1.039395 --> 1.038427).         Saving model ...\n",
      "Epoch: 274 \tTraining Loss: 0.964719 \tValidation Loss: 1.039633\n",
      "Epoch: 275 \tTraining Loss: 0.964279 \tValidation Loss: 1.037760\n",
      "Validation loss decreased (1.038427 --> 1.037760).         Saving model ...\n",
      "Epoch: 276 \tTraining Loss: 0.963807 \tValidation Loss: 1.037659\n",
      "Validation loss decreased (1.037760 --> 1.037659).         Saving model ...\n",
      "Epoch: 277 \tTraining Loss: 0.963388 \tValidation Loss: 1.037423\n",
      "Validation loss decreased (1.037659 --> 1.037423).         Saving model ...\n",
      "Epoch: 278 \tTraining Loss: 0.962958 \tValidation Loss: 1.037658\n",
      "Epoch: 279 \tTraining Loss: 0.962499 \tValidation Loss: 1.036882\n",
      "Validation loss decreased (1.037423 --> 1.036882).         Saving model ...\n",
      "Epoch: 280 \tTraining Loss: 0.962100 \tValidation Loss: 1.037553\n",
      "Epoch: 281 \tTraining Loss: 0.961668 \tValidation Loss: 1.034239\n",
      "Validation loss decreased (1.036882 --> 1.034239).         Saving model ...\n",
      "Epoch: 282 \tTraining Loss: 0.961240 \tValidation Loss: 1.035535\n",
      "Epoch: 283 \tTraining Loss: 0.960818 \tValidation Loss: 1.035639\n",
      "Epoch: 284 \tTraining Loss: 0.960426 \tValidation Loss: 1.035374\n",
      "Epoch: 285 \tTraining Loss: 0.960003 \tValidation Loss: 1.035023\n",
      "Epoch: 286 \tTraining Loss: 0.959578 \tValidation Loss: 1.034490\n",
      "Epoch: 287 \tTraining Loss: 0.959192 \tValidation Loss: 1.034312\n",
      "Epoch: 288 \tTraining Loss: 0.958781 \tValidation Loss: 1.033479\n",
      "Validation loss decreased (1.034239 --> 1.033479).         Saving model ...\n",
      "Epoch: 289 \tTraining Loss: 0.958367 \tValidation Loss: 1.035006\n",
      "Epoch: 290 \tTraining Loss: 0.957960 \tValidation Loss: 1.032605\n",
      "Validation loss decreased (1.033479 --> 1.032605).         Saving model ...\n",
      "Epoch: 291 \tTraining Loss: 0.957583 \tValidation Loss: 1.032544\n",
      "Validation loss decreased (1.032605 --> 1.032544).         Saving model ...\n",
      "Epoch: 292 \tTraining Loss: 0.957183 \tValidation Loss: 1.032208\n",
      "Validation loss decreased (1.032544 --> 1.032208).         Saving model ...\n",
      "Epoch: 293 \tTraining Loss: 0.956780 \tValidation Loss: 1.031608\n",
      "Validation loss decreased (1.032208 --> 1.031608).         Saving model ...\n",
      "Epoch: 294 \tTraining Loss: 0.956404 \tValidation Loss: 1.031995\n",
      "Epoch: 295 \tTraining Loss: 0.956025 \tValidation Loss: 1.031337\n",
      "Validation loss decreased (1.031608 --> 1.031337).         Saving model ...\n",
      "Epoch: 296 \tTraining Loss: 0.955645 \tValidation Loss: 1.031043\n",
      "Validation loss decreased (1.031337 --> 1.031043).         Saving model ...\n",
      "Epoch: 297 \tTraining Loss: 0.955254 \tValidation Loss: 1.030549\n",
      "Validation loss decreased (1.031043 --> 1.030549).         Saving model ...\n",
      "Epoch: 298 \tTraining Loss: 0.954862 \tValidation Loss: 1.030624\n",
      "Epoch: 299 \tTraining Loss: 0.954509 \tValidation Loss: 1.029934\n",
      "Validation loss decreased (1.030549 --> 1.029934).         Saving model ...\n",
      "Epoch: 300 \tTraining Loss: 0.954128 \tValidation Loss: 1.029099\n",
      "Validation loss decreased (1.029934 --> 1.029099).         Saving model ...\n",
      "Epoch: 301 \tTraining Loss: 0.953750 \tValidation Loss: 1.029198\n",
      "Epoch: 302 \tTraining Loss: 0.953394 \tValidation Loss: 1.029423\n",
      "Epoch: 303 \tTraining Loss: 0.953027 \tValidation Loss: 1.028895\n",
      "Validation loss decreased (1.029099 --> 1.028895).         Saving model ...\n",
      "Epoch: 304 \tTraining Loss: 0.952669 \tValidation Loss: 1.028656\n",
      "Validation loss decreased (1.028895 --> 1.028656).         Saving model ...\n",
      "Epoch: 305 \tTraining Loss: 0.952279 \tValidation Loss: 1.027548\n",
      "Validation loss decreased (1.028656 --> 1.027548).         Saving model ...\n",
      "Epoch: 306 \tTraining Loss: 0.951919 \tValidation Loss: 1.031516\n",
      "Epoch: 307 \tTraining Loss: 0.951594 \tValidation Loss: 1.031474\n",
      "Epoch: 308 \tTraining Loss: 0.951194 \tValidation Loss: 1.030639\n",
      "Epoch: 309 \tTraining Loss: 0.950881 \tValidation Loss: 1.030794\n",
      "Epoch: 310 \tTraining Loss: 0.950520 \tValidation Loss: 1.025445\n",
      "Validation loss decreased (1.027548 --> 1.025445).         Saving model ...\n",
      "Epoch: 311 \tTraining Loss: 0.950199 \tValidation Loss: 1.024908\n",
      "Validation loss decreased (1.025445 --> 1.024908).         Saving model ...\n",
      "Epoch: 312 \tTraining Loss: 0.949853 \tValidation Loss: 1.024925\n",
      "Epoch: 313 \tTraining Loss: 0.949494 \tValidation Loss: 1.024652\n",
      "Validation loss decreased (1.024908 --> 1.024652).         Saving model ...\n",
      "Epoch: 314 \tTraining Loss: 0.949119 \tValidation Loss: 1.023360\n",
      "Validation loss decreased (1.024652 --> 1.023360).         Saving model ...\n",
      "Epoch: 315 \tTraining Loss: 0.948784 \tValidation Loss: 1.022407\n",
      "Validation loss decreased (1.023360 --> 1.022407).         Saving model ...\n",
      "Epoch: 316 \tTraining Loss: 0.948445 \tValidation Loss: 1.023879\n",
      "Epoch: 317 \tTraining Loss: 0.948114 \tValidation Loss: 1.023821\n",
      "Epoch: 318 \tTraining Loss: 0.947778 \tValidation Loss: 1.023532\n",
      "Epoch: 319 \tTraining Loss: 0.947447 \tValidation Loss: 1.021533\n",
      "Validation loss decreased (1.022407 --> 1.021533).         Saving model ...\n",
      "Epoch: 320 \tTraining Loss: 0.947117 \tValidation Loss: 1.023730\n",
      "Epoch: 321 \tTraining Loss: 0.946805 \tValidation Loss: 1.022252\n",
      "Epoch: 322 \tTraining Loss: 0.946463 \tValidation Loss: 1.020624\n",
      "Validation loss decreased (1.021533 --> 1.020624).         Saving model ...\n",
      "Epoch: 323 \tTraining Loss: 0.946107 \tValidation Loss: 1.021904\n",
      "Epoch: 324 \tTraining Loss: 0.945810 \tValidation Loss: 1.021092\n",
      "Epoch: 325 \tTraining Loss: 0.945488 \tValidation Loss: 1.020839\n",
      "Epoch: 326 \tTraining Loss: 0.945163 \tValidation Loss: 1.020599\n",
      "Validation loss decreased (1.020624 --> 1.020599).         Saving model ...\n",
      "Epoch: 327 \tTraining Loss: 0.944856 \tValidation Loss: 1.020948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 328 \tTraining Loss: 0.944535 \tValidation Loss: 1.018997\n",
      "Validation loss decreased (1.020599 --> 1.018997).         Saving model ...\n",
      "Epoch: 329 \tTraining Loss: 0.944214 \tValidation Loss: 1.019978\n",
      "Epoch: 330 \tTraining Loss: 0.943912 \tValidation Loss: 1.017997\n",
      "Validation loss decreased (1.018997 --> 1.017997).         Saving model ...\n",
      "Epoch: 331 \tTraining Loss: 0.943616 \tValidation Loss: 1.017617\n",
      "Validation loss decreased (1.017997 --> 1.017617).         Saving model ...\n",
      "Epoch: 332 \tTraining Loss: 0.943278 \tValidation Loss: 1.018820\n",
      "Epoch: 333 \tTraining Loss: 0.942975 \tValidation Loss: 1.019032\n",
      "Epoch: 334 \tTraining Loss: 0.942656 \tValidation Loss: 1.018585\n",
      "Epoch: 335 \tTraining Loss: 0.942350 \tValidation Loss: 1.018344\n",
      "Epoch: 336 \tTraining Loss: 0.942054 \tValidation Loss: 1.016613\n",
      "Validation loss decreased (1.017617 --> 1.016613).         Saving model ...\n",
      "Epoch: 337 \tTraining Loss: 0.941755 \tValidation Loss: 1.016201\n",
      "Validation loss decreased (1.016613 --> 1.016201).         Saving model ...\n",
      "Epoch: 338 \tTraining Loss: 0.941463 \tValidation Loss: 1.015359\n",
      "Validation loss decreased (1.016201 --> 1.015359).         Saving model ...\n",
      "Epoch: 339 \tTraining Loss: 0.941128 \tValidation Loss: 1.015575\n",
      "Epoch: 340 \tTraining Loss: 0.940857 \tValidation Loss: 1.015160\n",
      "Validation loss decreased (1.015359 --> 1.015160).         Saving model ...\n",
      "Epoch: 341 \tTraining Loss: 0.940569 \tValidation Loss: 1.014903\n",
      "Validation loss decreased (1.015160 --> 1.014903).         Saving model ...\n",
      "Epoch: 342 \tTraining Loss: 0.940262 \tValidation Loss: 1.014634\n",
      "Validation loss decreased (1.014903 --> 1.014634).         Saving model ...\n",
      "Epoch: 343 \tTraining Loss: 0.939975 \tValidation Loss: 1.014834\n",
      "Epoch: 344 \tTraining Loss: 0.939673 \tValidation Loss: 1.014157\n",
      "Validation loss decreased (1.014634 --> 1.014157).         Saving model ...\n",
      "Epoch: 345 \tTraining Loss: 0.939392 \tValidation Loss: 1.013759\n",
      "Validation loss decreased (1.014157 --> 1.013759).         Saving model ...\n",
      "Epoch: 346 \tTraining Loss: 0.939097 \tValidation Loss: 1.014782\n",
      "Epoch: 347 \tTraining Loss: 0.938800 \tValidation Loss: 1.013484\n",
      "Validation loss decreased (1.013759 --> 1.013484).         Saving model ...\n",
      "Epoch: 348 \tTraining Loss: 0.938558 \tValidation Loss: 1.013097\n",
      "Validation loss decreased (1.013484 --> 1.013097).         Saving model ...\n",
      "Epoch: 349 \tTraining Loss: 0.938249 \tValidation Loss: 1.013331\n",
      "Epoch: 350 \tTraining Loss: 0.937961 \tValidation Loss: 1.013323\n",
      "Epoch: 351 \tTraining Loss: 0.937690 \tValidation Loss: 1.012670\n",
      "Validation loss decreased (1.013097 --> 1.012670).         Saving model ...\n",
      "Epoch: 352 \tTraining Loss: 0.937428 \tValidation Loss: 1.012297\n",
      "Validation loss decreased (1.012670 --> 1.012297).         Saving model ...\n",
      "Epoch: 353 \tTraining Loss: 0.937116 \tValidation Loss: 1.012013\n",
      "Validation loss decreased (1.012297 --> 1.012013).         Saving model ...\n",
      "Epoch: 354 \tTraining Loss: 0.936860 \tValidation Loss: 1.011674\n",
      "Validation loss decreased (1.012013 --> 1.011674).         Saving model ...\n",
      "Epoch: 355 \tTraining Loss: 0.936558 \tValidation Loss: 1.011510\n",
      "Validation loss decreased (1.011674 --> 1.011510).         Saving model ...\n",
      "Epoch: 356 \tTraining Loss: 0.936291 \tValidation Loss: 1.011474\n",
      "Validation loss decreased (1.011510 --> 1.011474).         Saving model ...\n",
      "Epoch: 357 \tTraining Loss: 0.936023 \tValidation Loss: 1.011175\n",
      "Validation loss decreased (1.011474 --> 1.011175).         Saving model ...\n",
      "Epoch: 358 \tTraining Loss: 0.935759 \tValidation Loss: 1.010556\n",
      "Validation loss decreased (1.011175 --> 1.010556).         Saving model ...\n",
      "Epoch: 359 \tTraining Loss: 0.935463 \tValidation Loss: 1.010240\n",
      "Validation loss decreased (1.010556 --> 1.010240).         Saving model ...\n",
      "Epoch: 360 \tTraining Loss: 0.935195 \tValidation Loss: 1.009968\n",
      "Validation loss decreased (1.010240 --> 1.009968).         Saving model ...\n",
      "Epoch: 361 \tTraining Loss: 0.934930 \tValidation Loss: 1.010405\n",
      "Epoch: 362 \tTraining Loss: 0.934671 \tValidation Loss: 1.009046\n",
      "Validation loss decreased (1.009968 --> 1.009046).         Saving model ...\n",
      "Epoch: 363 \tTraining Loss: 0.934416 \tValidation Loss: 1.008292\n",
      "Validation loss decreased (1.009046 --> 1.008292).         Saving model ...\n",
      "Epoch: 364 \tTraining Loss: 0.934164 \tValidation Loss: 1.008374\n",
      "Epoch: 365 \tTraining Loss: 0.933875 \tValidation Loss: 1.008205\n",
      "Validation loss decreased (1.008292 --> 1.008205).         Saving model ...\n",
      "Epoch: 366 \tTraining Loss: 0.933614 \tValidation Loss: 1.008756\n",
      "Epoch: 367 \tTraining Loss: 0.933331 \tValidation Loss: 1.008377\n",
      "Epoch: 368 \tTraining Loss: 0.933100 \tValidation Loss: 1.008018\n",
      "Validation loss decreased (1.008205 --> 1.008018).         Saving model ...\n",
      "Epoch: 369 \tTraining Loss: 0.932822 \tValidation Loss: 1.007534\n",
      "Validation loss decreased (1.008018 --> 1.007534).         Saving model ...\n",
      "Epoch: 370 \tTraining Loss: 0.932595 \tValidation Loss: 1.007391\n",
      "Validation loss decreased (1.007534 --> 1.007391).         Saving model ...\n",
      "Epoch: 371 \tTraining Loss: 0.932346 \tValidation Loss: 1.006653\n",
      "Validation loss decreased (1.007391 --> 1.006653).         Saving model ...\n",
      "Epoch: 372 \tTraining Loss: 0.932090 \tValidation Loss: 1.006583\n",
      "Validation loss decreased (1.006653 --> 1.006583).         Saving model ...\n",
      "Epoch: 373 \tTraining Loss: 0.931817 \tValidation Loss: 1.006369\n",
      "Validation loss decreased (1.006583 --> 1.006369).         Saving model ...\n",
      "Epoch: 374 \tTraining Loss: 0.931569 \tValidation Loss: 1.006453\n",
      "Epoch: 375 \tTraining Loss: 0.931317 \tValidation Loss: 1.006306\n",
      "Validation loss decreased (1.006369 --> 1.006306).         Saving model ...\n",
      "Epoch: 376 \tTraining Loss: 0.931061 \tValidation Loss: 1.006364\n",
      "Epoch: 377 \tTraining Loss: 0.930828 \tValidation Loss: 1.006438\n",
      "Epoch: 378 \tTraining Loss: 0.930570 \tValidation Loss: 1.005428\n",
      "Validation loss decreased (1.006306 --> 1.005428).         Saving model ...\n",
      "Epoch: 379 \tTraining Loss: 0.930321 \tValidation Loss: 1.006858\n",
      "Epoch: 380 \tTraining Loss: 0.930051 \tValidation Loss: 1.005048\n",
      "Validation loss decreased (1.005428 --> 1.005048).         Saving model ...\n",
      "Epoch: 381 \tTraining Loss: 0.929842 \tValidation Loss: 1.005010\n",
      "Validation loss decreased (1.005048 --> 1.005010).         Saving model ...\n",
      "Epoch: 382 \tTraining Loss: 0.929587 \tValidation Loss: 1.005665\n",
      "Epoch: 383 \tTraining Loss: 0.929363 \tValidation Loss: 1.004786\n",
      "Validation loss decreased (1.005010 --> 1.004786).         Saving model ...\n",
      "Epoch: 384 \tTraining Loss: 0.929096 \tValidation Loss: 1.004028\n",
      "Validation loss decreased (1.004786 --> 1.004028).         Saving model ...\n",
      "Epoch: 385 \tTraining Loss: 0.928858 \tValidation Loss: 1.004439\n",
      "Epoch: 386 \tTraining Loss: 0.928617 \tValidation Loss: 1.004141\n",
      "Epoch: 387 \tTraining Loss: 0.928344 \tValidation Loss: 1.003695\n",
      "Validation loss decreased (1.004028 --> 1.003695).         Saving model ...\n",
      "Epoch: 388 \tTraining Loss: 0.928133 \tValidation Loss: 1.003951\n",
      "Epoch: 389 \tTraining Loss: 0.927896 \tValidation Loss: 1.003042\n",
      "Validation loss decreased (1.003695 --> 1.003042).         Saving model ...\n",
      "Epoch: 390 \tTraining Loss: 0.927675 \tValidation Loss: 1.003356\n",
      "Epoch: 391 \tTraining Loss: 0.927412 \tValidation Loss: 1.002619\n",
      "Validation loss decreased (1.003042 --> 1.002619).         Saving model ...\n",
      "Epoch: 392 \tTraining Loss: 0.927176 \tValidation Loss: 1.002003\n",
      "Validation loss decreased (1.002619 --> 1.002003).         Saving model ...\n",
      "Epoch: 393 \tTraining Loss: 0.926951 \tValidation Loss: 1.002207\n",
      "Epoch: 394 \tTraining Loss: 0.926711 \tValidation Loss: 1.001909\n",
      "Validation loss decreased (1.002003 --> 1.001909).         Saving model ...\n",
      "Epoch: 395 \tTraining Loss: 0.926471 \tValidation Loss: 1.001960\n",
      "Epoch: 396 \tTraining Loss: 0.926252 \tValidation Loss: 1.001233\n",
      "Validation loss decreased (1.001909 --> 1.001233).         Saving model ...\n",
      "Epoch: 397 \tTraining Loss: 0.925998 \tValidation Loss: 1.002144\n",
      "Epoch: 398 \tTraining Loss: 0.925772 \tValidation Loss: 1.001387\n",
      "Epoch: 399 \tTraining Loss: 0.925550 \tValidation Loss: 1.001189\n",
      "Validation loss decreased (1.001233 --> 1.001189).         Saving model ...\n",
      "Epoch: 400 \tTraining Loss: 0.925320 \tValidation Loss: 1.001028\n",
      "Validation loss decreased (1.001189 --> 1.001028).         Saving model ...\n",
      "Epoch: 401 \tTraining Loss: 0.925077 \tValidation Loss: 1.000883\n",
      "Validation loss decreased (1.001028 --> 1.000883).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 402 \tTraining Loss: 0.924901 \tValidation Loss: 1.000904\n",
      "Epoch: 403 \tTraining Loss: 0.924623 \tValidation Loss: 0.999993\n",
      "Validation loss decreased (1.000883 --> 0.999993).         Saving model ...\n",
      "Epoch: 404 \tTraining Loss: 0.924415 \tValidation Loss: 1.000293\n",
      "Epoch: 405 \tTraining Loss: 0.924188 \tValidation Loss: 0.999895\n",
      "Validation loss decreased (0.999993 --> 0.999895).         Saving model ...\n",
      "Epoch: 406 \tTraining Loss: 0.923981 \tValidation Loss: 0.999646\n",
      "Validation loss decreased (0.999895 --> 0.999646).         Saving model ...\n",
      "Epoch: 407 \tTraining Loss: 0.923751 \tValidation Loss: 1.000289\n",
      "Epoch: 408 \tTraining Loss: 0.923530 \tValidation Loss: 0.998656\n",
      "Validation loss decreased (0.999646 --> 0.998656).         Saving model ...\n",
      "Epoch: 409 \tTraining Loss: 0.923305 \tValidation Loss: 0.999694\n",
      "Epoch: 410 \tTraining Loss: 0.923107 \tValidation Loss: 0.998850\n",
      "Epoch: 411 \tTraining Loss: 0.922886 \tValidation Loss: 1.000427\n",
      "Epoch: 412 \tTraining Loss: 0.922660 \tValidation Loss: 0.998447\n",
      "Validation loss decreased (0.998656 --> 0.998447).         Saving model ...\n",
      "Epoch: 413 \tTraining Loss: 0.922441 \tValidation Loss: 0.998599\n",
      "Epoch: 414 \tTraining Loss: 0.922214 \tValidation Loss: 0.997733\n",
      "Validation loss decreased (0.998447 --> 0.997733).         Saving model ...\n",
      "Epoch: 415 \tTraining Loss: 0.922012 \tValidation Loss: 0.998114\n",
      "Epoch: 416 \tTraining Loss: 0.921799 \tValidation Loss: 0.997334\n",
      "Validation loss decreased (0.997733 --> 0.997334).         Saving model ...\n",
      "Epoch: 417 \tTraining Loss: 0.921568 \tValidation Loss: 0.996518\n",
      "Validation loss decreased (0.997334 --> 0.996518).         Saving model ...\n",
      "Epoch: 418 \tTraining Loss: 0.921339 \tValidation Loss: 0.996728\n",
      "Epoch: 419 \tTraining Loss: 0.921161 \tValidation Loss: 0.997133\n",
      "Epoch: 420 \tTraining Loss: 0.920937 \tValidation Loss: 0.996909\n",
      "Epoch: 421 \tTraining Loss: 0.920719 \tValidation Loss: 0.996244\n",
      "Validation loss decreased (0.996518 --> 0.996244).         Saving model ...\n",
      "Epoch: 422 \tTraining Loss: 0.920497 \tValidation Loss: 0.995707\n",
      "Validation loss decreased (0.996244 --> 0.995707).         Saving model ...\n",
      "Epoch: 423 \tTraining Loss: 0.920298 \tValidation Loss: 0.996486\n",
      "Epoch: 424 \tTraining Loss: 0.920090 \tValidation Loss: 0.996183\n",
      "Epoch: 425 \tTraining Loss: 0.919900 \tValidation Loss: 0.995791\n",
      "Epoch: 426 \tTraining Loss: 0.919676 \tValidation Loss: 0.994883\n",
      "Validation loss decreased (0.995707 --> 0.994883).         Saving model ...\n",
      "Epoch: 427 \tTraining Loss: 0.919477 \tValidation Loss: 0.995343\n",
      "Epoch: 428 \tTraining Loss: 0.919272 \tValidation Loss: 0.994062\n",
      "Validation loss decreased (0.994883 --> 0.994062).         Saving model ...\n",
      "Epoch: 429 \tTraining Loss: 0.919065 \tValidation Loss: 0.995082\n",
      "Epoch: 430 \tTraining Loss: 0.918873 \tValidation Loss: 0.994031\n",
      "Validation loss decreased (0.994062 --> 0.994031).         Saving model ...\n",
      "Epoch: 431 \tTraining Loss: 0.918634 \tValidation Loss: 0.994899\n",
      "Epoch: 432 \tTraining Loss: 0.918453 \tValidation Loss: 0.992458\n",
      "Validation loss decreased (0.994031 --> 0.992458).         Saving model ...\n",
      "Epoch: 433 \tTraining Loss: 0.918243 \tValidation Loss: 0.993151\n",
      "Epoch: 434 \tTraining Loss: 0.918054 \tValidation Loss: 0.993050\n",
      "Epoch: 435 \tTraining Loss: 0.917829 \tValidation Loss: 0.993269\n",
      "Epoch: 436 \tTraining Loss: 0.917645 \tValidation Loss: 0.993242\n",
      "Epoch: 437 \tTraining Loss: 0.917425 \tValidation Loss: 0.993350\n",
      "Epoch: 438 \tTraining Loss: 0.917231 \tValidation Loss: 0.992768\n",
      "Epoch: 439 \tTraining Loss: 0.917043 \tValidation Loss: 0.993121\n",
      "Epoch: 440 \tTraining Loss: 0.916860 \tValidation Loss: 0.992379\n",
      "Validation loss decreased (0.992458 --> 0.992379).         Saving model ...\n",
      "Epoch: 441 \tTraining Loss: 0.916643 \tValidation Loss: 0.991111\n",
      "Validation loss decreased (0.992379 --> 0.991111).         Saving model ...\n",
      "Epoch: 442 \tTraining Loss: 0.916453 \tValidation Loss: 0.992263\n",
      "Epoch: 443 \tTraining Loss: 0.916261 \tValidation Loss: 0.991661\n",
      "Epoch: 444 \tTraining Loss: 0.916072 \tValidation Loss: 0.992331\n",
      "Epoch: 445 \tTraining Loss: 0.915884 \tValidation Loss: 0.991167\n",
      "Epoch: 446 \tTraining Loss: 0.915654 \tValidation Loss: 0.990885\n",
      "Validation loss decreased (0.991111 --> 0.990885).         Saving model ...\n",
      "Epoch: 447 \tTraining Loss: 0.915480 \tValidation Loss: 0.989688\n",
      "Validation loss decreased (0.990885 --> 0.989688).         Saving model ...\n",
      "Epoch: 448 \tTraining Loss: 0.915270 \tValidation Loss: 0.990624\n",
      "Epoch: 449 \tTraining Loss: 0.915096 \tValidation Loss: 0.990024\n",
      "Epoch: 450 \tTraining Loss: 0.914859 \tValidation Loss: 0.989786\n",
      "Epoch: 451 \tTraining Loss: 0.914702 \tValidation Loss: 0.990014\n",
      "Epoch: 452 \tTraining Loss: 0.914479 \tValidation Loss: 0.989538\n",
      "Validation loss decreased (0.989688 --> 0.989538).         Saving model ...\n",
      "Epoch: 453 \tTraining Loss: 0.914305 \tValidation Loss: 0.988916\n",
      "Validation loss decreased (0.989538 --> 0.988916).         Saving model ...\n",
      "Epoch: 454 \tTraining Loss: 0.914162 \tValidation Loss: 0.988907\n",
      "Validation loss decreased (0.988916 --> 0.988907).         Saving model ...\n",
      "Epoch: 455 \tTraining Loss: 0.913964 \tValidation Loss: 0.989050\n",
      "Epoch: 456 \tTraining Loss: 0.913765 \tValidation Loss: 0.987772\n",
      "Validation loss decreased (0.988907 --> 0.987772).         Saving model ...\n",
      "Epoch: 457 \tTraining Loss: 0.913588 \tValidation Loss: 0.987965\n",
      "Epoch: 458 \tTraining Loss: 0.913407 \tValidation Loss: 0.988900\n",
      "Epoch: 459 \tTraining Loss: 0.913223 \tValidation Loss: 0.987658\n",
      "Validation loss decreased (0.987772 --> 0.987658).         Saving model ...\n",
      "Epoch: 460 \tTraining Loss: 0.913046 \tValidation Loss: 0.987418\n",
      "Validation loss decreased (0.987658 --> 0.987418).         Saving model ...\n",
      "Epoch: 461 \tTraining Loss: 0.912836 \tValidation Loss: 0.987380\n",
      "Validation loss decreased (0.987418 --> 0.987380).         Saving model ...\n",
      "Epoch: 462 \tTraining Loss: 0.912651 \tValidation Loss: 0.987148\n",
      "Validation loss decreased (0.987380 --> 0.987148).         Saving model ...\n",
      "Epoch: 463 \tTraining Loss: 0.912455 \tValidation Loss: 0.986622\n",
      "Validation loss decreased (0.987148 --> 0.986622).         Saving model ...\n",
      "Epoch: 464 \tTraining Loss: 0.912290 \tValidation Loss: 0.986548\n",
      "Validation loss decreased (0.986622 --> 0.986548).         Saving model ...\n",
      "Epoch: 465 \tTraining Loss: 0.912095 \tValidation Loss: 0.985685\n",
      "Validation loss decreased (0.986548 --> 0.985685).         Saving model ...\n",
      "Epoch: 466 \tTraining Loss: 0.911936 \tValidation Loss: 0.986513\n",
      "Epoch: 467 \tTraining Loss: 0.911737 \tValidation Loss: 0.986542\n",
      "Epoch: 468 \tTraining Loss: 0.911572 \tValidation Loss: 0.986228\n",
      "Epoch: 469 \tTraining Loss: 0.911420 \tValidation Loss: 0.986430\n",
      "Epoch: 470 \tTraining Loss: 0.911182 \tValidation Loss: 0.985254\n",
      "Validation loss decreased (0.985685 --> 0.985254).         Saving model ...\n",
      "Epoch: 471 \tTraining Loss: 0.911012 \tValidation Loss: 0.985167\n",
      "Validation loss decreased (0.985254 --> 0.985167).         Saving model ...\n",
      "Epoch: 472 \tTraining Loss: 0.910841 \tValidation Loss: 0.984602\n",
      "Validation loss decreased (0.985167 --> 0.984602).         Saving model ...\n",
      "Epoch: 473 \tTraining Loss: 0.910679 \tValidation Loss: 0.986593\n",
      "Epoch: 474 \tTraining Loss: 0.910505 \tValidation Loss: 0.983990\n",
      "Validation loss decreased (0.984602 --> 0.983990).         Saving model ...\n",
      "Epoch: 475 \tTraining Loss: 0.910304 \tValidation Loss: 0.985218\n",
      "Epoch: 476 \tTraining Loss: 0.910126 \tValidation Loss: 0.984382\n",
      "Epoch: 477 \tTraining Loss: 0.909933 \tValidation Loss: 0.983828\n",
      "Validation loss decreased (0.983990 --> 0.983828).         Saving model ...\n",
      "Epoch: 478 \tTraining Loss: 0.909799 \tValidation Loss: 0.984615\n",
      "Epoch: 479 \tTraining Loss: 0.909618 \tValidation Loss: 0.983357\n",
      "Validation loss decreased (0.983828 --> 0.983357).         Saving model ...\n",
      "Epoch: 480 \tTraining Loss: 0.909418 \tValidation Loss: 0.983063\n",
      "Validation loss decreased (0.983357 --> 0.983063).         Saving model ...\n",
      "Epoch: 481 \tTraining Loss: 0.909275 \tValidation Loss: 0.983052\n",
      "Validation loss decreased (0.983063 --> 0.983052).         Saving model ...\n",
      "Epoch: 482 \tTraining Loss: 0.909127 \tValidation Loss: 0.982166\n",
      "Validation loss decreased (0.983052 --> 0.982166).         Saving model ...\n",
      "Epoch: 483 \tTraining Loss: 0.908947 \tValidation Loss: 0.983291\n",
      "Epoch: 484 \tTraining Loss: 0.908774 \tValidation Loss: 0.981032\n",
      "Validation loss decreased (0.982166 --> 0.981032).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 485 \tTraining Loss: 0.908606 \tValidation Loss: 0.982352\n",
      "Epoch: 486 \tTraining Loss: 0.908430 \tValidation Loss: 0.981490\n",
      "Epoch: 487 \tTraining Loss: 0.908242 \tValidation Loss: 0.981305\n",
      "Epoch: 488 \tTraining Loss: 0.908099 \tValidation Loss: 0.979771\n",
      "Validation loss decreased (0.981032 --> 0.979771).         Saving model ...\n",
      "Epoch: 489 \tTraining Loss: 0.907909 \tValidation Loss: 0.981084\n",
      "Epoch: 490 \tTraining Loss: 0.907745 \tValidation Loss: 0.980075\n",
      "Epoch: 491 \tTraining Loss: 0.907555 \tValidation Loss: 0.979977\n",
      "Epoch: 492 \tTraining Loss: 0.907392 \tValidation Loss: 0.979470\n",
      "Validation loss decreased (0.979771 --> 0.979470).         Saving model ...\n",
      "Epoch: 493 \tTraining Loss: 0.907234 \tValidation Loss: 0.979512\n",
      "Epoch: 494 \tTraining Loss: 0.907087 \tValidation Loss: 0.979547\n",
      "Epoch: 495 \tTraining Loss: 0.906916 \tValidation Loss: 0.979761\n",
      "Epoch: 496 \tTraining Loss: 0.906728 \tValidation Loss: 0.978475\n",
      "Validation loss decreased (0.979470 --> 0.978475).         Saving model ...\n",
      "Epoch: 497 \tTraining Loss: 0.906563 \tValidation Loss: 0.978116\n",
      "Validation loss decreased (0.978475 --> 0.978116).         Saving model ...\n",
      "Epoch: 498 \tTraining Loss: 0.906404 \tValidation Loss: 0.978048\n",
      "Validation loss decreased (0.978116 --> 0.978048).         Saving model ...\n",
      "Epoch: 499 \tTraining Loss: 0.906257 \tValidation Loss: 0.978888\n",
      "Epoch: 500 \tTraining Loss: 0.906103 \tValidation Loss: 0.978166\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 500\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    two_layer.train() # prep model for training\n",
    "    for data, target in train_loader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = two_layer(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    two_layer.eval() # prep model for evaluation\n",
    "    for data, target in test_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = two_layer(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(test_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}' \\\n",
    "          .format(epoch+1, train_loss, valid_loss))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}). \\\n",
    "        Saving model ...'.format(valid_loss_min, valid_loss))\n",
    "        \n",
    "        torch.save(two_layer.state_dict(), 'bert_two_layer.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6f9a39f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.90      0.68      8761\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.09      0.96      0.16        49\n",
      "           4       0.00      0.00      0.00         0\n",
      "           5       0.43      0.33      0.38     13634\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       0.00      0.29      0.01         7\n",
      "           8       0.00      0.00      0.00         0\n",
      "           9       0.00      0.00      0.00         0\n",
      "          10       0.96      0.73      0.83     47884\n",
      "          11       0.00      0.00      0.00         0\n",
      "          12       0.52      0.80      0.63      2903\n",
      "          13       0.00      0.00      0.00         0\n",
      "          14       0.00      0.00      0.00         0\n",
      "          15       0.35      0.60      0.44       581\n",
      "          16       0.00      0.12      0.00        26\n",
      "          17       0.00      0.00      0.00         0\n",
      "          18       0.00      0.00      0.00         0\n",
      "          19       0.54      0.81      0.65      1632\n",
      "          20       0.00      0.33      0.00         6\n",
      "          21       0.00      0.00      0.00         0\n",
      "          22       0.00      0.00      0.00         0\n",
      "          23       0.63      0.76      0.69       963\n",
      "          24       0.00      0.00      0.00         0\n",
      "          25       0.00      0.00      0.00         0\n",
      "          26       0.11      0.68      0.18       203\n",
      "          27       0.00      0.00      0.00         0\n",
      "          28       0.00      1.00      0.00         1\n",
      "          29       0.00      0.00      0.00         0\n",
      "          30       0.01      0.27      0.02        22\n",
      "          31       0.29      0.85      0.43       216\n",
      "          32       0.00      0.00      0.00         0\n",
      "          33       0.81      0.63      0.71      2441\n",
      "          34       0.00      0.00      0.00         0\n",
      "          35       0.75      0.65      0.70      5245\n",
      "          36       0.00      0.00      0.00         0\n",
      "          37       0.84      0.80      0.82     75121\n",
      "          38       0.69      0.57      0.62     30058\n",
      "          39       0.00      0.00      0.00         0\n",
      "          40       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.71    189753\n",
      "   macro avg       0.18      0.29      0.19    189753\n",
      "weighted avg       0.79      0.71      0.74    189753\n",
      "\n",
      "Testing Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.90      0.69       475\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.16      0.71      0.26         7\n",
      "           4       0.00      0.00      0.00         0\n",
      "           5       0.43      0.33      0.37       765\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       0.02      1.00      0.05         1\n",
      "           8       0.00      0.00      0.00         0\n",
      "           9       0.00      0.00      0.00         0\n",
      "          10       0.94      0.72      0.82      2468\n",
      "          11       0.00      0.00      0.00         0\n",
      "          12       0.48      0.82      0.60       143\n",
      "          13       0.00      0.00      0.00         0\n",
      "          14       0.00      0.00      0.00         0\n",
      "          15       0.38      0.59      0.46        34\n",
      "          16       0.00      0.00      0.00         3\n",
      "          17       0.00      0.00      0.00         0\n",
      "          18       0.00      0.00      0.00         0\n",
      "          19       0.48      0.73      0.58        79\n",
      "          20       0.00      0.00      0.00         0\n",
      "          21       0.00      0.00      0.00         0\n",
      "          22       0.00      0.00      0.00         0\n",
      "          23       0.60      0.75      0.67        56\n",
      "          24       0.00      0.00      0.00         0\n",
      "          25       0.00      0.00      0.00         0\n",
      "          26       0.10      0.78      0.18         9\n",
      "          27       0.00      0.00      0.00         0\n",
      "          28       0.00      0.00      0.00         0\n",
      "          29       0.00      0.00      0.00         0\n",
      "          30       0.03      1.00      0.07         1\n",
      "          31       0.11      1.00      0.20         3\n",
      "          32       0.00      0.00      0.00         0\n",
      "          33       0.85      0.65      0.74       126\n",
      "          34       0.00      0.00      0.00         0\n",
      "          35       0.75      0.63      0.68       268\n",
      "          36       0.00      0.00      0.00         0\n",
      "          37       0.84      0.80      0.82      3965\n",
      "          38       0.71      0.58      0.64      1584\n",
      "          39       0.00      0.00      0.00         0\n",
      "          40       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.71      9987\n",
      "   macro avg       0.18      0.29      0.19      9987\n",
      "weighted avg       0.78      0.71      0.74      9987\n",
      "\n"
     ]
    }
   ],
   "source": [
    "two_layer = Net()\n",
    "two_layer.load_state_dict(torch.load(\"bert_two_layer.pt\"))\n",
    "two_layer.eval()\n",
    "\n",
    "pred_list = torch.zeros(0, dtype=torch.long)\n",
    "target_list = torch.zeros(0, dtype=torch.long)\n",
    "train_count = 0\n",
    "count = 0\n",
    "\n",
    "train_pred_list = torch.zeros(0, dtype=torch.long)\n",
    "train_target_list = torch.zeros(0, dtype=torch.long)\n",
    "\n",
    "for data, target in train_loader:\n",
    "    output = two_layer(data)\n",
    "    _, preds = torch.max(output, 1)\n",
    "    train_pred_list = torch.cat([train_pred_list, preds.view(-1)])\n",
    "    train_target_list = torch.cat([train_target_list, target.view(-1)])\n",
    "#     for x in range(16):\n",
    "#         if preds[x] == target[x]: \n",
    "#             train_count+=1\n",
    "            \n",
    "# print(\"Training Accuracy =\", train_count / len(train_loader.dataset))\n",
    "train_result = classification_report(train_pred_list.numpy(), \n",
    "                                     train_target_list.numpy())\n",
    "print(\"Training Classification report: \\n\", train_result)\n",
    "            \n",
    "for data, target in test_loader:\n",
    "    output = two_layer(data)\n",
    "    _, preds = torch.max(output, 1) \n",
    "    pred_list = torch.cat([pred_list, preds.view(-1)])\n",
    "    target_list = torch.cat([target_list, target.view(-1)])\n",
    "#     for x in range(16):\n",
    "#         if preds[x] == target[x]: \n",
    "#             count+=1\n",
    "\n",
    "# print(\"Testing Accuracy =\", count / len(test_loader.dataset))\n",
    "\n",
    "result = classification_report(pred_list.numpy(), target_list.numpy())\n",
    "print(\"Testing Classification report: \\n\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "81e835db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module): \n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__() \n",
    "#         self.gru = nn.GRU(input_size=768, hidden_size=32, batch_first=True) \n",
    "        self.fc1 = nn.Linear(768,384)\n",
    "        self.fc2 = nn.Linear(384, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 42)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "#         self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "#         out, hn = self.gru(x, torch.randn(1, len(x), 32))\n",
    "        out = self.fc1(self.relu(x))\n",
    "        out = self.fc2(self.relu(out))\n",
    "        out = self.fc3(self.relu(out))\n",
    "        out = self.fc4(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "795065b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=768, out_features=384, bias=True)\n",
      "  (fc2): Linear(in_features=384, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc4): Linear(in_features=128, out_features=42, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "three_layer = Net()\n",
    "print(three_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4c5b11e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer = torch.optim.SGD(three_layer.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e0bff284",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (256x768 and 128x42)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# forward pass: compute predicted outputs by passing inputs to the model\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mthree_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# calculate the loss\u001b[39;00m\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[49], line 22\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out))\n\u001b[1;32m     21\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out))\n\u001b[0;32m---> 22\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (256x768 and 128x42)"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 150\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    three_layer.train() # prep model for training\n",
    "    for data, target in train_loader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = three_layer(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    three_layer.eval() # prep model for evaluation\n",
    "    for data, target in test_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = three_layer(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(test_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}' \\\n",
    "          .format(epoch+1, train_loss, valid_loss))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}). \\\n",
    "        Saving model ...'.format(valid_loss_min, valid_loss))\n",
    "        \n",
    "        torch.save(three_layer.state_dict(), 'bert_three_layer.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494b09be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "three_layer = Net()\n",
    "three_layer.load_state_dict(torch.load(\"bert_three_layer.pt\"))\n",
    "three_layer.eval()\n",
    "\n",
    "pred_list = torch.zeros(0, dtype=torch.long)\n",
    "target_list = torch.zeros(0, dtype=torch.long)\n",
    "train_count = 0\n",
    "count = 0\n",
    "\n",
    "train_pred_list = torch.zeros(0, dtype=torch.long)\n",
    "train_target_list = torch.zeros(0, dtype=torch.long)\n",
    "\n",
    "for data, target in train_loader:\n",
    "    output = three_layer(data)\n",
    "    _, preds = torch.max(output, 1)\n",
    "    train_pred_list = torch.cat([train_pred_list, preds.view(-1)])\n",
    "    train_target_list = torch.cat([train_target_list, target.view(-1)])\n",
    "#     for x in range(16):\n",
    "#         if preds[x] == target[x]: \n",
    "#             train_count+=1\n",
    "            \n",
    "# print(\"Training Accuracy =\", train_count / len(train_loader.dataset))\n",
    "train_result = classification_report(train_pred_list.numpy(), \n",
    "                                     train_target_list.numpy())\n",
    "print(\"Training Classification report: \\n\", train_result)\n",
    "            \n",
    "for data, target in test_loader:\n",
    "    output = three_layer(data)\n",
    "    _, preds = torch.max(output, 1) \n",
    "    pred_list = torch.cat([pred_list, preds.view(-1)])\n",
    "    target_list = torch.cat([target_list, target.view(-1)])\n",
    "#     for x in range(16):\n",
    "#         if preds[x] == target[x]: \n",
    "#             count+=1\n",
    "\n",
    "# print(\"Testing Accuracy =\", count / len(test_loader.dataset))\n",
    "\n",
    "result = classification_report(pred_list.numpy(), target_list.numpy(), output_dict = True)\n",
    "result = pd.DataFrame(result).transpose()\n",
    "print(\"Testing Classification report: \\n\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cffde3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
